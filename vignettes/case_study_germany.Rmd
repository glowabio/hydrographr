---
title: "Data processing prior to network analyses for freshwater fish in Germany"
subtitle: "Processing of spatial and species data"
author: ""
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Case study - Germany}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
bibliography: bib/references_germany.bib
link-citations: yes
linkcolor: blue
# csl: copernicus.csl
nocite: '@*'
editor_options:
  markdown:
    wrap: 72
---

```{r, include = FALSE, eval = FALSE}
# writes out the references for all packages
knitr::write_bib(file = 'references_germany.bib')
```

### Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,  
                      out.width='50%', fig.align='center')
knitr::opts_knit$set(root.dir = "./data_germany")


library(hydrographr)
library(rgbif)
library(terra)
library(tools)
library(data.table)
library(dplyr)
library(stringr)
library(knitr)
library(kableExtra)
library(leaflet)
library(leafem)
library(htmlwidgets)
library(mapview)
library(here)
```

Load required libraries

```{r}
library(hydrographr)
library(rgbif)
library(data.table)
library(dplyr)
library(terra)
library(tools)
library(stringr)
library(leaflet)
library(leafem)
```

Define working directory

```{r, eval=TRUE, include=FALSE}
wdir <- paste0(here(), "/vignettes/data_germany")
if(!dir.exists(paste0(wdir, "/data"))) dir.create(paste0(wdir, "/data"))
```

```{r, eval = FALSE}
# Define the "data_germany" directory, where you have downloaded all the data,
# as the working directory
wdir <- "my/working/directory/data_germany"
setwd(wdir)

# Create a new folder in the working directory to store all the data
dir.create("data")
```

### Species data

We first download the occurrence data with coordinates from GBIF

```{r, eval = F}
# Once: Download species occurrence data based on the key of the dataset
# and write out to working directory
spdata_all <- occ_download_get(key="0004551-231002084531237",
                              overwrite = TRUE) %>%
  occ_download_import
fwrite(spdata_all, paste0(wdir, "/data/fish_germany_gbif.csv"),
    row.names = F, quote = F, sep = "\t")
```

```{r}
# Import and clean the data
spdata <- fread(paste0(wdir, "/data/fish_germany_gbif.csv"), sep = "\t") %>%
  select(gbifID, decimalLongitude, decimalLatitude, species, year) %>%
  rename("longitude" = "decimalLongitude",
         "latitude" = "decimalLatitude")
```

```{r, echo = F}
spdata <- head(spdata)
```

```{r, eval = F}
head(spdata)
```

```{r, echo = F}
kbl(head(spdata)) %>%
  kable_styling(bootstrap_options = "striped", 
                full_width = F, position = "left") %>%
  scroll_box(width = "500px", height = "200px")
```

Let's visualise the species occurrences on the map

Let's define the extent (bounding box) of the study area (xmin, ymin,
xmax, ymax)

```{r, eval = F}
# Define the extent
bbox <- c(min(spdata$longitude), min(spdata$latitude),
          max(spdata$longitude), max(spdata$latitude))
```

```{r, eval = F}
# Define color palette for the different years of record
factpal <- colorFactor(hcl.colors(unique(spdata$year)), spdata$year)

# Create leaflet plot
spdata_plot <- leaflet(spdata) %>%
  addProviderTiles('Esri.WorldShadedRelief') %>%
  setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%
  addCircles(lng = ~longitude, lat = ~ latitude, 
             color =  ~factpal(as.factor(year)),
             opacity = 1) %>%
  addLegend(pal = factpal, values = ~as.factor(year),
            title = "Year of record")
spdata_plot

```

```{r, echo = F, eval = F}
saveWidget(spdata_plot, file=paste0(here(), "man/figures/germany_map.html"))
```

![](../man/figures/germany_map.png){width="800"}

### Abiotic variables data

#### 1. Hydrography90m

In order to download layers of the Hydrography90m, we need to know the
IDs of the 20°x20° tiles in which they are located. We can obtain these
IDs using the function *get_tile_id()*. This function downloads and uses
the auxiliary raster file that contains all the regional units globally,
and thus requires an active internet connection.

```{r, eval = FALSE}
tile_id <- get_tile_id(data = spdata,
                     lon = "longitude", lat = "latitude")

# Get reg unit id to crop all the regular tile layers so that 
# we have uninterrupted basins
reg_unit_id <- get_regional_unit_id(data = spdata,
                                  lon = "longitude", lat = "latitude")

```

```{r, include = FALSE}
tile_id <- c("h16v02", "h18v00", "h18v02")
```

```{r}
tile_id
```

Currently the function returns all the tiles of the regional unit where
the input points are located. However, some of them may be far from the
study area and hence not always needed in further steps. Please double
check which tile IDs are relevant for your purpose using the **Tile
map** found
[here](https://hydrography.org/hydrography90m/hydrography90m_layers/).

In our case, Germany spreads in just one tile, with the ID "h18v02", so
we will keep only this one.

```{r}
tile_id <- "h18v02"
```

Then we define the names of the raster and vector layers we want to
download.

```{r, eval = FALSE}
# Define the raster layers
vars_tif <- c("basin", "sub_catchment", "segment", "accumulation", "direction",
              "outlet_dist_dw_basin", "outlet_dist_dw_scatch",
              "channel_dist_up_seg", "order_strahler")
# Define the vector layers
# The "basin" layer contains the polygons of the drainage basins while the
# "order_vect_segment" layer is the stream network vector file
vars_gpkg <- c("basin", "order_vect_segment")

```

```{r, eval = FALSE}
# Extend timeout to 1000s to allow uninterrupted downloading
options(timeout = 1000)
# Download the .tif tiles of the desired variables
download_tiles(variable = vars_tif, tile_id = tile_id, file_format = "tif",
               download_dir = "data")

# Download the .gpkg tiles of the desired variables
download_tiles(variable = vars_gpkg, tile_id = tile_id, file_format = "gpkg",
               download_dir = "data")

# Download the raster mask of the regional unit
download_tiles(variable = "regional_unit",
               file_format = "tif",
               reg_unit_id = reg_unit_id,
               download_dir = "data")
```

#### 2. Elevation - MERIT-HYDRO

To download the elevation files of MERIT-HYDRO, we visit
<https://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_Hydro/> to define the
tiles that need to be downloaded. We download the zipped tiles into a
new directory called ***elv***, unzip the downloaded .tar file and keep
only the tiles that we need

```{r, eval = FALSE}
elv_dir <- paste0(wdir, "/data/elv") 
dir.create(elv_dir)
```

#### 3. Climate - CHELSA Bioclim

Finally, we will download three CHELSA present Bioclim variables. For a
quick outlook on the bioclimatic variables you can have a look
[here](https://chelsa-climate.org/bioclim/).

```{r, eval = FALSE}
# Create download directory
dir.create(paste0(wdir, "/data/chelsa_bioclim"))
```

```{r, eval = FALSE}
# Extend timeout to 1000s to allow uninterrupted downloading
options(timeout = 1000)

# Download
# Present, 1981-2010
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio12_1981-2010_V.2.1.tif",
destfile = "data/chelsa_bioclim/bio12_1981-2010.tif", mode = "wb")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio15_1981-2010_V.2.1.tif",
destfile = "data/chelsa_bioclim/bio15_1981-2010.tif", mode = "wb")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio1_1981-2010_V.2.1.tif",
destfile = "data/chelsa_bioclim/bio1_1981-2010.tif", mode = "wb")
```

### Cropping the raster files

After having downloaded all the layers, we need to crop them to the
extent of our study area extended by 500 km, so that our basins are not
split in half.

```{r, include = FALSE}
# Define and create a directory for the study area
#study_area_dir <-  paste0(wdir, "/data/study_area")
study_area_dir <- "/home/afroditi/Documents/PhD/external/fish_germany/data/study_area"

if(!dir.exists(study_area_dir)) dir.create(study_area_dir)


# Get the full paths of the raster tiles
raster_tiles_watershed <- list.files(paste0(wdir, "/data/r.watershed"),
                          pattern = ".tif", full.names = TRUE, recursive = TRUE)
raster_tiles_dist <- list.files(paste0(wdir, "/data/r.stream.distance"),
                          pattern = ".tif", full.names = TRUE, recursive = TRUE)
raster_tiles_chan <- list.files(paste0(wdir, "/data/r.stream.channel"),
                          pattern = ".tif", full.names = TRUE, recursive = TRUE)
raster_tiles_ord <- list.files(paste0(wdir, "/data/r.stream.order"),
                               pattern = ".tif", full.names = TRUE, recursive = TRUE)
raster_tiles <- c(raster_tiles_watershed, raster_tiles_dist, 
                  raster_tiles_chan, raster_tiles_ord)
```

Let's define the extent (bounding box) of the study area (xmin, ymin,
xmax, ymax)

```{r, include = FALSE}
bb <- ext(vect(paste0(wdir, "/data/r.watershed/basin_tiles20d/basin_h18v02_filtered.gpkg")))
```

```{r, eval = F}
bb <- c(0.256, 20, 45, 55.4325)
```

We then crop the raster tiles to the extent using the function
*crop_to_extent()* in a loop

```{r, eval = FALSE}

for(itile in raster_tiles) {

  crop_to_extent(raster_layer = itile,
                 bounding_box = bb,
                 out_dir = study_area_dir,
                 file_name =  paste0(str_remove(basename(itile), ".tif"),
                                     "_crop.tif"),
                 quiet = FALSE,
                 compression = "high",
                 bigtiff = TRUE,
                 read = FALSE)
}
```

### Filtering the sub-catchment and basin .gpkg files

In case you don't work on a server, we suggest you to download the
output files of this chunk from the following links and then copy them
in the folder *`study_area_dir`*:

-   [order_vect_segment_h18v02_crop.gpkg](https://drive.google.com/file/d/14jh2zG7eqlS-KQZlZufLcZfPdEhRncd8/view?usp=drive_link)

-   [basin_h18v02_crop.gpkg](https://drive.google.com/file/d/1l9l7k3s9YIVkjhSoRqLVb8L2rGStSvz0/view?usp=drive_link)

```{r, eval = FALSE}
# !! Only run this chunk on a machine with more than 16 GB RAM, 
# as the input files are really big !!

# Load the cropped stream and basin raster layer of the study area.
# The stream raster can be used interchangeably with the sub_catchment raster, 
# because the stream IDs are the same as the sub-catchment IDs. 
# Here we use the stream raster because it's smaller in size. 

stream_layer <- rast(paste0(study_area_dir, "/segment_h18v02_crop.tif"))
basin_layer <- rast(paste0(study_area_dir, "/basin_h18v02_crop.tif"))

# Get all sub-catchment and basin IDs of the study area
subc_ids <- terra::unique(stream_layer)
basin_ids <- terra::unique(basin_layer)

# Get the full path of the stream order segment GeoPackage tile
order_tile <- list.files(wdir, pattern = "order.+_h[v0-8]+.gpkg$",
                         full.names = TRUE, recursive = TRUE)
basin_gpkg_tile <- list.files(wdir, pattern = "bas.+_h[v0-8]+.gpkg$",
                              full.names = TRUE, recursive = TRUE)

# Filter the sub-catchment IDs from the GeoPackage of the order_vector_segment
# tiles (sub-catchment ID = stream ID)
# Save the stream segments of the study area
filtered_stream <- read_geopackage(order_tile,
                                 import_as = "sf",
                                 subc_id = subc_ids$segment_h18v02_crop,
                                 name = "stream")

sf::write_sf(filtered_stream, paste(study_area_dir,
                              paste0(str_remove(basename(order_tile), ".gpkg"),
                                         "_crop.gpkg"), sep="/"))

filtered_bas <- read_geopackage(basin_gpkg_tile,
                                 import_as = "sf",
                                 subc_id = basin_ids$basin_h18v02_crop,
                                 name = "ID")

sf::write_sf(filtered_bas, paste(study_area_dir,
                          paste0(str_remove(basename(basin_gpkg_tile), ".gpkg"),
                                         "_crop.gpkg"), sep="/"))

```

### Merging the elevation tiles

```{r, eval = FALSE}
# These are the elevation tiles that include our study area
elv_tiles <- c("n45e000_elv.tif", "n50e010_elv.tif", "n60e000_elv.tif",
                "n45e005_elv.tif", "n50e015_elv.tif", "n60e005_elv.tif",
                "n45e010_elv.tif", "n55e000_elv.tif", "n60e010_elv.tif",
                "n45e015_elv.tif", "n55e005_elv.tif", "n60e015_elv.tif",
                "n50e000_elv.tif", "n55e010_elv.tif", "n50e005_elv.tif",
                "n55e015_elv.tif")

merge_tiles(tile_dir = elv_dir, 
            tile_names = elv_tiles,
            out_dir = study_area_dir, 
            file_name = "elv_study_area.tif",
            compression = "high",
            bigtiff = TRUE,
            quiet = FALSE)


# crop to our extent
crop_to_extent(raster_layer = paste0(study_area_dir, "/elv_study_area.tif"),
               bounding_box = bb,
               out_dir = study_area_dir,
               file_name =  "elv_study_area_crop.tif",
               quiet = FALSE,
               compression = "high", 
               bigtiff = TRUE, 
               read = FALSE)

```

Finally, we will crop the CHELSA Bioclim layers.

We define the directory containing the layers to be cropped and we list
their file names

```{r, eval = FALSE}
dir_chelsa <- paste0(wdir, "/data/chelsa_bioclim")
files_chelsa <- list.files(dir_chelsa, pattern = ".tif", full.names = TRUE)
```

```{r, eval = FALSE}
for(ifile in files_chelsa) {
    crop_to_extent(
      raster_layer = ifile,
      bounding_box = bb,
      out_dir = study_area_dir,
      file_name = basename(ifile),
      read = FALSE,
      quiet = TRUE)
}
```

### Extracting sub-catchment IDs

Extract the IDs of the sub-catchments where the points are located. This
step is crucial, as many of the functions that we will later use require
a vector of sub-catchment IDs as input. Note that the function
*extract_ids()* can be used to extract the values at specific points of
any raster file provided to the argument *subc_layer*. It can be safely
used to query very large raster files, as these are not loaded into R.

```{r, eval = TRUE}
spdata_ids <- extract_ids(data = spdata,
  id = "gbifID",
  lon = "longitude", lat = "latitude",
  basin_layer = paste0(study_area_dir, "/basin_h18v02_crop.tif"),
  subc_layer = paste0(study_area_dir, "/sub_catchment_h18v02_crop.tif"))
```

```{r, echo = FALSE}
knitr::kable(head(spdata_ids),
             caption = "The species data have now their corresponding sub-catchment ids")
```

```{r, echo = FALSE}
# # write points as gpkg
# spdata_vect <- vect(spdata_ids, geom=c("longitude", "latitude"))
# writeVector(spdata_vect, paste0(wdir, "/data/spdata.gpkg"), overwrite=T)
```

### Snapping points to the network

Before we can calculate the distance along the stream network between
species occurrences, we need to snap the coordinates of the sites to the
stream network. Recorded coordinates of point locations usually do not
exactly overlap with the digital stream network and, therefore, need to
be slightly corrected.

The hydrographr package offers two different snapping functions,
`snap_to_network` and `snap_to_subc_segment`. The first function uses a
defined distance radius and a flow accumulation threshold, while the
second function snaps the point to the stream segment of the
sub-catchment the point was originally located in.

For this case study we will use the function `snap_to_network` to be
able to define a certain flow accumulation threshold and ensure that the
fish occurrences will not be snapped to a headwater stream (first order
stream) if there is also a higher order stream next to it.

![](../man/figures/snapping.png)

```{r, eval = FALSE}
# Define full paths of raster layers
stream_rast <- paste0(study_area_dir, "/segment_h18v02_crop.tif")
flow_rast <- paste0(study_area_dir, "/accumulation_h18v02_crop.tif")
```

```{r, eval = FALSE}
# We need to shorten the gbifIDs because they are too long for GRASS-GIS
# We will delete the first 2 characters ("40") from all IDs
spdata_ids$gbifID_tmp <- str_replace(spdata_ids$gbifID, "40", "")
```

The function is implemented in a for-loop that starts searching for
streams with a very high flow accumulation of 400,000 km² in a very
short distance and then slowly decreases the flow accumulation to 100
km². If there are still sites left which were not snapped to a stream
segment, the distance will increase from 10 up to 30 cells.

```{r, eval = FALSE}
# Define thresholds for the flow accumulation of the stream segment, where
# the point location should be snapped to
accu_threshold <- c(400000, 300000, 100000, 50000, 10000, 5000, 1000, 500, 100) 
# Define the distance radius
dist_radius <- c(10, 20, 30)

# Create a temporary data.table
point_locations_tmp <- spdata_ids

# Note: The for loop takes about 9 minutes
first <- TRUE
for (idist in dist_radius) {
    
   # If the distance increases to 20 cells only a flow accumulation of 100 km²
   # will be used
   if (idist == 20) {
    # Set accu_threshold to 100
    accu_threshold <- c(100)
   }
  

  for (iaccu in accu_threshold) {
    # Snap point locations to the stream network
    point_locations_snapped_tmp <- snap_to_network(data = point_locations_tmp,
                                    id = "gbifID",
                                    lon = "longitude", lat = "latitude",
                                    stream_layer = stream_rast,
                                    accu_layer = flow_rast,
                                    method = "accumulation",
                                    distance = idist,
                                    accumulation = iaccu,
                                    quiet = FALSE)

    
    # Keep point location with NAs for the next loop
    point_locations_tmp <- point_locations_snapped_tmp %>% 
      filter(is.na(subc_id_snap_accu))
  
  if (first == TRUE) {
    # Keep the point locations with the new coordinates and remove rows with NA
    point_locations_snapped <- point_locations_snapped_tmp %>% 
    filter(!is.na(subc_id_snap_accu))
    first <- FALSE
  } else {
    # Bind the new data.frame to the data.frame of the loop before
    # and remove the NA
    point_locations_snapped <- point_locations_snapped %>% 
      bind_rows(point_locations_snapped_tmp) %>% 
      filter(!is.na(subc_id_snap_accu))
    
  }
  
  }
    
}
```

We can write out the result of the snapping

```{r, eval=F}
fwrite(point_locations_snapped, paste0(wdir, "/data/spdata_snapped.csv"), sep = ",", 
                      row.names = FALSE, quote = FALSE)
```

```{r, echo = F}
point_locations_snapped <- fread(paste0(wdir, "/data/spdata_snapped.csv"), sep = ",") %>% 
  head()
```

```{r, eval = F}
head(point_locations_snapped)
```

```{r, echo = FALSE}
knitr::kable(head(point_locations_snapped),
      caption = "The species data have been attributed new coordinates and in 
      some cases a new sub-catchment id (\"subc_id_snap_accu\")")
```

### Calculating distances between points

We will calculate the distance between all point locations. The
following chunks are computationally heavy, so we suggest to run them on
a server.

```{r, eval = FALSE}
# Load as graph
stream_graph <- read_geopackage(
  gpkg = paste0(study_area_dir, "/order_vect_segment_h18v02_crop.gpkg"),
  import_as = "graph")

# Get the network distance (in meters) between all input pairs.
# We provide the subcatchment ids of the snapped points to the argument "subc_id"
subc_distances <- get_distance_graph(stream_graph,
                            subc_id = point_locations_snapped$subc_id_snap_accu,
                            variable = "length",
                            distance_m = TRUE)

```

```{r, echo = F, eval = FALSE}
fwrite(subc_distances, 
        paste0(wdir, "/subc_distances.csv"), sep = ",", row.names = F, quote = F)
```

```{r, echo = F}
subc_distances <- fread(paste0(wdir, "/data/subc_distances.csv")) %>% head()
```

```{r, eval = F}
head(subc_distances)
```

```{r, echo =F}
kbl(subc_distances)
```

### Obtaining network centrality indices

We will now calculate centrality indices using the directed stream
network graph and the function `get_centrality` We want to consider only
the upstream connected segments, so we set `mode = "in".`

The following chunks are computationally heavy, so we suggest to run
them on a server.

```{r, eval = F}
centrality <- get_centrality(stream_graph, index = "all", mode = "in")
```

```{r, eval = F, echo = F}
fwrite(centrality, paste0(wdir, "/data/subc_centrality.csv"), 
       sep = ",", row.names = F, quote = F)
```

We now attach the centrality indices to the species data using the
subcatchment id

```{r, eval = F}
point_locations_snapped <- left_join(point_locations_snapped, centrality, 
                          by = c('subc_id_snap_accu'= 'subc_id'))
head(point_locations_snapped)
```

```{r, echo = F}
point_locations_snapped <- fread(paste0(wdir, "/data/spdata_centrality.csv")) %>% 
  head()
```

```{r, echo = F}
kbl(point_locations_snapped)
```

We can reclassify the stream segment and sub-catchment raster layers
based on the centrality value of each stream, using the function
`reclass_raster`. Essentially, we will create two new raster files, in
which each stream pixel will have a centrality value, instead of the
unique ID of the stream or sub-catchment.

```{r, eval = F}
reclass_raster(data = point_locations_snapped, 
              rast_val = "subc_id", 
              new_val = "betweeness",
              raster_layer = paste0(study_area_dir, "/segment_h18v02_crop.tif"),
              recl_layer =  paste0(study_area_dir,"/segment_h18v02_betweeness.tif"), 
              read = TRUE)

reclass_raster(data = point_locations_snapped, 
              rast_val = "subc_id", 
              new_val = "betweeness",
              raster_layer = paste0(study_area_dir, "/sub_catchment_h18v02_crop.tif"),
              recl_layer =  paste0(study_area_dir,"/sub_catchment_h18v02_betweeness.tif"), 
              read = TRUE)
```

Here's what the new segment and sub-catchment layers look like on QGIS,
when overlayed with the original layers. The yellow streams, or
sub-catchments, have high betweenness values, whereas the darker ones
have lower betweenness

![](../man/figures/reclass_segm_centrality.png)

![](../man/figures/reclass_subcs_centrality.png)

Bonus! If we want to include the centrality indices to the stream
network .gpkg, we can import the .gpkg and merge it with the centrality
table

```{r, eval = F}
# Load stream network .gpkg as vector
stream_vect <- read_geopackage(paste0(study_area_dir, "/order_vect_segment_h18v02_crop.gpkg"),
                               import_as = "SpatVect")

# Merge the centrality table with the vector
stream_vect <- terra::merge(stream_vect, spdata_centr, 
                        by.x = c('stream'), by.y="subc_id")

# Write out the stream gpkg including the centrality indices
writeVector(stream_vect, paste0(study_area_dir, "/order_vect_segment_centr.gpkg"))
```

### Extracting zonal statistics of topographic variables

We will calculate the zonal statistics of the Hydrography90m,
MERIT-HYDRO DEM and the CHELSA Bioclim variables for the sub-catchments
of our species points. Caution, don't increase the number of cores to
more than 3 as this can cause memory problems. However, this highly
depends to the number of sub-catchments as well. We recommend to test
the function with different parameters to find out what works best for
your case.

Let's first define the input `var_layers` for the `extract_zonal_stat`
function

```{r, eval = F}
list.files(study_area_dir)
var_layers <- c("bio1_1981-2010.tif", "bio12_1981-2010.tif",
                "bio15_1981-2010.tif", "elv_study_area_crop.tif",
                "outlet_dist_dw_basin_h18v02_crop.tif")
```

A good practice before aggregating the variables is to check their
NoData values:

```{r, eval = FALSE}
report_no_data(data_dir = study_area_dir, var_layer = var_layers)
```

```{r, eval = F}
#                                 Raster                  NoData
# 1                   bio1_1981-2010.tif                  -99999
# 2                  bio15_1981-2010.tif 3.40282346600000016e+38
# 3 outlet_dist_dw_basin_h18v02_crop.tif                   -9999
# 4                  bio12_1981-2010.tif                  -99999
# 5              elv_study_area_crop.tif                   -9999
```

```{r, eval = FALSE}
# Run the function that returns the zonal statistics.
# We provide the subcatchment ids of the fish points to the argument 'subc_id'.
stats_table_zon <- extract_zonal_stat(
                    data_dir = study_area_dir,
                    subc_layer = paste0(study_area_dir, "/sub_catchment_h18v02_crop.tif"),
                    subc_id = point_locations_snapped$subc_id_snap_accu,
                    var_layer = var_layers,
                    out_dir = paste0(wdir, "/data"),
                    file_name = "zonal_stats.csv",
                    n_cores = 3)
```

The function also reports the NoData values that are used in the
calculation of the zonal statistics of each variable.

We will keep only the mean and sd of each variable of the stats_table:

```{r, eval = F}
stats_table_zon <- stats_table_zon %>%
  dplyr::select(contains("subc") |  ends_with("_mean") | ends_with("_sd")) %>%
  rename('subcatchment_id' = 'subc_id')

```

The values in some of the original raster files were scaled, so we need
to re-scale them before proceeding to any analyses.

We define the following functions:

```{r, eval = F}
clim_scale <- function(x, na.rm = F) (x * 0.1)
offset <- function(x, na.rm = F) (x - 273.15)
```

... and apply them to rescale the values

```{r, eval = F}
stats_table_zon <- stats_table_zon  %>%
  mutate(across(starts_with("bio"), clim_scale))  %>%
  mutate(across(matches("bio1_.*_mean"), offset))
```

```{r, eval = F, echo = F}
fwrite(stats_table_zon, paste0(wdir, "/data/zonal_stats.csv"), 
       sep =",", quote=F, row.names=F)
```

```{r, eval=F}
head(stats_table_zon)
```

```{r, echo = F}
stats_table_zon <- fread(paste0(wdir, "/data/zonal_stats.csv")) %>% head()
kbl(stats_table_zon) %>% 
  kable_styling(bootstrap_options = "striped", 
                full_width = F, position = "left") %>%
  scroll_box(width = "500px", height = "200px")

```

### Putting together the final table

We will now perform some left joins to match the zonal statistics table
with the original species data.

```{r, eval = F}
data_fin <- left_join(point_locations_snapped, stats_table_zon, 
          by = c('subc_id_snap_accu'= 'subcatchment_id'))

data_fin$gbifID <- as.character(data_fin$gbifID)
data_fin <- left_join(spdata_ids, data_fin)

# Convert the gbifIDs back to the original values
data_fin$gbifID <- str_c("40", data_fin$gbifID)
spdata$gbifID <- as.character(spdata$gbifID)

data_fin <- left_join(spdata, data_fin)
```

```{r, eval=F, echo=F}
fwrite(data_fin, paste0(wdir, "/data/data_final.csv"), 
       sep = ",", quote = F, row.names = F)
```

```{r, eval = F}
head(data_fin)
```

```{r, echo = F}
data_fin <- fread(paste0(wdir, "/data/data_final.csv")) %>% head()
kbl(data_fin) %>% 
  kable_styling(bootstrap_options = "striped", 
                full_width = F, position = "left") %>%
  scroll_box(width = "500px", height = "200px")
```

### References
