---
title: "Case study"
subtitle: "Processing of spatial data before and after a Species Distribution Model"
author: ""
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Case study}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
#bibliography: literature.bib
link-citations: yes
linkcolor: blue
#csl: copernicus.csl

 
  
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,  
                      out.width='50%', fig.align='center')
knitr::opts_knit$set(root.dir = "/home/afroditi/Documents/PhD/hydrographr_case_study")
library(hydrographr)
library(terra)
library(tools)
library(data.table)
library(dplyr)
library(stringr)
library(ranger)
library(knitr)
library(kableExtra)
library(leaflet)
library(leafem)
```

### Introduction



Load the required libraries
```{r}
library(hydrographr)
library(terra)
library(tools)
library(data.table)
library(dplyr)
library(stringr)
library(ranger)
```



Define working directory
```{r}
wdir <- "/home/afroditi/Documents/PhD/hydrographr_case_study"
setwd(wdir)
```


Import dataset
```{r}
spdata <- fread(paste0(wdir, "/data/spdata.csv"), sep = "\t",
              keepLeadingZeros = TRUE, fill = TRUE)
spdata <- spdata[spdata$species=="Hypolestes_trinitatis",]
spdata <- spdata %>% select(c("occurrence_ID", "species", "longitude",
                              "latitude", "year", "elevation",
                              "WDPA_protection_status"))


head(spdata)

```

Visualise the species occurrences
```{r}
# Convert species data to a spatial vector object to plot the points
spdata_vect <- vect(spdata, geom=c("longitude", "latitude"))
```


```{r}
leaflet() %>% addTiles() %>% 
  addCircles(data = spdata_vect, color = "purple")
```

Get the ID of the tile where the points are located using the function get_tile_id. 
This function downloads and uses the auxiliary file that contains all the regional units globally.

```{r, eval = FALSE}
tile_id <- get_tile_id(data = spdata, lon = "longitude", lat = "latitude")
```
```{r, echo=FALSE}
tile_id <- c("h08v06", "h10v04", "h10v06")
```

```{r}
tile_id
```



Define the Hydrography90m variables for downloading.

```{r}
# Variables in raster format
vars_tif <- c("sub_catchment", "basin", "slope_curv_max_dw_cel")
# Variables in vector format
vars_gpkg <- c("order_vect_point")
```



```{r, eval = FALSE}
# Download the .tif tiles of the desired variables
download_tiles(variable = vars_tif, tile_id = tile_id, file_format = "tif",
              download_dir = "data")

# Download the .gpkg tiles of the desired variables
download_tiles(variable = vars_gpkg, tile_id = tile_id, file_format = "gpkg",
              download_dir = "data")
```


Download the CHELSA present and future bioclim variables

```{r}
# Create download directory
dir.create("data/chelsa_bioclim")
```



```{r, eval = FALSE}
# Extend timeout to 1000s to allow uninterrupted downloading
options(timeout = 1000)

# Download
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio12_1981-2010_V.2.1.tif",
destfile = "data/chelsa_bioclim/bio12_1981-2010.tif")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio15_1981-2010_V.2.1.tif",
destfile = "data/chelsa_bioclim/bio15_1981-2010.tif")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio1_1981-2010_V.2.1.tif",
destfile = "data/chelsa_bioclim/bio1_1981-2010.tif")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio12_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif",
destfile = "data/chelsa_bioclim/bio12_IPSL-CM6A-LR_ssp370_2041-2070.tif")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio15_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif",
destfile = "data/chelsa_bioclim/bio15_IPSL-CM6A-LR_ssp370_2041-2070.tif")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio1_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif",
destfile = "data/chelsa_bioclim/bio1_IPSL-CM6A-LR_ssp370_2041-2070.tif")
```

Define study area extent to crop all the layers
```{r}
bbox = c(-84.9749110583, 19.8554808619, -74.1780248685, 23.1886107447)
```

Crop the 2 tiles for all the variables to the bounding box of Cuba
```{r}
# Define directories containing the input layers
tile_dirs <- list.dirs(paste0(wdir, "/data"),
                       recursive = TRUE, full.names = TRUE)
tile_dirs <- tile_dirs[grep("tiles20d", tile_dirs)]
tile_dirs
```


Crop raster tiles. Store output in the same directory as the input files
```{r, eval = FALSE}
for(idir in tile_dirs) {
  # only choose rasters
  tiles <- list.files(idir, pattern = ".tif", full.names = TRUE)
  for(itile in tiles) {
      crop_to_extent(
        raster_layer = itile,
        bounding_box = bbox,
        out_dir = idir,
        file_name = paste0(str_remove(basename(itile), ".tif"), "_crop.tif"),
        read = FALSE,
        quiet = FALSE)
  }
}
```

Merge the cropped raster tiles
```{r}
# Define output directory for merged files
out_dir <- "data/tiles_crop_merged"
# Create the directory
dir.create(out_dir)
# Define the directories that only contain .tif files
tile_dirs_tif <- tile_dirs[-2]
tile_dirs_gpkg <- tile_dirs[2]
```

```{r, eval = FALSE}
# Merge the files in a loop
for(idir in tile_dirs_tif) {
  # Get input file extension
  file_extension <- file_ext(list.files(idir, full.names = FALSE)[1])

  # Assign file extension to output files
  ivar_name <- paste0(
    str_remove(basename(idir), "_tiles20d"), ".", file_extension
  )

  merge_tiles(tile_dir = idir,
              tile_names = list.files(idir, full.names = FALSE,
                                      pattern = "_crop.tif"),
              out_dir = out_dir,
              file_name = ivar_name,
              read = FALSE)
}
```



Extract the ids of the basins and sub-catchments where the points are located
```{r}
spdata_ids <- extract_ids(data = spdata, lon = "longitude", lat = "latitude",
                               id = "occurrence_ID", quiet = FALSE,
                               subc_layer = paste0(
                                wdir,"/data/tiles_crop_merged/sub_catchment.tif"
                               ),
                               basin_layer = paste0(
                                wdir, "/data/tiles_crop_merged/basin.tif"
                               ))
```

```{r}
knitr::kable(head(spdata_ids),
 caption = "The species data have now their corresponding basin and sub-catchment ids", )
```

Extract the zonal statistics of the hydrography90m variables
for all the sub-catchments of the study area.
Caution, don't increase the number of cores to more than 3

```{r}
# Define input var_layers for the extract_zonal_stat() function
var_layers <- list.files("data/tiles_crop_merged")[c(2:7,9)]
var_layers
```
```{r}
stats_table <- extract_zonal_stat(
                    data_dir = paste0(
                                wdir,"/data/tiles_crop_merged/"),
                   subc_layer = paste0(wdir,
                          "/data/tiles_crop_merged/sub_catchment.tif"),
                   subc_id = "all",
                   var_layer = var_layers,
                   file_name = paste0(wdir, "/data/zonal_stats.csv"),
                   n_cores = 1)

```
The function reports the NoData values that are used in the calculation of the zonal statistics of each variable.

```{r}
colnames(stats_table)
```
We will keep only the mean and sd of each variable of the stats_table, to use them as predictors in the species distribution model

```{r}
stats_table <- stats_table %>%
  select(contains(c("subc", "_mean", "_sd"))) %>%
  rename('subcatchment_id' = 'subc_id')

```



<!-- stats_table <- fread(paste0(wdir, "/data/zonal_stats.csv")) -->


```{r, echo = FALSE}
# Read in the .gpkg databases of the two tiles, filtering only the
# sub-catchments of the study area. These can be found in the stats_table

stats_gpkg_h08v06 <- read_geopackage(
  "data/r.stream.order/order_vect_tiles20d/order_vect_point_h08v06.gpkg",
  subc_id = stats_table$subcatchment_id) %>%
  rename('subcatchment_id' = 'stream')

stats_gpkg_h10v06 <- read_geopackage(
  "data/r.stream.order/order_vect_tiles20d/order_vect_point_h10v06.gpkg",
  subc_id = stats_table$subcatchment_id) %>%
  rename('subcatchment_id' = 'stream')
```

Then we join the two .gpkg databases and select desired columns.
We will keep the cumulative length of the stream and use it as an indicator of the sub-catchment size

```{r, echo = FALSE}
stats_gpkg <- rbind(stats_gpkg_h08v06, stats_gpkg_h10v06) %>%
  select(c("subcatchment_id", "cum_length"))

# Clear up memory
rm(stats_gpkg_h08v06, stats_gpkg_h10v06); gc()

```


```{r}
# Join the stats_table with the .gpkg database
stats_table <- left_join(stats_table, stats_gpkg)
```


```{r}
# Exclude NAs for now
stats_table <-stats_table[!is.na(stats_table$slope_curv_max_dw_cel_mean),]
```

<!-- # the order_vect_point id 422875962 is different than the subcatchment id in this case -->
<!-- # and slope gets NA -->
<!-- # prob <- read_geopackage("data/r.stream.order/order_vect_tiles20d/order_vect_point_h10v06.gpkg", -->
<!-- #                  subc_id = 422875962) %>% -->
<!-- #   rename('subcatchment_id' = 'stream') -->

<!-- # recl <- data.frame("old" = -->
<!-- #                      as.integer(c(422875962, -->
<!-- #                        stats_table$subcatchment_id[stats_table$subcatchment_id!=422875962] -->
<!-- #                        )), "new" =  as.integer(c(422875962,rep(-9999, 586640)))) -->

<!-- # reclass_raster( -->
<!-- #   data = recl, -->
<!-- #   rast_val = "old", -->
<!-- #   new_val = "new", -->
<!-- #   raster_layer = paste0(wdir, "/data/tiles_crop_merged/sub_catchment.tif"), -->
<!-- #   recl_layer = paste0(wdir, "/data/recl_prob.tif"), -->
<!-- #   read = FALSE, type = "Byte") -->

The values in the original raster files were scaled, therefore we need to re-scale
them before the modelling

We define the following rescaling functions:
```{r}
slope_scale <- function(x, na.rm = F) (x*0.000001)
clim_scale <- function(x, na.rm = F) (x * 0.1)
offset <- function(x, na.rm = F) (x - 273.15)
```

... and apply them to rescale the values
```{r}
stats_table <- stats_table  %>%
  mutate(across(contains("slope_curv_max_dw_cel_mean"), slope_scale)) %>%
  mutate(across(matches("bio[0-9]+.*_mean"), clim_scale))  %>%
  mutate(across(matches("bio1_.*_mean"), offset))
```

We split the dataset according to present and future bioclim variables
```{r}
stats_table_present <- stats_table %>%
  select(!contains("IPSL")) %>%
  rename(bio1_mean = bio1_1981.2010_mean,
         bio1_sd = bio1_1981.2010_sd,
         bio12_mean = bio12_1981.2010_mean,
         bio12_sd = bio12_1981.2010_sd,
         bio15_mean = bio15_1981.2010_mean,
         bio15_sd = bio15_1981.2010_sd)

stats_table_future <- stats_table %>%
  select(!contains("1981")) %>%
  rename(bio1_mean = bio1_IPSL.CM6A.LR_ssp370_2041.2070_mean,
         bio1_sd = bio1_IPSL.CM6A.LR_ssp370_2041.2070_sd,
         bio12_mean = bio12_IPSL.CM6A.LR_ssp370_2041.2070_mean,
         bio12_sd = bio12_IPSL.CM6A.LR_ssp370_2041.2070_sd,
         bio15_mean = bio15_IPSL.CM6A.LR_ssp370_2041.2070_mean,
         bio15_sd = bio15_IPSL.CM6A.LR_ssp370_2041.2070_sd)

# Clear up memory
rm(stats_table) ; gc()

```


Subset 10,000 random sub-catchments that will be used as pseudoabsences in the model.
Avoid sampling sub-catchments with known presences of the species


```{r}
pseudoabs <- stats_table_present %>%
  filter(!subcatchment_id %in% spdata_ids$subcatchment_id) %>%
  sample_n(10000)

head(pseudoabs)
```


Join presences with stats_table_present
```{r}
presence <- left_join(spdata_ids, stats_table_present, by = "subcatchment_id")
```

```{r}
# Join predictors of presences and of pseudoabsences to get the training data
# of the model
data_model <- rbind(presence, pseudoabs, fill = TRUE)

```

```{r}
# Define a binary column of presence-absence (0-1)
data_model$occurrence <- ifelse(!is.na(data_model$occurrence_ID), 1, 0)
# Convert to factor for random forest
data_model$occurrence <- as.factor(data_model$occurrence)

head(data_model)

```



```{r}
# Split data into train and test
set.seed(11)

#obtain stratified training sample
train_idx <- sample(nrow(data_model), 2/3 * nrow(data_model))
data_train <- data_model[train_idx, ]
data_test <- data_model[-train_idx, ]
```

Down-sampling method for presence-only data (Valavi et al., 2021)

```{r}
# Calculate sub-samples.
# number of presence records:
pres_num <- as.numeric(table(data_train$occurrence)["1"])
sample_size <- c("0" = pres_num/nrow(data_train),
                 "1" = pres_num/nrow(data_train))


```


Build the model using ranger random forest and the down-sampling method

```{r}
model <- ranger(data_train$occurrence ~ .,
                 data = data_train[, 6:14],
                 num.trees = 1000,
                 mtry=6,
                 replace = T,
                 sample.fraction = sample_size,
                 oob.error = T,
                 keep.inbag = TRUE,
                 num.threads = 4,
                 importance ='impurity',
                 probability=T)
model
```


Predict to the table with the future bioclims

```{r}
pred <- predict(model, data = stats_table_future[,!1])
```


Keep the probabilities of presence in each sub-catchment
```{r}
prediction <- data.table(subcatchment_id = stats_table_future$subcatchment_id,
                         pred_occur = as.numeric(pred$predictions[,2]))
head(prediction)
```

Create a table with reclassification rules to reclassify the sub-catchment
raster. Multiply the probability values by 100 to convert to integer,
which is a requirement of the reclass_raster function
```{r}
prediction$pred_occur <- as.integer(round(prediction$pred_occur, 2) * 100)

```

Reclassify the sub-catchment raster
```{r}
pred_raster <- reclass_raster(
  data = prediction,
  rast_val = "subcatchment_id",
  new_val = "pred_occur",
  raster_layer = paste0(wdir, "/data/tiles_crop_merged/sub_catchment.tif"),
  recl_layer = paste0(wdir, "/data/prediction.tif"),
  read = TRUE)

```



Let's plot the habitat suitability map, and add the presence points
```{r}

leaflet() %>% addTiles() %>%
    addGeotiff(
  file = paste0(wdir, "/data/prediction.tif"), opacity = 0.9,
   colorOptions = colorOptions(
                  palette = hcl.colors(256, palette = "inferno"),
                  na.color = "transparent"
                  )
  ) %>%
  leaflet::addCircles(data = spdata_vect, color = "turquoise") #%>%
   # addLegend(pal = hcl.colors(256, palette = "inferno"), values = values(pred_raster),
   #  title = "Habitat suitability map")

```

