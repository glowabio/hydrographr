---
title: "Case study - Cuba"
subtitle: "Processing of spatial data before and after a Species Distribution Model"
author: ""
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Case study}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
bibliography: bib/references_cuba.bib
link-citations: yes
linkcolor: blue
# csl: copernicus.csl
nocite: '@*'
editor_options:
  markdown:
    wrap: 72
---

```{r, include = FALSE, eval = FALSE}
# writes out the references for all packages
knitr::write_bib(file = 'packages.bib')
```

### Introduction

The following example showcases the functionality of the hydrographr
package along a species distribution modelling (SDM) workflow. We will
use an SDM to predict the suitable habitats of a dragonfly species in
Cuba. We will start by defining the regular tiles of the Hydrography90m
(Amatulli et al., 2022) where the occurrence points of the species are
located. Afterwards we will download the Hydrography90m and CHELSA
Bioclim (Karger & Zimmermann, 2019) layers of the predictor variables,
crop them and merge them, if necessary. Then, we will aggregate the
variable values within each sub-catchment of the study area, keeping as
predictors their mean and SD per sub-catchment. Using the random forest
(RF) algorithm, we will build a simple SDM and predict where the species
could potentially occur. Finally, we will reclassify the layer of all
the sub-catchments in the study area, creating a habitat suitability
map.

*Hypolestes trinitatis* is a damselfly species endemic to Cuba. The
species inhabits rivers and streams located in forest areas at the main
mountain ranges of eastern and central Cuba. The presence of riparian
forests as well as clean and well oxygenated water seems to be important
ecological conditions required by *H. trinitatis*. Larvae can be found
clinging to boulders and cobbles in the river bed at fast-flowing water
stream segments and pre-reproductive adults of both sexes and sexually
mature females remain most of the time in the vegetation of the riparian
forest where they find shelter and prey.

Let's get started!

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,  
                      out.width='50%', fig.align='center')
knitr::opts_knit$set(root.dir = "./data_cuba")
library(hydrographr)
library(terra)
library(tools)
library(data.table)
library(dplyr)
library(stringr)
library(ranger)
library(knitr)
library(kableExtra)
library(leaflet)
library(leafem)
library(htmlwidgets)
library(mapview)
```

Load required libraries

```{r}
library(hydrographr)
library(data.table)
library(dplyr)
library(terra)
library(tools)
library(stringr)
library(ranger)
library(leaflet)
library(leafem)
library(here)
```

Define working directory

```{r, eval=FALSE, include=FALSE}
wdir <- paste0(here(), "/vignettes/data_cuba")
if(!dir.exists(paste0(wdir, "/data"))) dir.create(paste0(wdir, "/data"))
```

```{r, eval=FALSE, include=FALSE}
wdir <- "/home/afroditi/Documents/PhD/hydrographr_case_study"
setwd(wdir)
dir.create("data_cuba")
```


```{r, eval = FALSE}
wdir <- "my/working/directory"
setwd(wdir)

# Create a new folder in the working directory to store all the data
dir.create("data_cuba")
```

### Species data

We will start by downloading a dataset including aquatic insect
occurrence records in Cuba from
<https://doi.org/10.18728/igb-fred-778.3> (Cambas & Salina, 2022).

```{r, eval = FALSE}
download.file("https://fred.igb-berlin.de/data_cuba/file_download/1395",
              destfile = paste0(wdir, "/data_cuba/spdata_cuba.csv"))

```

Import the dataset and select the occurrences of the species of
interest, sampled after the year 1980 (to match with the climate data)

```{r, eval = FALSE}
spdata <- fread(paste0(wdir, "/data/spdata_cuba.csv"), sep = "\t", fill = TRUE) %>%
          filter(species == "Hypolestes_trinitatis") %>%
          filter(year > 1980)

```

```{r include=FALSE}
knitr::opts_knit$set(root.dir = "./data_cuba")
spdata<- fread("./data/spdata_cuba.csv")
```


Subset columns

```{r}
spdata <- spdata %>%
  dplyr::select(c("occurrence_ID", "species", "longitude", "latitude", "year"))

```

```{r, eval = F}
spdata
```

```{r, echo=F}
kbl(spdata) %>%                        
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>%     
  scroll_box(width = "500px", height = "200px")
```

Let's visualise the species occurrences on the map

```{r}
# Convert species data to a spatial vector object to plot the points
spdata_vect <- vect(spdata, geom=c("longitude", "latitude"))
```

Let's define the extent (bounding box) of the study area (xmin, ymin,
xmax, ymax)

```{r}
bbox <- c(-84.9749110583, 19.8554808619, -74.1780248685, 23.1886107447)
```

```{r, include = FALSE, eval = FALSE}
m <- leaflet() %>%
  addProviderTiles('Esri.WorldShadedRelief') %>%
  setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%
  addCircles(data = spdata_vect, color = "purple")

saveWidget(m, file=paste0(wdir, "/cuba_map.html"))
```

```{r, eval = FALSE}
m <- leaflet() %>%
  addProviderTiles('Esri.WorldShadedRelief') %>%
  setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%
  addCircles(data = spdata_vect, color = "purple")

m
```

![](../man/figures/cuba_map.png)

### Download files

In order to download layers of the Hydrography90m, we need to know the
IDs of the 20°x20° tiles in which they are located. We can obtain these
IDs using the function *get_tile_id()*. This function downloads and uses
the auxiliary raster file that contains all the regional units globally,
and thus requires an active internet connection.

```{r, eval = FALSE}
tile_id <- get_tile_id(data = spdata, lon = "longitude", lat = "latitude")
```

```{r, echo = FALSE}
tile_id <- c("h08v06", "h10v04", "h10v06")
```

```{r}
tile_id
```

Currently the function returns all the tiles of the regional unit where
the input points are located. However, some of them may be far from the
study area and hence not always needed in further steps. Please double
check which tile IDs are relevant for your purpose using the **Tile
map** found
[here](https://hydrography.org/hydrography90m/hydrography90m_layers/).

In our case, Cuba spreads across two tiles, which hold the IDs "h08v06"
and "h10v06", so we will keep only these two.

```{r}
tile_id <- tile_id[c(1,3)]
```

Let's define the Hydrography90m variables that we would like to
download. We will use the variables
[slope_curv_max_dw_cel](https://geo.igb-berlin.de/maps/new?layer=geonode:hydrography90m_v1_slope_curv_max_dw_cel_cog&view=True)
(maximum curvature between the highest upstream, focal and downstream
cell) and
[spi](https://geo.igb-berlin.de/maps/new?layer=geonode:hydrography90m_v1_spi_cog&view=True)
(Stream Power Index) as predictors in our model. Additionally, we will
download the
[sub_catchment](https://geo.igb-berlin.de/maps/new?layer=geonode:hydrography90m_v1_sub_catchment_cog&view=True)
layer, which is the necessary base-layer for the workflow.

A list of all available Hydrography90m variables, as well as details and
visualisations are available
[here](https://hydrography.org/hydrography90m/hydrography90m_layers/).

```{r, eval = FALSE}
# Variables in raster format
vars_tif <- c("sub_catchment", "slope_curv_max_dw_cel", "spi")
# Variables in vector format
vars_gpkg <- c("order_vect_point")
```

```{r, eval = FALSE}
# Download the .tif tiles of the desired variables
download_tiles(variable = vars_tif, tile_id = tile_id, file_format = "tif",
              download_dir = "data_cuba")

# Download the .gpkg tiles of the desired variables
download_tiles(variable = vars_gpkg, tile_id = tile_id, file_format = "gpkg",
              download_dir = "data_cuba")
```

Then download the CHELSA present and future Bioclim variables

```{r, eval = FALSE}
# Create download directory
dir.create(paste0(wdir, "/data_cuba/chelsa_bioclim"))
```

```{r, eval = FALSE}
# Extend timeout to 1000s to allow uninterrupted downloading
options(timeout = 1000)

# Download
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio12_1981-2010_V.2.1.tif",
destfile = "data_cuba/chelsa_bioclim/bio12_1981-2010.tif")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio15_1981-2010_V.2.1.tif",
destfile = "data_cuba/chelsa_bioclim/bio15_1981-2010.tif")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio1_1981-2010_V.2.1.tif",
destfile = "data_cuba/chelsa_bioclim/bio1_1981-2010.tif")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio12_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif",
destfile = "data_cuba/chelsa_bioclim/bio12_IPSL-CM6A-LR_ssp370_2041-2070.tif")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio15_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif",
destfile = "data_cuba/chelsa_bioclim/bio15_IPSL-CM6A-LR_ssp370_2041-2070.tif")
download.file("https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio1_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif",
destfile = "data_cuba/chelsa_bioclim/bio1_IPSL-CM6A-LR_ssp370_2041-2070.tif")

```

### Cropping

After having downloaded all the layers, we need to crop them to the
extent of our study area. This is straightforward in the case of the
global CHELSA layers:

We define the directory containing the layers to be cropped

```{r, eval = FALSE}
dirs_chelsa <- list.dirs(paste0(wdir, "/data/chelsa_bioclim"),
                       recursive = TRUE, full.names = TRUE)
```

... and the final output directory

```{r, eval = FALSE}
# Define output directory for merged files
layer_dir <- paste0(wdir, "/data_cuba/final_layers")
# Create the directory
dir.create(layer_dir)
```

We then crop the files using the function *crop_to_extent()* in a loop

```{r, eval = FALSE}
files_chelsa <- list.files(dirs_chelsa, pattern = ".tif", full.names = TRUE)

for(ifile in files_chelsa) {
    crop_to_extent(
      raster_layer = ifile,
      bounding_box = bbox,
      out_dir = layer_dir,
      file_name = basename(ifile),
      read = FALSE,
      quiet = TRUE)
}

```

However, the hydrographic variable *slope_curv_max_dw_cel* (the maximum
curvature between the highest upstream, the focal, and the downstream
cell) is split across two tiles. We highly recommend to first crop the
tiles to the extent of the study area to limit their size, and
afterwards merge them into one file.

Again, we will first define the input directories, but this time we will
store the cropped files in the same directory as the input files, as
there is an extra step before reaching to the final output.

```{r, eval = FALSE}
dirs_h90m <- list.dirs(paste0(wdir, "/data"),
                       recursive = TRUE, full.names = TRUE)

dirs_h90m <- dirs_h90m[grep("tiles20d", dirs_h90m)]
```

```{r, eval = FALSE}
for(idir in dirs_h90m) {
  # only choose rasters
  tiles <- list.files(idir, pattern = ".tif$", full.names = TRUE)
  for(itile in tiles) {
      crop_to_extent(
        raster_layer = itile,
        bounding_box = bbox,
        out_dir = idir,
        file_name = paste0(str_remove(basename(itile), ".tif"), "_crop.tif"),
        read = FALSE,
        quiet = TRUE)
  }
}
```


### Merging

The cropped tiles of the variable now need to be merged together.

```{r, eval = FALSE}
# Merge the files in a loop
for(idir in dirs_h90m) {
  # Get input file extension
  file_extension <- file_ext(list.files(idir, full.names = FALSE)[1])

  # Assign file extension to output files
  ivar_name <- paste0(
    str_remove(basename(idir), "_tiles20d"), ".", file_extension
  )

  # Run the function
  merge_tiles(tile_dir = idir,
              tile_names = list.files(idir, full.names = FALSE,
                                      pattern = "_crop.tif"),
              out_dir = layer_dir,
              file_name = ivar_name,
              read = FALSE)
}
```

### Extraction of sub-catchment IDs

Extract the IDs of the sub-catchments where the points are located. This
step is crucial, as many of the functions that we will later use require
a vector of sub-catchment IDs as input. Note that the function
*extract_ids()* can be used to extract the values at specific points of
any raster file provided to the argument *subc_layer*. It can be safely
used to query very large raster files, as these are not loaded into R.

```{r, eval = FALSE}
spdata_ids <- extract_ids(data = spdata, lon = "longitude", lat = "latitude",
                          id = "occurrence_ID", quiet = FALSE,
                          subc_layer = paste0(layer_dir, "/sub_catchment.tif"))

```

```{r include=FALSE}
knitr::opts_knit$set(root.dir = "./data_cuba")
spdata_ids <- fread("./data/spdata_ids.csv")

```

```{r, echo = FALSE}
knitr::kable(head(spdata_ids),
             caption = "The species data have now their corresponding sub-catchment ids")
```


### Aggregation of environmental layers

We will calculate the zonal statistics of the Hydrography90m and the
CHELSA Bioclim variables for all the sub-catchments of the study area.
Caution, don't increase the number of cores to more than 3 as this can
cause memory problems. However, this highly depends to the number of
sub-catchments as well. We recommend to test the function with different
parameters to find out what works best for your case.

```{r, eval = FALSE}
# Define input var_layers for the extract_zonal_stat() function
var_layers <- list.files(layer_dir)[-9]
# var_layers <- list.files(layer_dir)[c(3,5,7)]
var_layers
```

```{r}
## [1] "bio1_1981-2010.tif"                     
## [2] "bio1_IPSL-CM6A-LR_ssp370_2041-2070.tif"
## [3] "bio12_1981-2010.tif"                    
## [4] "bio12_IPSL-CM6A-LR_ssp370_2041-2070.tif"
## [5] "bio15_1981-2010.tif"                    
## [6] "bio15_IPSL-CM6A-LR_ssp370_2041-2070.tif"
## [7] "slope_curv_max_dw_cel.tif"              
## [8] "spi.tif"
```

A good practice before aggregating the variables is to check their
NoData values:

```{r, eval = FALSE}
report_no_data(data_dir = layer_dir, var_layer = var_layers)
```

```{r}
##                                    Raster   NoData
## 1                     bio15_1981-2010.tif        0
## 2                      bio1_1981-2010.tif        0
## 3  bio1_IPSL-CM6A-LR_ssp370_2041-2070.tif        0
## 4                     bio12_1981-2010.tif        0
## 5 bio12_IPSL-CM6A-LR_ssp370_2041-2070.tif        0
## 6 bio15_IPSL-CM6A-LR_ssp370_2041-2070.tif        0
## 7               slope_curv_max_dw_cel.tif -9999999
## 8                                 spi.tif -9999999
```


```{r, eval = FALSE}
# Run the function that returns the zonal statistics
stats_table_zon <- extract_zonal_stat(
                    data_dir = layer_dir,
                    subc_layer = paste0(layer_dir, "/sub_catchment.tif"),
                    subc_id = "all",
                    var_layer = var_layers,
                    file_name = paste0(wdir, "/data_cuba/zonal_stats.csv"),
                    n_cores = 2)
```


The function also reports the NoData values that are used in the
calculation of the zonal statistics of each variable.

```{r, eval = FALSE}
# Read the previously created file
stats_table_zon <- fread(paste0(wdir, "/data_cuba/zonal_stats.csv"))
```

```{r, include=FALSE}
knitr::opts_knit$set(root.dir = "./data_cuba")
stats_table_zon <- fread("./data/zonal_stats.csv")

```

Let's inspect the resulting table

```{r, include=FALSE}
head(stats_table_zon)
```

```{r, echo = FALSE}
kbl(head(stats_table_zon)) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>%
  scroll_box(width = "800px", height = "200px")
```

```{r}
colnames(stats_table_zon)
```

We will keep only the mean and sd of each variable of the stats_table,
to use them as predictors in the species distribution model

```{r}
stats_table_zon <- stats_table_zon %>%
  dplyr::select(contains(c("subc", "_mean", "_sd"))) %>%
  rename('subcatchment_id' = 'subc_id')

```

### Reading vector files

Read in the .gpkg databases of the two tiles, filtering only the
sub-catchments of the study area. Their IDs can be retrieved from the
stats_table

```{r, eval = FALSE}
stats_gpkg_h08v06 <- read_geopackage(
  "data_cuba/r.stream.order/order_vect_tiles20d/order_vect_point_h08v06.gpkg",
  subc_id = stats_table_zon$subcatchment_id) %>%
  rename('subcatchment_id' = 'stream')

stats_gpkg_h10v06 <- read_geopackage(
  "data_cuba/r.stream.order/order_vect_tiles20d/order_vect_point_h10v06.gpkg",
  subc_id = stats_table_zon$subcatchment_id) %>%
  rename('subcatchment_id' = 'stream')
```

```{r, include = FALSE, eval = FALSE}
# We used these lines to write out the filtered .gpkg files as data.tables,
# so that the size of data in the repository is smaller
fwrite(stats_gpkg_h08v06, sep = ",", row.names = FALSE, quote = FALSE,
  paste0(wdir,
  "/data_cuba/r.stream.order/order_vect_tiles20d/order_vect_point_h08v06.csv"))
fwrite(stats_gpkg_h10v06, sep = ",", row.names = FALSE, quote = FALSE,
  paste0(wdir,
  "/data_cuba/r.stream.order/order_vect_tiles20d/order_vect_point_h10v06.csv"))

```


```{r, eval = FALSE}
# These are the lines that actually run.
# They read the pre-filtered gpkg which was exported as a csv in the previous
# step. The result is the same as if we ran "read_geopackage" in the .gpkg files
stats_gpkg_h08v06 <- fread(paste0(wdir,
  "/data_cuba/r.stream.order/order_vect_tiles20d/order_vect_point_h08v06.csv"),
  sep = ",")

stats_gpkg_h10v06 <- fread(paste0(wdir,
  "/data_cuba/r.stream.order/order_vect_tiles20d/order_vect_point_h10v06.csv"),
  sep = ",")
```

```{r include=FALSE}
knitr::opts_knit$set(root.dir = "./data_cuba")
stats_gpkg_h08v06  <- fread("./data/r.stream.order/order_vect_tiles20d/order_vect_point_h08v06.csv")
stats_gpkg_h10v06 <- fread("./data/r.stream.order/order_vect_tiles20d/order_vect_point_h10v06.csv", sep = ",")
```

Then we join the two .gpkg databases and select the columns
"subcatchment_id" and "length". We will use the length of the stream as
an indicator of the sub-catchment size

```{r}
stats_gpkg <- rbind(stats_gpkg_h08v06, stats_gpkg_h10v06) %>%
  dplyr::select(c("subcatchment_id", "length"))

# Clear up memory
rm(stats_gpkg_h08v06, stats_gpkg_h10v06); gc()

```


### Prepare data for modelling

Join the stats_table with the .gpkg database

```{r}
stats_table <- left_join(stats_table_zon, stats_gpkg, by = "subcatchment_id")
```

The values in the original raster files were scaled, so we need to
re-scale them before the modelling

We define the following functions:

```{r}
slope_scale <- function(x, na.rm = F) (x * 0.000001)
clim_scale <- function(x, na.rm = F) (x * 0.1)
offset <- function(x, na.rm = F) (x - 273.15)
```

... and apply them to rescale the values

```{r}
stats_table <- stats_table  %>%
  mutate(across(contains("slope_curv_max_dw_cel"), slope_scale)) %>%
  mutate(across(starts_with("bio"), clim_scale))  %>%
  mutate(across(matches("bio1_.*_mean"), offset))
```

```{r, eval = FALSE}
head(stats_table)
```

```{r, echo = FALSE}
kbl(head(stats_table)) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>%      
  scroll_box(width = "800px", height = "200px")
```


<!-- Check if there are NAs in the rows of the table -->

<!-- ```{r, eval = FALSE} -->

<!-- head(stats_table[is.na(stats_table$length),]) -->

<!-- ``` -->

<!-- ```{r, echo = FALSE} -->

<!-- kbl(head(stats_table[is.na(stats_table$length), ])) %>%                         -->

<!--   kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>%      -->

<!--   scroll_box(width = "800px", height = "200px") -->

<!-- ``` -->

<!-- There are streams with NA in the length column. Should we convert them -->

<!-- to 0? -->

<!-- ```{r} -->

<!-- stats_table$length[is.na(stats_table$length)] <- 0 -->

<!-- ``` -->


```{r, eval = FALSE}
head(stats_table[is.na(stats_table$slope_curv_max_dw_cel_mean),])
```

```{r, echo = FALSE}
kbl(head(stats_table[is.na(stats_table$slope_curv_max_dw_cel_mean), ])) %>%                        
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>%     
  scroll_box(width = "800px", height = "200px")
```

```{r, eval = FALSE}
head(stats_table[is.na(stats_table$spi_mean),])
```

```{r, echo = FALSE}
kbl(head(stats_table[is.na(stats_table$spi_mean),])) %>%                        
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>%     
  scroll_box(width = "800px", height = "200px")
```

If we check the IDs of the sub-catchments that have null values for both
variables, we observe that they are identical:

```{r}
stats_table[is.na(stats_table$spi_mean),]$subcatchment_id == stats_table[is.na(stats_table$slope_curv_max_dw_cel_mean),]$subcatchment_id
```

This happens because the variables are not defined for a few
single-pixel sub-catchments, which are outlets. We will remove these
sub-catchments, as the model will not be able to handle the no data
values.

```{r}
stats_table <- stats_table[!is.na(stats_table$slope_curv_max_dw_cel_mean),]
```

We split the dataset into two datasets, according to present and future
Bioclim variables. The first will be used in the training of the model
and the second in the prediction of future suitable habitats

```{r}
stats_table_present <- stats_table %>%
  dplyr::select(!contains("IPSL")) %>%
  rename(bio1_mean = bio1_1981.2010_mean,
         bio1_sd = bio1_1981.2010_sd,
         bio12_mean = bio12_1981.2010_mean,
         bio12_sd = bio12_1981.2010_sd,
         bio15_mean = bio15_1981.2010_mean,
         bio15_sd = bio15_1981.2010_sd)

stats_table_future <- stats_table %>%
  dplyr::select(!contains("1981")) %>%
  rename(bio1_mean = bio1_IPSL.CM6A.LR_ssp370_2041.2070_mean,
         bio1_sd = bio1_IPSL.CM6A.LR_ssp370_2041.2070_sd,
         bio12_mean = bio12_IPSL.CM6A.LR_ssp370_2041.2070_mean,
         bio12_sd = bio12_IPSL.CM6A.LR_ssp370_2041.2070_sd,
         bio15_mean = bio15_IPSL.CM6A.LR_ssp370_2041.2070_mean,
         bio15_sd = bio15_IPSL.CM6A.LR_ssp370_2041.2070_sd)

# Clear up memory
# rm(stats_table) ; gc()

```


The classification model needs at least two classes, in this case
presences and absences. We will sample 10,000 random sub-catchments that
will be used as pseudoabsences in the model. The filtering that takes
place before the sampling assures that we will avoid sampling
sub-catchments with known presences of the species

```{r}
pseudoabs <- stats_table_present %>%
  filter(!subcatchment_id %in% spdata_ids$subcatchment_id) %>%
  sample_n(10000)
```

```{r, eval = FALSE}
head(pseudoabs)
```

```{r, echo = FALSE}
kbl(head(pseudoabs)) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>%      
  scroll_box(width = "800px", height = "200px")
```

We join the species occurrences with the present environmental variables
table

```{r}
presence <- left_join(spdata_ids, stats_table_present, by = "subcatchment_id")
```

We then join the predictors of the presences and those of the
pseudoabsences, to obtain the modelling data table

```{r}
data_model <- data.table::rbindlist(list(presence, pseudoabs), fill = TRUE)
```

Define a binary column of presence-absence (0-1) and set it to factor

```{r}
data_model$occurrence <- ifelse(!is.na(data_model$occurrence_ID), 1, 0)
data_model$occurrence <- as.factor(data_model$occurrence)
```

```{r, eval = FALSE}
head(data_model)
```

```{r, echo = FALSE}
kbl(head(data_model)) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>%      
  scroll_box(width = "800px", height = "200px")
```


In this step, there are two alternatives: Either we split the data into
train and test sets:

```{r}
set.seed(17)
```

```{r}
#obtain stratified training sample
train_idx <- sample(nrow(data_model), 2/3 * nrow(data_model))
data_train <- data_model[train_idx, ]
data_test <- data_model[-train_idx, ]
```

...or we use the whole dataset as the training set, since RF performs
intrinsic evaluation in every tree

```{r}
data_train <- data_model
```


### A simple SDM using Random Forest

We will use the ranger random forest (RF) algorithm (Wright & Ziegler,
2017) to build a species distribution model. Random forest can handle
huge quantities of data and is therefore scalable to larger datasets. As
our dataset is highly imbalanced towards the pseudo-absence data, we
will apply the down-sampling method for presence-only data in the
modelling process (Valavi et al., 2021). Down-sampling works by
balancing the training data, which contain significantly more absence
that presence points. Following this approach, the classification-RF
model will use the same number of pseudo-absences as presence points in
each classification tree, by sampling with replacement (bootstrapping)
from the full pseudo-absence set (Valavi et al., 2021).

Let's define the sample size of each classification tree as a proportion
of the whole training set, based on the number of presences.

```{r}
# number of presence records:
pres_num <- as.numeric(table(data_train$occurrence)["1"])
sample_size <- c("0" = pres_num / nrow(data_train),
                 "1" = pres_num / nrow(data_train))
sample_size
```

Run and inspect the model.

```{r}
model <- ranger(data_train$occurrence ~ .,
                 data = data_train[, 6:14],
                 num.trees = 1000,
                 mtry= 6,
                 replace = T,
                 sample.fraction = sample_size,
                 oob.error = T,
                 keep.inbag = T,
                 num.threads = 4,
                 importance ='impurity',
                 probability = T)

model
```

The model can be evaluated based on the OOB prediction error. This
metric lies on the fact that the data points that are not used in
training a tree can be used to test the tree. The errors on the OOB
samples are called the **out-of-bag errors**.


### Model prediction

We will now predict to the table of all the sub-catchments across Cuba,
including the future Bioclim variables.

```{r}
pred <- predict(model, data = stats_table_future[,!1])
```

We will now reclassify the sub-catchment raster based on the predicted
probabilities of occurrence, to visualise the potential suitable
habitats of the species. For this purpose we will use the
'reclass_raster' function. The function requires a table with the
reclassification rules (i.e., subc_id = predicted_occurrence), the
values of which need to be integers.

First, let's join the probabilities of presence in each sub-catchment
with the corresponding sub-catchment IDs, to create a table with
reclassification rules

```{r}
prediction <- data.table(subcatchment_id = stats_table_future$subcatchment_id,
                         pred_occur = as.numeric(pred$predictions[,2]))
head(prediction)
```

and multiply the probability values by 100

```{r}
prediction$pred_occur <- as.integer(round(prediction$pred_occur, 2) * 100)
```

We can now reclassify the sub-catchment raster

```{r, eval = FALSE}
reclass_raster(
  data = prediction,
  rast_val = "subcatchment_id",
  new_val = "pred_occur",
  raster_layer = paste0(layer_dir, "/sub_catchment.tif"),
  recl_layer = paste0(wdir, "/prediction.tif"),
  read = FALSE)
```

Let's plot the future habitat suitability map, and add the presence
points

```{r, eval = FALSE}
# Define colour palette
num_pal <- colorNumeric(
  viridisLite::inferno(256)
  , domain = prediction$pred_occur
  , na.color = "transparent"
)

p <- leaflet() %>% addTiles() %>%
  setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%
    leafem::addGeotiff(
  file = paste0(wdir, "/prediction.tif"), opacity = 1,
   colorOptions = colorOptions(
                  palette = hcl.colors(256, palette = "inferno"),
                  na.color = "transparent"
                  ) # read external raster file without loading it to R
  ) %>%
  leaflet::addCircles(data = spdata_vect, color = "turquoise", stroke = TRUE,
                      weight = 5, opacity = 1) %>% # add points
  addLegend(pal = num_pal,
           values = prediction$pred_occur,
           labels = palette(),
           title = "Future habitat suitability map</br>for Hypolestes trinitatis",
           position = "bottomleft", opacity = 1, labFormat = labelFormat(suffix = "%"))  # add a legend

p
```

![](../man/figures/prediction.png){width="800"}

```{r, include = FALSE, eval = FALSE}
saveWidget(p, paste0(wdir, "/prediction.html"))
```

If we zoom in to the habitat suitability map, we can observe the
sub-catchments that could offer a suitable habitat to the species in a
more detailed view.

![](../man/figures/prediction_zoom.png){width="800"}

### References
