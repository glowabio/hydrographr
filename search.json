[{"path":"/articles/benchmark.html","id":"introduction-and-scope","dir":"Articles","previous_headings":"","what":"Introduction and scope","title":"Function benchmarks","text":"main focus hydrographr process Hydrography90m (Amatulli et al. 2022) data include workflows R. Nevertheless, hydrographr provides geospatial functions can useful working geospatial data general, goal computationally RAM usage efficient. Benchmark tests selected hydrographr functions demonstrate capabilities R package. compared tested functions corresponding workflows employ terra package put performance perspective without claiming superiority either one R packages . focused five common geospatial tasks can easily implemented hydrographr terra: Merging multiple raster layers one file. Cropping raster layer bounding box extent. Cropping raster layer irregular boundary vector polygon. Reclassifying values raster layer. Zonal statistics raster layer. benchmark experiment tasks consisted reading input data, processing data, writing results back disk (output raster layer). experiment recorded total run-time entire task using function microbenchmark::microbenchmark. peak RAM usage estimated observing task manager (Windows) top (Linux). ensure reliable estimates total run-time, repeated experiment three times. hydrographr requires different implementations Windows Linux systems. Therefore performed experiments two systems, Windows laptop computer (Windows 11 Pro 10.0.22621/Intel i5-8350U/16 GB RAM) Laptop running Ubuntu Linux (Ubuntu 20.04.6 LTS/Intel i5-10310U/16 GB RAM) identify major performance differences operating systems. following chapter summarizes results benchmark experiments performed two different systems. results summary documented code individual geospatial tasks reproducability. implemented hydrographr terra functions default settings acknowledge specific input arguments may improve performance. Nevertheless benchmark experiments provide first general performance overview.","code":""},{"path":"/articles/benchmark.html","id":"results-of-benchmark-experiments","dir":"Articles","previous_headings":"","what":"Results of benchmark experiments","title":"Function benchmarks","text":"benchmark shows hydrographr::merge_tiles overall merged six tiles faster using less RAM compared terra tiles loaded R merged terra::merge written hard drive. average, performance hydrographr::merge_tiles surpassed terra Windows Ubuntu, completing task 2.5 times faster (179 compared 457 seconds) 4 times faster (158 compared 623 seconds), respectively. Additionally, terra required Windows five times amount RAM perform merge operation compared hydrographr (0.84 compared 4.24 GB). tasks, cropping raster extent bounding box boundary vector polygon, terra::crop outperforms hydrographr::crop_to_extent experiments Windows system, performing tasks 3.5 times (130 compared 454 seconds) 2.5 times (225 compared 571 seconds) faster. differences Ubuntu system less significant. Particularly, Windows system, terra used 4.5 times (4.43 compared 0.97 GB) 3.4 times (3.53 compared 1.05 GB) RAM perform cropping task. reclassification task performed sub-catchments entire Amazon basin, reassigning random integer values 31 million sub-catchment IDs, thus requiring rather large reclassification table. hydrographr::reclass_raster able perform task 179 (Windows) 116 seconds (Ubuntu), respectively, aborted terra::classify 12 hours run-time progress approximately 10%. assess performance computing zonal statistics using hydrographr::extract_zonal_stat, implemented terra::zonal calculate mean standard deviation flow accumulation raster layer, ca. 31 million sub-catchments Amazon basin used zones. hydrographr::extract_zonal_stat returns following nine statistical measures default single run: number cells within zone, minimum maximum cell values, range, arithmetic mean, population variance, standard deviation, coefficient variation, sum (provided underlying r.univar function GRASS GIS). Hence, several statistical measures interest, run-times individual terra::zonal executions must added comparison. calculation means 2.6 times faster performed terra::zonal Windows 247 compared 645 seconds) Ubuntu (197 compared 520 seconds). RAM usage fairly large calculating zonal statistics hydrographr, 9.38 GB Windows 6.96 GB Linux, mean calculation terra used 2.68 5.39 GB, respectively. standard deviation included hydrographr output table, terra::zonal complete task calculating standard deviation ran memory.","code":""},{"path":[]},{"path":"/articles/benchmark.html","id":"r-packages-and-paths","dir":"Articles","previous_headings":"Benchmark experiments","what":"R packages and paths","title":"Function benchmarks","text":"Load required libraries: Function definitions: Define working directory:","code":"library(hydrographr) library(terra) library(microbenchmark) library(readr) # Small helper function to print benchmark experiments print_experiment <- function(micro) {   cat(\"Run times in seconds:\\n\")   t <- round(sort(micro$time*1e-9), digits = 1)   names(t) <- c(\"min\", \"median\", \"max\")   t } # Define the working directory for the benchmark experiments wdir <- \"my/working/directory/benchmark\"  data_dir <- paste0(wdir, \"/data\") out_dir  <- paste0(wdir, \"/output\")  # Create a new folder in the working directory to store all the data dir.create(data_dir) dir.create(out_dir)"},{"path":"/articles/benchmark.html","id":"loading-and-preparing-hydrography90m-test-data","dir":"Articles","previous_headings":"Benchmark experiments","what":"Loading and preparing Hydrography90m test data","title":"Function benchmarks","text":"selected Amazon river basin test case, covers six Hydrography90m tiles total. required tile ids download defined tile_id. calculation zonal statistics want perform flow accumulation. also download respective layer tiles Hydrography90m (defined vars_tif). also load Amazon \"basin\" boundary vector layer use cropping task. download Hydrography90m data can use hydrographr function download_tiles shown . also define extent bounding box Amazon basin. bounding box used benchmark test cropping, also preparation flow accumulation layer. use zonal statistics calculation merge downloaded flow accumulation tiles hydrographr function merge_tilesand crop merged layer defined bounding box crop_to_extent.","code":"# Define tile IDs tile_id <-  c(\"h10v06\",\"h10v08\", \"h10v10\", \"h12v06\", \"h12v08\", \"h12v10\") # Variables in raster format vars_tif <- c(\"sub_catchment\", \"accumulation\") # Variables in vector format vars_gpkg <- \"basin\"  # Download the .tif tiles of the desired variables download_tiles(variable = vars_tif, tile_id = tile_id, file_format = \"tif\",                download_dir = data_dir)  # Download the .gpkg tiles of the desired variables download_tiles(variable = vars_gpkg, tile_id = tile_id, file_format = \"gpkg\",                download_dir =   paste0(wdir, \"/data\")) bbox <- c(-79.6175000, -20.4991667, -50.3400000, 5.2808333) # Path where the flow accumulation tiles where loaded to. acc_dir <- paste0(data_dir, \"/r.watershed/accumulation_tiles20d/\") # List all flow accumulation tiles in the folder. acc_tifs <- list.files(acc_dir)  # Merge the tiles to one flow accumulation layer. merge_tiles(tile_dir = acc_dir,             tile_names = acc_tifs,             out_dir = acc_dir,             file_name = \"acc_merge.tif\")  crop_to_extent(raster_layer = paste0(acc_dir, \"acc_merge.tif\"),                bounding_box = bbox,                out_dir = acc_dir,                file_name =  \"acc_merge_crop.tif\")"},{"path":"/articles/benchmark.html","id":"experiment-1-merging-tiles","dir":"Articles","previous_headings":"Benchmark experiments","what":"Experiment 1: Merging tiles","title":"Function benchmarks","text":"first benchmark experiment merge 6 tiles cover Amazon basin “.tif” file. run time entire workflow evaluated, includes loading merging input tiles writing outputs back hard drive. tile files (tile_tifs) loaded data folder now define tile_dir. Merging tiles hydrographr performed function merge_tiles. output layer saved out_dir. default compression level (compression) \"low\", results already relatively small output files compromising run time much. tested terra workflow involves reading 6 sub-catchment tiles function rast, merging merge writing result layer writeRaster.","code":"# Folder where the sub-catchment tiles were downloaded to. tile_dir   <- paste0(data_dir, \"/r.watershed/sub_catchment_tiles20d\") # List all tile names in tile_dir tile_tifs  <- list.files(tile_dir, pattern = \".tif$\") # Run the benchmark test 3 times and save the run times in t_merge_hydr t_merge_hydr <- microbenchmark({   merge_tiles(tile_dir = tile_dir,               tile_names = tile_tifs,               out_dir = out_dir,               file_name = \"merge_hydr.tif\",               compression = \"low\") }, times = 3)  # Print the run times print_experiment(t_merge_hydr) ## Run times in seconds: ##    min median    max  ##  160.5  179.4  190.3 # Run the benchmark test 3 times and save the run times in t_merge_terra t_merge_terra <- microbenchmark({   r1 <- rast(paste0(tile_dir, \"/\", tile_tifs[1]))   r2 <- rast(paste0(tile_dir, \"/\", tile_tifs[2]))   r3 <- rast(paste0(tile_dir, \"/\", tile_tifs[3]))   r4 <- rast(paste0(tile_dir, \"/\", tile_tifs[4]))   r5 <- rast(paste0(tile_dir, \"/\", tile_tifs[5]))   r6 <- rast(paste0(tile_dir, \"/\", tile_tifs[6]))   r_merge <- merge(r1, r2, r3, r4, r5, r6)   writeRaster(r_merge, paste0(out_dir, \"/merge_terra.tif\"), overwrite = TRUE) }, times = 3)  # Print the run times print_experiment(t_merge_terra) ## Run times in seconds: ##    min median    max  ##  454.9  457.4  457.4"},{"path":"/articles/benchmark.html","id":"experiment-2-cropping-to-bounding-box","dir":"Articles","previous_headings":"Benchmark experiments","what":"Experiment 2: Cropping to bounding box","title":"Function benchmarks","text":"following experiment merged layer “merge_hydr.tif” cropped defined bounding box Amazon basin. Cropping hydrographr performed function crop_to_extent. corresponding terra workflow includes loading merged layer rast, cropping layer defined bounding box crop writing cropped layer hard drive writeRaster.","code":"# Run the benchmark test 3 times and save the run times in t_crop_bbox_hydr t_crop_bbox_hydr <- microbenchmark({   crop_to_extent(raster_layer = paste0(out_dir, '/merge_hydr.tif'),                  bounding_box = bbox,                  out_dir = out_dir,                  file_name =  'crop_bbox_hydr.tif') }, times = 3)  # Print the run times print_experiment(t_crop_bbox_hydr) ## Run times in seconds: ##    min median    max  ##  449.8  453.7  478.4 # Convert the numeric extent vector to an extent bboxe <- ext(bbox)  # Run the benchmark test 3 times and save the run times in t_crop_bbox_terra t_crop_bbox_terra <- microbenchmark({   r <- rast(paste0(out_dir, '/merge_hydr.tif'))   r_crop <- crop(r, bboxe)   writeRaster(r_crop, paste0(out_dir, '/crop_bbox_terra.tif'), overwrite = TRUE) }, times = 3)  # Print the run times print_experiment(t_crop_bbox_terra) ## Run times in seconds: ##    min median    max  ##  130.1  130.6  143.6"},{"path":"/articles/benchmark.html","id":"experiment-3-cropping-to-basin-boundary","dir":"Articles","previous_headings":"Benchmark experiments","what":"Experiment 3: Cropping to basin boundary","title":"Function benchmarks","text":"Experiment 3 generally experiment 2, instead bounding box merged layer cropped masked basin boundary Amazon basin. basin boundary layer shold located data folder. Cropping hydrographr performed function crop_to_extent. Instead bounding_box path ‘gpkg’ input file basin boundary passed function vector_layer. corresponding terra workflow includes loading merged layer rast, loading basin boundary vect, cropping layer basin boundary crop writing cropped layer hard drive writeRaster. crop input argument mask = TRUE set, crop mask layer basin boundary polygon.","code":"basin_gpkg <- paste0(data_dir, \"/basin_514761/basin_514761.gpkg\") # Run the benchmark test 3 times and save the run times in t_crop_vect_hydr t_crop_vect_hydr <- microbenchmark({   crop_to_extent(raster_layer = paste0(out_dir, '/merge_hydr.tif'),                  vector_layer = basin_gpkg,                  out_dir = out_dir,                  file_name =  'crop_vect_hydr.tif') }, times = 3)  # Print the run times print_experiment(t_crop_vect_hydr) ## Run times in seconds: ##    min median    max  ##  568.0  571.0  592.3 # Run the benchmark test 3 times and save the run times in t_crop_vect_terra t_crop_vect_terra <- microbenchmark({   r <- rast(paste0(out_dir, '/merge_terra_linux.tif'))   basin <- vect(basin_gpkg)   r_crop <- crop(r, basin, mask = TRUE)   writeRaster(r_crop, paste0(out_dir, '/crop_vect_terra.tif'), overwrite = TRUE) }, times = 3)  # Print the run times print_experiment(t_crop_vect_terra) ## Run times in seconds: ##    min median    max  ##  224.3  225.0  225.5"},{"path":"/articles/benchmark.html","id":"experiment-4-reclassifying-raster-values","dir":"Articles","previous_headings":"Benchmark experiments","what":"Experiment 4: Reclassifying raster values","title":"Function benchmarks","text":"reclassification task sub-catchment IDs Amazon basin reassigned new random value. set experiment, table generated includes unique sub-catchment IDs (recl_hydr$id) corresponding “new” random integer number 1 100 (recl_hydr$new). Amazon basin approximately 31 million sub-catchments reclassified. reclassification hydrographr performed function reclass_raster. reclassification sub-catchment IDs generated dummy input table recl_hydr passed function data cropped masked sub-catchment layer used reclass task. corresponding terra workflow includes loading cropped sub-catchment layer rast, reclassification classify writing cropped layer hard drive writeRaster. classify requires matrix defines /values. reclass workflow terra aborted 12 hours progress approximately 10% compared performance hydrographr.","code":"r <- rast(paste0(out_dir, '/crop_vect_hydr.tif')) ids <- terra::unique(r)  recl_hydr <- data.frame(id = ids$crop_vect_hydr_low_linux,                          new = round(runif(nrow(ids), 1, 100))) recl_hydr$id <- as.integer(recl_hydr$id) recl_hydr$new <- as.integer(recl_hydr$new) # Run the benchmark test 3 times and save the run times in t_recl_hydr t_recl_hydr <- microbenchmark({   reclass_raster(data = recl_hydr, rast_val = \"id\", new_val = \"new\",                  raster_layer = paste0(out_dir, '/crop_vect_hydr.tif'),                  recl_layer = paste0(out_dir, '/recl_hydr.tif'),                  read = FALSE, no_data = 0, type = \"Int32\") }, times = 3)  # Print the run times print_experiment(t_recl_hydr) ## Run times in seconds: ##    min median    max  ##  162.4  179.5  188.0 ids_mtx <- as.matrix(recl_hydr)  # Run the benchmark test 3 times and save the run times in t_recl_terra t_recl_terra <-microbenchmark({   r <- rast(paste0(out_dir, '/crop_vect_hydr.tif'))   recl_terra <- classify(r, ids_mtx)   writeRaster(recl_terra, paste0(out_dir, '/recl_terra.tif')) }, times = 5)"},{"path":"/articles/benchmark.html","id":"experiment-5-zonal-statistics","dir":"Articles","previous_headings":"Benchmark experiments","what":"Experiment 5: Zonal statistics","title":"Function benchmarks","text":"Zonal statistics calculated based cropped masked sub-catchment layer define zones flow accumulation Amazon basin zonal statistics calculated. hydrographr calculates zonal statistics function extract_zonal_stat. current version select specific statistical metrics calculated, returns table statistics including minimum maximum values, range, arithmetic mean, variance standard deviation, coefficient variation, number cells per zone. Therefore, run time calculate metrics compared calculation arithmetic mean standard deviation terra. first test workflow terra mean value flow accumulation sub-catchments calculated. corresponding terra workflow includes loading cropped masked sub-catchment layer rast, loading cropped flow accumulation layer, calculating mean value sub-catchments zonal input argument fun = \"mean\" writing cropped layer hard drive writeRaster. second experiment terra standard deviation calculated. workflow remains . difference input argument fun zonal now set fun = \"sd\". execution experiment resulted memory overflow completed.","code":"# Run the benchmark test 3 times and save the run times in t_zonal_hydr t_zonal_hydr <- microbenchmark({   extract_zonal_stat(data_dir= acc_dir,                      subc_id = \"all\",                      subc_layer = paste0(out_dir, '/crop_vect_hydr.tif'),                      var_layer = \"acc_merge_crop.tif\",                      out_dir = out_dir,                      file_name = \"zonal_hydr.csv\",                      n_cores =1) }, times = 3)  # Print the run times print_experiment(t_zonal_hydr) ## Run times in seconds: ##    min median    max  ##  631.6  644.7  657.0 # Run the benchmark test 3 times and save the run times in t_zonal_terra_mean t_zonal_terra_mean <- microbenchmark({   z <- rast(paste0(out_dir, '/crop_vect_hydr.tif'))   acc <- rast(paste0(acc_dir, \"/acc_merge_crop.tif\"))   zonal_terra_mean <- zonal(acc, z, fun = \"mean\") }, times = 3)  # Print the run times print_experiment(t_zonal_terra_mean) ## Run times in seconds: ##    min median    max  ##  246.6  247.0  251.7 # Run the benchmark test 3 times and save the run times in t_zonal_terra_sd t_zonal_terra_sd <- microbenchmark({   z <- rast(paste0(out_dir, '/crop_vect_hydr.tif'))   acc <- rast(paste0(acc_dir, \"/acc_merge_crop.tif\"))   zonal_terra_sd <- zonal(acc, z, fun = \"sd\") }, times = 3)"},{"path":[]},{"path":"/articles/case_study_brazil.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Case study - São Francisco drainage basin","text":"following example showcases functionality hydrographr package along workflow analyse fragmentation stream network. evaluate number dams - downstream fish occurrence site calculate distance along stream network fish occurrence site closest existing future dam. example focus catfish species Conorhynchos conirostris, catfish species endemic São Francisco drainage basin Brazil. catfish listed ‘endangered’ IUCN red list (IUCNRedList2022?). Wetlands preferred habitats species. Conorhynchos conirostris migratory species, migrates spawning. migratory species, mostly threatened dams along stream network. , threatened fishing spawning season upstream part river. start defining regular tiles Hydrography90m (Amatulli et al. 2022) occurrence points species located. Afterwards download Hydrography90m layers, filter/crop merge . select fish occurrence dam point locations using polygon drainage basin snap point locations stream network. Afterwards, reduce drainage basin using dam downstream fish occurrence sites, Sobradinho dam, outlet location calculate upstream catchment point location. next step, calculate distances along stream network point location (fish dams) get - downstream stream segments point locations graph. Afterwards, query tables evaluate shortest distance next dam well number dams.  Let’s get started!","code":"# Load libraries library(hydrographr) library(terra) library(sf) library(rgbif) library(data.table) library(dplyr) library(stringr) library(purrr) library(forcats) library(leaflet) library(ggplot2) # Define working directory wdir <- \"my/working/directory/data_brazil\" # Set defined working directory setwd(wdir) # Create a directory to store all the data of the case study dir.create(\"data\")"},{"path":"/articles/case_study_brazil.html","id":"species-data","dir":"Articles","previous_headings":"","what":"Species data","title":"Case study - São Francisco drainage basin","text":"start downloading occurrence records Conorhynchos conirostris GBIF.org using R package rgbif. Let’s visualise species occurrences map.","code":"# Download occurrence data with coordinates from GBIF gbif_data <- occ_data(scientificName = \"Conorhynchos conirostris\",                        hasCoordinate = TRUE)  # To cite the data use: # gbif_citation(gbif_data)          # Clean the data spdata <- gbif_data$data %>%    select(decimalLongitude, decimalLatitude, species, occurrenceStatus,           country, year) %>%    filter(!is.na(year)) %>%    distinct() %>%    mutate(occurrence_id = 1:nrow(.)) %>%    rename(\"longitude\" = \"decimalLongitude\",          \"latitude\" = \"decimalLatitude\") %>%    select(7, 1:6)  spdata # Define the extent bbox <- c(min(spdata$longitude), min(spdata$latitude),           max(spdata$longitude), max(spdata$latitude))  # Define color palette for the different years of record factpal <- colorFactor(hcl.colors(unique(spdata$year)), spdata$year)  # Create leaflet plot spdata_plot <- leaflet(spdata) %>%   addProviderTiles('Esri.WorldShadedRelief') %>%   setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%   addCircles(lng = ~longitude, lat = ~ latitude, color =  ~factpal(as.factor(year)),               opacity = 1) %>%   addLegend(pal = factpal, values = ~as.factor(year),             title = \"Year of record\") spdata_plot"},{"path":"/articles/case_study_brazil.html","id":"download-files","dir":"Articles","previous_headings":"","what":"Download files","title":"Case study - São Francisco drainage basin","text":"order download layers Hydrography90m, need know IDs 20°x20° tiles occurrence sites located. can obtain IDs using function get_tile_id. function downloads uses auxiliary raster file contains regional units globally, thus requires active internet connection. Currently function returns tiles regional unit input points located . However, tiles may far study area hence always needed steps. Please double check tile IDs relevant purpose using Tile map found . case, São Francisco river basin spreads across three tiles, keep tiles. Let’s define Hydrography90m raster layers GeoPackages like download. use basin raster vector layer, contains drainage basin São Francisco river basin. sub_catchment layer, contains sub-catchment single stream segment unique ID. need layers cropping filtering. need raster layer stream segment flow accumulation snap fish occurrence dam sites stream network. , download raster layer direction calculate upstream catchment defined point location. need Geopackage order_vect_segment, containing stream network, stream (sub-catchment) IDs, ID next stream additional attributes. need GeoPackage calculate distance point locations create graph find stream segments - downstream fish occurrence sites. list available Hydrography90m variables, well details visualisations available .","code":"# Get the tile IDs where the points are located tile_id <- get_tile_id(data = spdata, lon = \"longitude\", lat = \"latitude\") tile_id ## [1] \"h12v08\" \"h12v10\" \"h14v08\" # Define the raster layers vars_tif <- c(\"basin\", \"sub_catchment\", \"segment\", \"accumulation\", \"direction\") # Define the vector layers vars_gpkg <- c(\"basin\", \"order_vect_segment\") # Download the .tif tiles of the defined raster layer download_tiles(variable = vars_tif, tile_id = tile_id, file_format = \"tif\",                download_dir = paste0(wdir, \"/data\"))  # Download the .gpkg tiles of the defined vector layers download_tiles(variable = vars_gpkg, tile_id = tile_id, file_format = \"gpkg\",                download_dir =   paste0(wdir, \"/data\"))"},{"path":"/articles/case_study_brazil.html","id":"filtering-cropping-and-merging","dir":"Articles","previous_headings":"","what":"Filtering, cropping and merging","title":"Case study - São Francisco drainage basin","text":"downloaded layers, need filter crop extent study area. faster processing, recommend filter (vector) crop (raster) files first merge . First, filter GeoPackage tiles containing drainage basins basin ID 481051 (São Francisco drainage basin) merge get extent drainage basin São Francisco river. use polygon drainage basin crop downloaded raster tiles merge . Afterwards, extract unique sub-catchment IDs raster layer filter stream network GeoPackage tiles (order_vect_segment) stream segments sub-catchment ID merge well.","code":"# Define a directory for the São Francisco drainage basin saofra_dir <-  paste0(wdir, \"/data/basin_481051\") if(!dir.exists(saofra_dir)) dir.create(saofra_dir)   # Get the full paths of the basin  GeoPackage tiles basin_dir <- list.files(wdir, pattern = \"basin_h[v0-8]+.gpkg$\",                          full.names = TRUE, recursive = TRUE)  # Filter basin ID from the GeoPackages of the basin tiles # Save the filtered tiles for(itile in basin_dir) {      filtered_tile <- read_geopackage(itile,                                    import_as = \"sf\",                                    subc_id = 481051,                                    name = \"ID\")           write_sf(filtered_tile, paste(saofra_dir,                                 paste0(str_remove(basename(itile), \".gpkg\"),                                        \"_tmp.gpkg\"), sep=\"/\")) }  # Merge filtered GeoPackage tiles merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"basin_.+_tmp.gpkg$\"),             out_dir = saofra_dir,             file_name = \"basin_481051.gpkg\",             name = \"ID\",             read = FALSE)   # Get the full paths of the raster tiles raster_dir <- list.files(paste0(wdir, \"/data/r.watershed\"), pattern = \".tif\",                        full.names = TRUE, recursive = TRUE)  # Crop raster tiles to the extent of the drainage basin for(itile in raster_dir) {      crop_to_extent(raster_layer = itile,                  vector_layer = paste0(saofra_dir, \"/basin_481051.gpkg\"),                  out_dir = saofra_dir,                  file_name =  paste0(str_remove(basename(itile), \".tif\"),                                       \"_tmp.tif\"))    }    # Merge the cropped raster layers of the different variables merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"basin_.+_tmp.tif$\"),             out_dir = saofra_dir,             file_name = \"basin_481051.tif\")  merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"segment_.+_tmp.tif$\"),             out_dir = saofra_dir,             file_name = \"segment_481051.tif\")  merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"accumulation_.+_tmp.tif$\"),             out_dir = saofra_dir,             file_name = \"accumulation_481051.tif\")   merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"direction_.+_tmp.tif$\"),             out_dir = saofra_dir,             file_name = \"direction_481051.tif\")   # Merge the cropped sub-catchment raster layers subc_layer <- merge_tiles(tile_dir = saofra_dir,                           tile_names = list.files(saofra_dir,                                                    full.names = FALSE,                                                   pattern = \"sub_.+_tmp.tif$\"),                           out_dir = saofra_dir,                           file_name = \"sub_catchment_481051.tif\",                           read = TRUE) # Load the merged sub-catchment raster layer of the São Francisco drainage basin subc_layer <- rast(paste0(saofra_dir, \"/sub_catchment_481051.tif\")) # Get all sub-catchment IDs of the São Francisco drainage basin subc_ids <- terra::unique(subc_layer)   # Get the full paths of the stream order segment GeoPackage tiles order_dir <- list.files(wdir, pattern = \"order_.+_h[v0-8]+.gpkg$\",                         full.names = TRUE, recursive = TRUE)  # Filter the sub-catchment IDs from the GeoPackage of the order_vector_segment # tiles (sub-catchment ID = stream ID) # Save the stream segments of the São Francisco drainage basin for(itile in order_dir) {      filtered_tile <- read_geopackage(itile,                                    import_as = \"sf\",                                    subc_id = subc_ids$sub_catchment_481051,                                    name = \"stream\")          write_sf(filtered_tile, paste(saofra_dir,                                 paste0(str_remove(basename(itile), \".gpkg\"),                                        \"_tmp.gpkg\"), sep=\"/\"))   }     # Merge filtered GeoPackage tiles # This process takes a few minutes merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"order_.+_tmp.gpkg$\"),             out_dir = saofra_dir,             file_name = \"order_vect_segment_481051.gpkg\",             name = \"stream\",             read = FALSE)   # Delete temporary files of the filtered and cropped tiles  tmp_files <-  list.files(saofra_dir, pattern = \"_tmp.\",                           full.names = TRUE, recursive = TRUE) file.remove(tmp_files)"},{"path":"/articles/case_study_brazil.html","id":"filtering-of-the-species-occurrences-and-dams","dir":"Articles","previous_headings":"","what":"Filtering of the species occurrences and dams","title":"Case study - São Francisco drainage basin","text":"information existing future dams provided GRanD (Lehner2011?) FHReD (Zarfl2015?) database, respectively. datasets can downloaded globaldamwatch.org. select point locations fish occurrences dams, located within São Francisco drainage use polygon drainage basin. step use functions R package sf.","code":"# Load the GRanD data shape file grand_pts <- st_read(paste0(wdir,                              \"/data/GRanD_Version_1_3/GRanD_dams_v1_3.shp\")) # Load the FHReD dataset fhred_corr <- fread(paste0(wdir,    \"/data/FHReD_2015/FHReD_2015_future_dams_Zarfl_et_al_beta_version.csv\")) # Load the polygon of the drainage basin basin_poly <- read_sf(paste0(saofra_dir,\"/basin_481051.gpkg\"))  # Create simple features from the data tables spdata_pts <- st_as_sf(spdata,                          coords = c(\"longitude\",\"latitude\"),                         remove = FALSE, crs=\"WGS84\") fhred_pts <- st_as_sf(fhred_corr,                        coords = c(\"Lon_Cleaned\",\"LAT_cleaned\"),                        crs=\"WGS84\")  # Only keep species occurrences and dams within the drainage basin spdata_pts_481051 <- st_filter(spdata_pts, basin_poly) grand_pts_481051 <- st_filter(grand_pts, basin_poly) fhred_pts_481051 <- st_filter(fhred_pts, basin_poly)   # Transfer the simple features back to a data table and harmonise the column names # Existing dams (ED) existing_dams <- grand_pts_481051 %>%    mutate(longitude = st_coordinates(.)[,1],          latitude = st_coordinates(.)[,2]) %>%    st_drop_geometry() %>%    mutate(type = \"ED\") %>%   rename(site_id = GRAND_ID) %>%    select(site_id, type, longitude, latitude)  # Future dams (Under construction UD, planned PD) future_dams <- fhred_pts_481051 %>%    mutate(longitude = sf::st_coordinates(.)[,1],          latitude = sf::st_coordinates(.)[,2]) %>%    sf::st_drop_geometry() %>%    mutate(type = paste0(.$Stage, \"D\")) %>%    rename(site_id = DAM_ID) %>%    select(site_id, type, longitude, latitude)  # Fish occurrence (FS) species_occurrence <- spdata_pts_481051 %>%   sf::st_drop_geometry() %>%    mutate(type = \"FS\") %>%    rename(site_id = occurrence_id) %>%   select(site_id, type, longitude, latitude)  # Bind the data.frames of the dam  and the fish occurrence locations # Remove locations of the same type with duplicated coordinates point_locations <- existing_dams %>%    bind_rows(future_dams) %>%    bind_rows(species_occurrence) %>%    distinct(type, longitude, latitude, .keep_all = TRUE)"},{"path":"/articles/case_study_brazil.html","id":"snap-species-occurrences-and-dams-to-a-stream-segment-within-a-certain-distance-and-with-a-certain-flow-accumulation","dir":"Articles","previous_headings":"","what":"Snap species occurrences and dams to a stream segment within a certain distance and with a certain flow accumulation","title":"Case study - São Francisco drainage basin","text":"can calculate upstream catchment defined point location distance along stream network species occurrences dams, need snap coordinates sites stream network. (Recorded coordinates point locations usually exactly overlap digital stream network therefore, need corrected slightly.) hydrographr package offers two different snapping functions snap_to_network snap_to_subc_segment. first function uses defined distance radius flow accumulation threshold, second function snaps point stream segment sub-catchment point originally located . case study use function snap_to_network able define certain flow accumulation threshold ensure fish occurrences dams snapped headwater stream (first order stream) also higher order stream next .  function implemented -loop start searching streams high flow accumulation 400,000 km² short distance slowly decreases flow accumulation 100 km². still sites left snapped stream segment yet. distance increase 10 30 cells. addition new coordinates function also report unique sub-catchment IDs. cases GRASS GIS function r.stream.snap snap point locations stream network, matter much increase distance radius. happens coordinates need changed slightly within cell point gets snapped. might happen needs done multiple times.","code":"# Define full path to the stream network raster layer stream_rast <- paste0(saofra_dir, \"/segment_481051.tif\") # Define full path to the flow accumulation raster layer flow_rast <- paste0(saofra_dir, \"/accumulation_481051.tif\")   # Define thresholds for the flow accumulation of the stream segment, where # the point location should be snapped to accu_threshold <- c(400000, 300000, 100000, 50000, 10000, 5000, 1000, 500, 100)  # Define the distance radius dist_radius <- c(10, 20, 30)  # Create a temporary data.table point_locations_tmp <- point_locations  # Note: The for loop takes about 9 minutes first <- TRUE for (idist in dist_radius) {         # If the distance increases to 20 cells only a flow accumulation of 100 km²    # will be used    if (idist == 20) {     # Set accu_threshold to 100     accu_threshold <- c(100)    }       for (iaccu in accu_threshold) {     # Snap point locations to the stream network     point_locations_snapped_tmp <- snap_to_network(data = point_locations_tmp,                                                    lon = \"longitude\",                                                    lat = \"latitude\",                                                    id = \"site_id\",                                                    stream_layer = stream_rast,                                                    accu_layer = flow_rast,                                                    method = \"accumulation\",                                                    distance = idist,                                                    accumulation = iaccu,                                                    quiet = FALSE)          # Keep point location with NAs for the next loop     point_locations_tmp <- point_locations_snapped_tmp %>%        filter(is.na(subc_id_snap_accu))      if (first == TRUE) {     # Keep the point locations with the new coordinates and remove rows with NA     point_locations_snapped <- point_locations_snapped_tmp %>%      filter(!is.na(subc_id_snap_accu))     first <- FALSE   } else {     # Bind the new data.frame to the data.frame of the loop before     # and remove the NA     point_locations_snapped <- point_locations_snapped %>%        bind_rows(point_locations_snapped_tmp) %>%        filter(!is.na(subc_id_snap_accu))        }      }      } # Run the snapping function until all points are snapped # Note: The while condition runs a few times and # takes about 5 minutes while(nrow(point_locations_tmp) > 0) {   # Create random number smaller than the size if a cell   rn <- runif(n=1, min=-0.000833333, max=+0.000833333)   # Add random number to the longitude and latitude    point_locations_tmp <- point_locations_tmp %>%      mutate(longitude = longitude + rn,            latitude = latitude + rn)          point_locations_snapped_tmp <- snap_to_network(data = point_locations_tmp,                                                    lon = \"longitude\",                                                    lat = \"latitude\",                                                    id = \"site_id\",                                                    stream_layer = stream_rast,                                                    accu_layer = flow_rast,                                                    method = \"accumulation\",                                                    distance = idist,                                                    accumulation = iaccu,                                                    quiet = FALSE)        # Keep point location with NAs for the next loop   point_locations_tmp <- point_locations_snapped_tmp %>%      filter(is.na(subc_id_snap_accu))        # Bind the new data.frame to the data.frame of the loop before   # and remove the NA   point_locations_snapped <- point_locations_snapped %>%      bind_rows(point_locations_snapped_tmp) %>%      filter(!is.na(subc_id_snap_accu)) }  point_locations_snapped # Load the polygon of the drainage basin basin_poly <- read_sf(paste0(saofra_dir,\"/basin_481051.gpkg\"))  # Define the extent bbox <- c(min(spdata$longitude), min(spdata$latitude),           max(spdata$longitude), max(spdata$latitude))  point_locations_snapped <- point_locations[ ,1:2] %>%    left_join(., point_locations_snapped,              by=c(\"site_id\")) %>%    mutate(type = factor(type, levels = c(\"FS\", \"ED\", \"UD\", \"PD\")))   # Define color palette for the different site types pal <- colorFactor(   palette = c('blue', 'red', 'orange', 'yellow'),   domain = point_locations_snapped$type )  labels <-  c(\"Fish occurrence\", \"Existing dam\",               \"Dam under construction \", \"Planned dam\")  # Create leaflet plot locations_plot <- leaflet() %>%   addProviderTiles('Esri.WorldShadedRelief') %>%   setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%   addPolygons(data = basin_poly ,color = \"#444444\",                weight = 1, smoothFactor = 0.5,               opacity = 1.0, fillOpacity = 0.5 ) %>%    #addPolylines(data = stream_vect) %>%    addCircles(data = point_locations_snapped, lng = ~lon_snap_accu,               lat = ~lat_snap_accu, radius = 0.2, color = ~pal(type),               opacity = 1) %>%   addLegend(data = point_locations_snapped, pal = pal, values = ~type,             labFormat = function(type, cuts, p) {paste0(labels)},             title = \"Site type\")  locations_plot"},{"path":"/articles/case_study_brazil.html","id":"snap-point-locations-to-the-stream-segment-within-the-sub-catchment-with-the-same-unique-id","dir":"Articles","previous_headings":"","what":"Snap point locations to the stream segment within the sub-catchment with the same unique ID","title":"Case study - São Francisco drainage basin","text":"use snap_to_subc_segment example code just show second option snap point locations stream network. Points snapped function always located middle stream segment. calculation function needs unique basin sub- catchment IDs. can done using function extract_ids arguments subc_id basin_id set NULL snap_to_subc_segment function extract IDs well.","code":"# # Define full path to the GeoPackage of the stream segments # stream_vect <- paste0(saofra_dir, \"/order_vect_segment_481051.gpkg\") #  # # # Note: The snapping of 138 points will takes about an hour # # Snap point locations to the stream segment within the sub-catchment the fish # # occurrence or dam site is located # point_data_snapped <- snap_to_subc_segment(data = point_locations, #                                            lon = \"longitude\", #                                            lat = \"latitude\"\", #                                            id = \"site_id\", #                                            basin_id = NULL, #                                            subc_id = NULL, #                                            basin_layer = basin_rast, #                                            subc_layer = subc_rast, #                                            stream_layer = stream_vect, #                                            n_cores = 3)"},{"path":"/articles/case_study_brazil.html","id":"get-upstream-catchment","dir":"Articles","previous_headings":"","what":"Get upstream catchment","title":"Case study - São Francisco drainage basin","text":"website IUCN red list can see habitat range Conorhynchos conirostris restricted area upstream Sobradinho dam (site ID 2516) fish occurrences located upstream dam well. Therefore, interested dams upstream Sobradinho dam. get upstream catchment point location Sobradinho dam use function get_upstream_catchment. Afterwards use terra package polygonise raster file upstream catchment use polygon upstream catchment select point locations using functions provided sf package.","code":"# Define full path for the direction raster layer direction_rast <- paste0(saofra_dir, \"/direction_481051.tif\") # Define full path for the output upcatch_dir <- paste0(saofra_dir, \"/upstream_catchment\") # Create output folder if it doesn't exist if(!dir.exists(upcatch_dir)) dir.create(upcatch_dir)  # Get the upstream catchment of ED 2516 # Increasing the number of cores only makes sense if the calculation # is done for multiple points get_upstream_catchment(as.data.table(point_locations_snapped)[site_id == 2516,],                        lon = \"lon_snap_accu\",                        lat = \"lat_snap_accu\",                        id = \"site_id\",                        direction_layer = direction_rast,                        out_dir = upcatch_dir,                        n_cores = 1)  # Load raster file of the upstream catchment  upstream_basin_rast <- rast(paste0(upcatch_dir , \"/upstream_basin_2516.tif\")) # Polygonise the raster upstream_basin_vect <- as.polygons(upstream_basin_rast) # Save the vector file of the upstream catchment writeVector(upstream_basin_vect, paste0(upcatch_dir ,                                          \"/upstream_basin_2516.gpkg\"),              filetype = \"gpkg\", overwrite=TRUE )  upstream_subc_rast <- crop_to_extent(raster_layer =                                         paste0(saofra_dir,                                               \"/sub_catchment_481051.tif\"),                                      vector_layer =                                          paste0(upcatch_dir ,                                               \"/upstream_basin_2516.gpkg\"),                                      out_dir = upcatch_dir,                                      file_name =  \"sub_catchment_2516.tif\",                                      read = TRUE)   # Get all sub-catchment IDs of the upstream basin subc_ids <- terra::unique(upstream_subc_rast)   # Filter the sub-catchment IDs from the GeoPackages of the order_vector_segment # basin file (sub-catchment ID = stream ID) upstream_segment_vect <- read_geopackage(gpkg =                                              paste0(saofra_dir,                                               \"/order_vect_segment_481051.gpkg\"),                                         import_as = \"sf\",                                         subc_id = subc_ids$sub_catchment_2516,                                         name = \"stream\") # Save the stream segments of the upstream drainage basin   write_sf(upstream_segment_vect, paste0(upcatch_dir,                                         \"/order_vect_segment_2516.gpkg\"))                                 # Convert the data.frame into a simple feature point_locations_snapped_pts <- st_as_sf(point_locations_snapped,                                           coords = c(\"lon_snap_accu\",                                                    \"lat_snap_accu\"),                                          remove = FALSE, crs=\"WGS84\")  # Save point locations as a GeoPackage # write_sf(point_locations_snapped_pts,  #   paste0(upcatch_dir, \"/point_locations_snapped_2516.gpkg\"))  # Use the polygon to filter the upstream point locations upstream_point_locations <- st_filter(point_locations_snapped_pts,                                        st_as_sf(upstream_basin_vect)) %>%    sf::st_drop_geometry()  # Save upstream point locations fwrite(upstream_point_locations, paste0(upcatch_dir,                                          \"/upstream_point_locations_2516.csv\")) # Load the GeoPackage of the upstream basin (if not already loaded) # upstream_basin_vect <- vect(paste0(upcatch_dir , \"/upstream_basin_2516.gpkg\"))  # Create leaflet plot upcatch_plot <- leaflet() %>%   addProviderTiles('Esri.WorldShadedRelief') %>%   setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%   addPolygons(data = st_as_sf(upstream_basin_vect),               color = \"#444444\", weight = 1,                smoothFactor = 0.5, opacity = 1.0,                fillOpacity = 0.5 ) %>%    #addPolylines(data = stream_vect) %>%    addCircles(data = upstream_point_locations,               lng = ~lon_snap_accu, lat = ~lat_snap_accu,               radius = 0.2, color = ~pal(type),              opacity = 1) %>%   addLegend(data = upstream_point_locations,              pal = pal, values = ~type,             labFormat = function(type, cuts, p) {paste0(labels)},             title = \"Site type\")  upcatch_plot"},{"path":"/articles/case_study_brazil.html","id":"get-distance-along-the-stream-network","dir":"Articles","previous_headings":"","what":"Get distance along the stream network","title":"Case study - São Francisco drainage basin","text":"determine distance closest dam - downstream fish occurrence first calculate distance point locations (fish occurrences dams) using function get_distance.","code":"# Define the full path to the upstream basin raster layer and the GeoPackage # of the upstream stream segments  upstream_basin_rast <- paste0(upcatch_dir, \"/upstream_basin_2516.tif\") upstream_segment_vect <- paste0(upcatch_dir, \"/order_vect_segment_2516.gpkg\")  # Get the distances along the stream network between each pair of  # point locations # The process takes about 1 hour  # Increasing the number of cores only makes sense if the calculation # is done for point locations in multiple drainage basins upstream_point_locations <- upstream_point_locations %>%    mutate(basin_id = 2516)  network_distance <- get_distance(data = upstream_point_locations,                                  lon = \"lon_snap_accu\",                                  lat = \"lat_snap_accu\",                                  id = \"site_id\",                                  basin_id = \"basin_id\",                                  basin_layer = upstream_basin_rast,                                  stream_layer = upstream_segment_vect,                                  distance = \"network\",                                  n_cores = 1)"},{"path":"/articles/case_study_brazil.html","id":"get-catchment-graph","dir":"Articles","previous_headings":"","what":"Get catchment graph","title":"Case study - São Francisco drainage basin","text":"evaluate many dames located - downstream fish occurrences, need load GeoPackage stream segments ( order_vect_segment) graph. , need sub-catchment IDs fish occurrences use function get_catchment_graph. function return fish occurrence sub-catchment IDs located - downstream. head example data.frame one fish occurrence site (sub-catchment ID 168036446).","code":"# Define full path to the GeoPackage of the stream segments # upstream_segment_vect <- paste0(upcatch_dir, \"/order_vect_segment_2516.gpkg\")  # Load the GeoPackage as graph stream_network_graph <- read_geopackage(gpkg = upstream_segment_vect,                                          import_as = \"graph\")  # Get sub-catchment IDs of the fish occurrences fs_subc_ids <- upstream_point_locations %>%    rename(subcatchment_id = subc_id_snap_accu) %>%    filter(site_id %in% c(1:2, 4:14)) %>%    .$subcatchment_id %>%   unique() %>%    as.character()  # Get all stream segment IDs (=sub-catchments IDs) upstream of each # fish occurrence # Note: There will be no output for fish occurrences which are located # in headwaters (Strahler order 1) upstream_segments <- get_catchment_graph(g = stream_network_graph,                                          subc_id = fs_subc_ids, mode = \"in\")  # Get all stream segment IDs downstream for each # fish occurrence # Note: There will be no output for fish occurrences which are located # in the last segments before the outlet downstream_segments <- get_catchment_graph(g = stream_network_graph,                                            subc_id = fs_subc_ids, mode = \"out\")"},{"path":"/articles/case_study_brazil.html","id":"determine-the-number-of-dams-and-the-distance-along-the-network-to-the-closest-dam","dir":"Articles","previous_headings":"","what":"Determine the number of dams and the distance along the network to the closest dam","title":"Case study - São Francisco drainage basin","text":"determined distance point locations sub-catchment IDs - downstream fish occurrence, can estimate number dams evaluate distance closest dam.  exception fish occurrence sites 4 6 number upstream dams dramatically increase. Especially site 9, already 14 dams located upstream, number might increase 94 dams future. Downstream fish occurrence sites currently one existing dam. dam ID 2516 outlet delineated upstream catchment. leads assumption fish occurrence sites currently connected. future number dams increase fish occurrence sites 7 8 one two dames. future dam might lead disconnection two sites others.  distance next existing dam upstream ranges 12.6km fish occurrence site 8 528.9km fish occurrence site 9. sites distance next dam upstream decrease future. fish occurrence sites 7 8 next future dam upstream farther away existing dam. distance existing dam downstream ranges 415.2km 1672.0km. future distance next dam downstream decrease 1475km 114km fish occurrence sites 7 8.","code":"# Get all sub-catchment IDs of fish occurrences with stream segments upstream fs_subc_ids_up <- names(upstream_segments)  # Get unique site IDs of the fish occurrences fs_site_id <- upstream_point_locations %>%    filter(subc_id_snap_accu %in% fs_subc_ids_up) %>%    arrange(match(subc_id_snap_accu, fs_subc_ids_up)) %>%    select(site_id) %>%    rename(from_site_id = site_id)  # Get the sub-catchment IDs where the existing dams are located ed_site_id <- upstream_point_locations %>%    filter(type == \"ED\") %>%    rename(stream =  subc_id_snap_accu) %>%    mutate(stream = as.character(stream))  # Get the number of existing dams upstream and the distance to the closest # dam upstream ed_upstream <- upstream_segments %>%    map(., ~ inner_join(ed_site_id, .x, by= \"stream\")) %>%    map2(., fs_site_id$from_site_id, ~ mutate(.x, from_site_id = .y)) %>%    list_rbind(.) %>%    rename(., to_site_id = site_id) %>%    select(., from_site_id, to_site_id) %>%    inner_join(network_distance, ., by=c(\"from_site_id\", \"to_site_id\")) %>%    group_by(from_site_id) %>%    mutate(., n_dams = n()) %>%    filter(., dist == min(dist)) %>%    left_join(fs_site_id, ., by = \"from_site_id\") %>%    mutate(type = \"ED\",          direction = \"upstream\")  # Get the sub-catchment IDs where the future dams will be located fd_site_id <- upstream_point_locations %>%    filter(site_id > 100 & site_id < 2000) %>%    rename(stream =  subc_id_snap_accu) %>%    mutate(stream = as.character(stream)) %>%    inner_join(.,point_locations, by = c(\"site_id\", \"longitude\", \"latitude\"))  # Get the number of future dams upstream and the distance to the closest # dam upstream fd_upstream <- upstream_segments %>%    map(., ~ inner_join(fd_site_id, .x, by= \"stream\")) %>%    map2(., fs_site_id$from_site_id, ~ mutate(.x, from_site_id = .y)) %>%    list_rbind(.) %>%    rename(., to_site_id = site_id) %>%    select(., from_site_id, to_site_id) %>%    inner_join(network_distance, ., by=c(\"from_site_id\", \"to_site_id\")) %>%    group_by(from_site_id) %>%    mutate(., n_dams = n()) %>%    filter(., dist == min(dist)) %>%    left_join(fs_site_id, ., by = \"from_site_id\")  %>%    mutate(type = \"FD\",          direction = \"upstream\")  # Get all sub-catchment IDs of fish occurrence sites with stream  # segments downstream fs_subc_ids_down <- names(downstream_segments)  # Get unique site IDs of the fish occurrences  fs_site_id <- upstream_point_locations %>%    filter(subc_id_snap_accu %in% fs_subc_ids_down) %>%    arrange(match(subc_id_snap_accu, fs_subc_ids_down)) %>%    select(site_id) %>%    rename(from_site_id = site_id) %>%    filter(from_site_id != 14)   # Get the number of existing dams downstream and the distance to the closest # dam downstream ed_downstream <- downstream_segments %>%    map(., ~ inner_join(ed_site_id, .x, by= \"stream\")) %>%    map2(., fs_site_id$from_site_id, ~ mutate(.x, from_site_id = .y)) %>%    list_rbind(.) %>%    rename(., to_site_id = site_id) %>%    select(., from_site_id, to_site_id) %>%    inner_join(network_distance, ., by=c(\"from_site_id\", \"to_site_id\")) %>%    group_by(from_site_id) %>%    mutate(., n_dams = n()) %>%    filter(., dist == min(dist)) %>%    left_join(fs_site_id, ., by = \"from_site_id\") %>%    mutate(type = \"ED\",          direction = \"downstream\")  # Get the number of future dams downstream and the distance to the closest # dam downstream fd_downstream <- downstream_segments %>%    map(., ~ inner_join(fd_site_id, .x, by= \"stream\")) %>%    map2(., fs_site_id$from_site_id, ~ mutate(.x, from_site_id = .y)) %>%    list_rbind(.) %>%    rename(., to_site_id = site_id) %>%    select(., from_site_id, to_site_id) %>%    inner_join(network_distance, ., by=c(\"from_site_id\", \"to_site_id\")) %>%    group_by(from_site_id) %>%    mutate(., n_dams = n()) %>%    filter(., dist == min(dist)) %>%    left_join(fs_site_id, ., by = \"from_site_id\") %>%    mutate(type = \"FD\",          direction = \"downstream\")  # Bind tables dam_num_dist <- ed_upstream %>%    bind_rows(fd_upstream) %>%    bind_rows(ed_downstream) %>%    bind_rows(fd_downstream) # Prepare data.frame for the plot plot_data <- dam_num_dist %>%    mutate(dist = dist/1000) %>%    mutate(direction = factor(direction, levels = c(\"upstream\", \"downstream\")))  # Define colors cols <- c(ED = \"red3\", FD = \"orange\")  # Create plot to show the number of dams ggplot(plot_data, aes(x = as.factor(from_site_id), y = n_dams,                        fill = fct_rev(type))) +    geom_bar(stat=\"identity\", position= \"stack\", width = 0.5) +    facet_grid(~ direction, scales = \"free\") +   theme_bw() +   scale_fill_manual(name = \"Dams\",                       breaks = c(\"ED\", \"FD\"),                       labels = c(\"Existing\", \"Future\"),                       values = cols) +   labs(x = \"Fish occurrence site ID\", y = \"Number of dams\") # Create plot to show the distance to the closest dam ggplot(plot_data, aes(x = as.factor(from_site_id), y = dist, fill = type)) +    geom_bar(stat=\"identity\", position= \"dodge\", width = 0.5) +    facet_grid(~ direction, scales = \"free\") +   theme_bw() +   scale_fill_manual(name = \"Dams\",                       breaks = c(\"ED\", \"FD\"),                       labels = c(\"Existing\", \"Future\"),                       values = cols) +   labs(x = \"Fish occurrence site ID\", y = \"Distance along the stream network (km)\")"},{"path":[]},{"path":"/articles/case_study_cuba.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Case study - Cuba","text":"following example showcases functionality hydrographr package along species distribution modelling (SDM) workflow. use SDM predict suitable habitats dragonfly species Cuba. start defining regular tiles Hydrography90m (Amatulli et al., 2022) occurrence points species located. Afterwards download Hydrography90m CHELSA Bioclim (Karger & Zimmermann, 2019) layers predictor variables, crop merge , necessary. , aggregate variable values within sub-catchment study area, keeping predictors mean SD per sub-catchment. Using random forest (RF) algorithm, build simple SDM predict species potentially occur. Finally, reclassify layer sub-catchments study area, creating habitat suitability map.  Hypolestes trinitatis damselfly species endemic Cuba. species inhabits rivers streams located forest areas main mountain ranges eastern central Cuba. presence riparian forests well clean well oxygenated water seems important ecological conditions required H. trinitatis. Larvae can found clinging boulders cobbles river bed fast-flowing water stream segments pre-reproductive adults sexes sexually mature females remain time vegetation riparian forest find shelter prey. Let’s get started! Load required libraries Define working directory","code":"library(hydrographr) library(data.table) library(dplyr) library(terra) library(tools) library(stringr) library(ranger) library(leaflet) library(leafem) # Define the \"data_cuba\" directory, where you have downloaded all the data, # as the working directory wdir <- \"my/working/directory/data_cuba\" setwd(wdir)  # Create a new folder in the working directory to store all the data dir.create(\"data\")"},{"path":"/articles/case_study_cuba.html","id":"species-data","dir":"Articles","previous_headings":"","what":"Species data","title":"Case study - Cuba","text":"start downloading dataset including aquatic insect occurrence records Cuba https://doi.org/10.18728/igb-fred-778.3 (Cambas & Salina, 2022). Import dataset select occurrences species interest, sampled year 1980 (match climate data) Subset columns Let’s visualise species occurrences map Let’s define extent (bounding box) study area (xmin, ymin, xmax, ymax)","code":"download.file(\"https://fred.igb-berlin.de/data/file_download/1395\",               destfile = paste0(wdir, \"/data/spdata_cuba.csv\"), mode = \"wb\") spdata <- fread(paste0(wdir, \"/data/spdata_cuba.csv\"), sep = \"\\t\", fill = TRUE) %>%           filter(species == \"Hypolestes_trinitatis\") %>%           filter(year > 1980) spdata <- spdata %>%   dplyr::select(c(\"occurrence_ID\", \"species\", \"longitude\", \"latitude\", \"year\")) spdata # Convert species data to a spatial vector object to plot the points spdata_vect <- vect(spdata, geom=c(\"longitude\", \"latitude\")) bbox <- c(-84.9749110583, 19.8554808619, -74.1780248685, 23.1886107447) m <- leaflet() %>%   addProviderTiles('Esri.WorldShadedRelief') %>%   setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%   addCircles(data = spdata_vect, color = \"purple\") # data = spdata  m"},{"path":"/articles/case_study_cuba.html","id":"download-files","dir":"Articles","previous_headings":"","what":"Download files","title":"Case study - Cuba","text":"order download layers Hydrography90m, need know IDs 20°x20° tiles located. can obtain IDs using function get_tile_id(). function downloads uses auxiliary raster file contains regional units globally, thus requires active internet connection. Currently function returns tiles regional unit input points located. However, may far study area hence always needed steps. Please double check tile IDs relevant purpose using Tile map found . case, Cuba spreads across two tiles, hold IDs “h08v06” “h10v06”, keep two. Let’s define Hydrography90m variables like download. use variables slope_curv_max_dw_cel (maximum curvature highest upstream, focal downstream cell) spi (Stream Power Index) stream length predictors model. Additionally, download sub_catchment layer, necessary base-layer workflow. list available Hydrography90m variables, well details visualisations available . download CHELSA present future Bioclim variables quick outlook bioclimatic variables can look .","code":"tile_id <- get_tile_id(data = spdata, lon = \"longitude\", lat = \"latitude\") tile_id ## [1] \"h08v06\" \"h10v04\" \"h10v06\" tile_id <- tile_id[c(1,3)] # Variables in raster format vars_tif <- c(\"sub_catchment\", \"slope_curv_max_dw_cel\", \"spi\") # Variables in vector format. This layerholds the information on stream length vars_gpkg <- c(\"order_vect_point\") # Download the .tif tiles of the desired variables download_tiles(variable = vars_tif, tile_id = tile_id, file_format = \"tif\",               download_dir = \"data\")  # Download the .gpkg tiles of the desired variables download_tiles(variable = vars_gpkg, tile_id = tile_id, file_format = \"gpkg\",               download_dir = \"data\") # Create download directory dir.create(paste0(wdir, \"/data/chelsa_bioclim\")) # Extend timeout to 1000s to allow uninterrupted downloading options(timeout = 1000)  # Download # Present, 1981-2010 download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio12_1981-2010_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio12_1981-2010.tif\", mode = \"wb\") download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio15_1981-2010_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio15_1981-2010.tif\", mode = \"wb\") download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio1_1981-2010_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio1_1981-2010.tif\", mode = \"wb\") # Future, 2041-2070 download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio12_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio12_IPSL-CM6A-LR_ssp370_2041-2070.tif\", mode = \"wb\") download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio15_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio15_IPSL-CM6A-LR_ssp370_2041-2070.tif\", mode = \"wb\") download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio1_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio1_IPSL-CM6A-LR_ssp370_2041-2070.tif\", mode = \"wb\")"},{"path":"/articles/case_study_cuba.html","id":"cropping","dir":"Articles","previous_headings":"","what":"Cropping","title":"Case study - Cuba","text":"downloaded layers, need crop extent study area. straightforward case global CHELSA layers: define directory containing layers cropped … final output directory crop files using function crop_to_extent() loop However, hydrographic variable slope_curv_max_dw_cel (maximum curvature highest upstream, focal, downstream cell) split across two tiles. highly recommend first crop tiles extent study area limit size, afterwards merge one file. , first define input directories, time store cropped files directory input files, extra step reaching final output.","code":"dirs_chelsa <- paste0(wdir, \"/data/chelsa_bioclim\") # Define output directory for merged files layer_dir <- paste0(wdir, \"/data/final_layers\") # Create the directory dir.create(layer_dir) files_chelsa <- list.files(dirs_chelsa, pattern = \".tif\", full.names = TRUE)  for(ifile in files_chelsa) {     crop_to_extent(       raster_layer = ifile,       bounding_box = bbox,       out_dir = layer_dir,       file_name = basename(ifile),       read = FALSE,       quiet = TRUE) } dirs_h90m <- list.dirs(paste0(wdir, \"/data\"),                        recursive = TRUE, full.names = TRUE)  dirs_h90m <- dirs_h90m[grep(\"tiles20d\", dirs_h90m)] for(idir in dirs_h90m) {   # only choose rasters   tiles <- list.files(idir, pattern = \".tif$\", full.names = TRUE)   for(itile in tiles) {       crop_to_extent(         raster_layer = itile,         bounding_box = bbox,         out_dir = idir,         file_name = paste0(str_remove(basename(itile), \".tif\"), \"_crop.tif\"),         read = FALSE,         quiet = TRUE)   } }"},{"path":"/articles/case_study_cuba.html","id":"merging","dir":"Articles","previous_headings":"","what":"Merging","title":"Case study - Cuba","text":"cropped tiles variable now need merged together.","code":"# This can be done sequentially: idir <- dirs_h90m[1] merge_tiles(tile_dir = idir,             tile_names = list.files(idir, full.names = FALSE,                                     pattern = \"_crop.tif\"),             out_dir = layer_dir,             file_name = \"spi.tif\",             read = FALSE)  idir <- dirs_h90m[3] merge_tiles(tile_dir = idir,             tile_names = list.files(idir, full.names = FALSE,                                     pattern = \"_crop.tif\"),             out_dir = layer_dir,             file_name = \"slope_curv_max_dw_cel.tif\",             read = FALSE)  idir <- dirs_h90m[4] merge_tiles(tile_dir = idir,             tile_names = list.files(idir, full.names = FALSE,                                     pattern = \"_crop.tif\"),             out_dir = layer_dir,             file_name = \"sub_catchment.tif\",             read = FALSE)    # ...or in a loop for(idir in dirs_h90m[c(1,3,4)]) {   # Get input file extension   file_extension <- file_ext(list.files(idir, full.names = FALSE)[1])    # Assign file extension to output files   ivar_name <- paste0(     str_remove(basename(idir), \"_tiles20d\"), \".\", file_extension   )    # Run the function   merge_tiles(tile_dir = idir,               tile_names = list.files(idir, full.names = FALSE,                                       pattern = \"_crop.tif\"),               out_dir = layer_dir,               file_name = ivar_name,               read = FALSE,                bigtiff = TRUE) }"},{"path":"/articles/case_study_cuba.html","id":"extraction-of-sub-catchment-ids","dir":"Articles","previous_headings":"","what":"Extraction of sub-catchment IDs","title":"Case study - Cuba","text":"Extract IDs sub-catchments points located. step crucial, many functions later use require vector sub-catchment IDs input. Note function extract_ids() can used extract values specific points raster file provided argument subc_layer. can safely used query large raster files, loaded R. species data now corresponding sub-catchment ids","code":"spdata_ids <- extract_ids(data = spdata, lon = \"longitude\", lat = \"latitude\",                           id = \"occurrence_ID\", quiet = FALSE,                           subc_layer = paste0(layer_dir, \"/sub_catchment.tif\"))"},{"path":"/articles/case_study_cuba.html","id":"aggregation-of-environmental-layers","dir":"Articles","previous_headings":"","what":"Aggregation of environmental layers","title":"Case study - Cuba","text":"calculate zonal statistics Hydrography90m CHELSA Bioclim variables sub-catchments study area. Caution, don’t increase number cores 3 can cause memory problems. However, highly depends number sub-catchments well. recommend test function different parameters find works best case. good practice aggregating variables check NoData values: function also reports NoData values used calculation zonal statistics variable. Let’s inspect resulting table keep mean sd variable stats_table, use predictors species distribution model","code":"# Define input var_layers for the extract_zonal_stat() function var_layers <- list.files(layer_dir)[-9] # var_layers <- list.files(layer_dir)[c(3,5,7)] var_layers ## [1] \"bio1_1981-2010.tif\"                      ## [2] \"bio1_IPSL-CM6A-LR_ssp370_2041-2070.tif\" ## [3] \"bio12_1981-2010.tif\"                     ## [4] \"bio12_IPSL-CM6A-LR_ssp370_2041-2070.tif\" ## [5] \"bio15_1981-2010.tif\"                     ## [6] \"bio15_IPSL-CM6A-LR_ssp370_2041-2070.tif\" ## [7] \"slope_curv_max_dw_cel.tif\"               ## [8] \"spi.tif\" report_no_data(data_dir = layer_dir, var_layer = var_layers) ##                                    Raster   NoData ## 1                     bio15_1981-2010.tif        0 ## 2                      bio1_1981-2010.tif        0 ## 3  bio1_IPSL-CM6A-LR_ssp370_2041-2070.tif        0 ## 4                     bio12_1981-2010.tif        0 ## 5 bio12_IPSL-CM6A-LR_ssp370_2041-2070.tif        0 ## 6 bio15_IPSL-CM6A-LR_ssp370_2041-2070.tif        0 ## 7               slope_curv_max_dw_cel.tif -9999999 ## 8                                 spi.tif -9999999 # Run the function that returns the zonal statistics stats_table_zon <- extract_zonal_stat(                     data_dir = layer_dir,                     subc_layer = paste0(layer_dir, \"/sub_catchment.tif\"),                     subc_id = \"all\",                     var_layer = var_layers,                     out_dir = paste0(wdir, \"/data\"),                     file_name = \"zonal_stats.csv\",                     n_cores = 2) # Read the previously created file stats_table_zon <- fread(paste0(wdir, \"/data/zonal_stats.csv\")) colnames(stats_table_zon) ##  [1] \"subc_id\"                                          ##  [2] \"bio1_1981.2010_data_cells\"                        ##  [3] \"bio1_1981.2010_nodata_cells\"                      ##  [4] \"bio1_1981.2010_min\"                               ##  [5] \"bio1_1981.2010_max\"                               ##  [6] \"bio1_1981.2010_range\"                             ##  [7] \"bio1_1981.2010_mean\"                              ##  [8] \"bio1_1981.2010_mean_abs\"                          ##  [9] \"bio1_1981.2010_sd\"                                ## [10] \"bio1_1981.2010_var\"                               ## [11] \"bio1_1981.2010_cv\"                                ## [12] \"bio1_1981.2010_sum\"                               ## [13] \"bio1_1981.2010_sum_abs\"                           ## [14] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_data_cells\"    ## [15] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_nodata_cells\"  ## [16] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_min\"           ## [17] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_max\"           ## [18] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_range\"         ## [19] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_mean\"          ## [20] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_mean_abs\"      ## [21] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_sd\"            ## [22] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_var\"           ## [23] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_cv\"            ## [24] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_sum\"           ## [25] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_sum_abs\"       ## [26] \"bio12_1981.2010_data_cells\"                       ## [27] \"bio12_1981.2010_nodata_cells\"                     ## [28] \"bio12_1981.2010_min\"                              ## [29] \"bio12_1981.2010_max\"                              ## [30] \"bio12_1981.2010_range\"                            ## [31] \"bio12_1981.2010_mean\"                             ## [32] \"bio12_1981.2010_mean_abs\"                         ## [33] \"bio12_1981.2010_sd\"                               ## [34] \"bio12_1981.2010_var\"                              ## [35] \"bio12_1981.2010_cv\"                               ## [36] \"bio12_1981.2010_sum\"                              ## [37] \"bio12_1981.2010_sum_abs\"                          ## [38] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_data_cells\"   ## [39] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_nodata_cells\" ## [40] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_min\"          ## [41] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_max\"          ## [42] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_range\"        ## [43] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_mean\"         ## [44] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_mean_abs\"     ## [45] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_sd\"           ## [46] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_var\"          ## [47] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_cv\"           ## [48] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_sum\"          ## [49] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_sum_abs\"      ## [50] \"bio15_1981.2010_data_cells\"                       ## [51] \"bio15_1981.2010_nodata_cells\"                     ## [52] \"bio15_1981.2010_min\"                              ## [53] \"bio15_1981.2010_max\"                              ## [54] \"bio15_1981.2010_range\"                            ## [55] \"bio15_1981.2010_mean\"                             ## [56] \"bio15_1981.2010_mean_abs\"                         ## [57] \"bio15_1981.2010_sd\"                               ## [58] \"bio15_1981.2010_var\"                              ## [59] \"bio15_1981.2010_cv\"                               ## [60] \"bio15_1981.2010_sum\"                              ## [61] \"bio15_1981.2010_sum_abs\"                          ## [62] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_data_cells\"   ## [63] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_nodata_cells\" ## [64] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_min\"          ## [65] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_max\"          ## [66] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_range\"        ## [67] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_mean\"         ## [68] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_mean_abs\"     ## [69] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_sd\"           ## [70] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_var\"          ## [71] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_cv\"           ## [72] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_sum\"          ## [73] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_sum_abs\"      ## [74] \"slope_curv_max_dw_cel_data_cells\"                 ## [75] \"slope_curv_max_dw_cel_nodata_cells\"               ## [76] \"slope_curv_max_dw_cel_min\"                        ## [77] \"slope_curv_max_dw_cel_max\"                        ## [78] \"slope_curv_max_dw_cel_range\"                      ## [79] \"slope_curv_max_dw_cel_mean\"                       ## [80] \"slope_curv_max_dw_cel_mean_abs\"                   ## [81] \"slope_curv_max_dw_cel_sd\"                         ## [82] \"slope_curv_max_dw_cel_var\"                        ## [83] \"slope_curv_max_dw_cel_cv\"                         ## [84] \"slope_curv_max_dw_cel_sum\"                        ## [85] \"slope_curv_max_dw_cel_sum_abs\"                    ## [86] \"spi_data_cells\"                                   ## [87] \"spi_nodata_cells\"                                 ## [88] \"spi_min\"                                          ## [89] \"spi_max\"                                          ## [90] \"spi_range\"                                        ## [91] \"spi_mean\"                                         ## [92] \"spi_mean_abs\"                                     ## [93] \"spi_sd\"                                           ## [94] \"spi_var\"                                          ## [95] \"spi_cv\"                                           ## [96] \"spi_sum\"                                          ## [97] \"spi_sum_abs\" stats_table_zon <- stats_table_zon %>%   dplyr::select(contains(\"subc\") |  ends_with(\"_mean\") | ends_with(\"_sd\")) %>%   rename('subcatchment_id' = 'subc_id')"},{"path":"/articles/case_study_cuba.html","id":"reading-vector-files","dir":"Articles","previous_headings":"","what":"Reading vector files","title":"Case study - Cuba","text":"Read .gpkg databases two tiles, filtering sub-catchments study area. IDs can retrieved stats_table join two .gpkg databases select columns “subcatchment_id” “length”. use length stream indicator sub-catchment size","code":"stats_gpkg_h08v06 <- read_geopackage(   \"data/r.stream.order/order_vect_tiles20d/order_vect_point_h08v06.gpkg\",   subc_id = stats_table_zon$subcatchment_id) %>%   rename('subcatchment_id' = 'stream')  stats_gpkg_h10v06 <- read_geopackage(   \"data/r.stream.order/order_vect_tiles20d/order_vect_point_h10v06.gpkg\",   subc_id = stats_table_zon$subcatchment_id) %>%   rename('subcatchment_id' = 'stream') stats_gpkg <- rbind(stats_gpkg_h08v06, stats_gpkg_h10v06) %>%   dplyr::select(c(\"subcatchment_id\", \"length\"))  # Clear up memory rm(stats_gpkg_h08v06, stats_gpkg_h10v06); gc() ##            used  (Mb) gc trigger  (Mb) max used  (Mb) ## Ncells  4721961 252.2    8683248 463.8  6479017 346.1 ## Vcells 27243622 207.9   87582737 668.3 87582359 668.3"},{"path":"/articles/case_study_cuba.html","id":"prepare-data-for-modelling","dir":"Articles","previous_headings":"","what":"Prepare data for modelling","title":"Case study - Cuba","text":"Join stats_table .gpkg database values original raster files scaled, need re-scale modelling define following functions: … apply rescale values check IDs sub-catchments null values variables, observe identical: happens variables defined single-pixel sub-catchments, outlets. remove sub-catchments, model able handle data values. split dataset two datasets, according present future Bioclim variables. first used training model second prediction future suitable habitats classification model needs least two classes, case presences absences. sample 10,000 random sub-catchments used pseudoabsences model. filtering takes place sampling assures avoid sampling sub-catchments known presences species join species occurrences present environmental variables table join predictors presences pseudoabsences, obtain modelling data table Define binary column presence-absence (0-1) set factor step, two alternatives: Either split data train test sets: …use whole dataset training set, since RF performs intrinsic evaluation every tree","code":"stats_table <- left_join(stats_table_zon, stats_gpkg, by = \"subcatchment_id\") slope_scale <- function(x, na.rm = F) (x * 0.000001) clim_scale <- function(x, na.rm = F) (x * 0.1) offset <- function(x, na.rm = F) (x - 273.15) stats_table <- stats_table  %>%   mutate(across(contains(\"slope_curv_max_dw_cel\"), slope_scale)) %>%   mutate(across(starts_with(\"bio\"), clim_scale))  %>%   mutate(across(matches(\"bio1_.*_mean\"), offset)) head(stats_table) head(stats_table[is.na(stats_table$slope_curv_max_dw_cel_mean),]) head(stats_table[is.na(stats_table$spi_mean),]) stats_table[is.na(stats_table$spi_mean),]$subcatchment_id == stats_table[is.na(stats_table$slope_curv_max_dw_cel_mean),]$subcatchment_id ##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE stats_table <- stats_table[!is.na(stats_table$slope_curv_max_dw_cel_mean),] stats_table_present <- stats_table %>%   dplyr::select(!contains(\"IPSL\")) %>%   rename(bio1_mean = bio1_1981.2010_mean,          bio1_sd = bio1_1981.2010_sd,          bio12_mean = bio12_1981.2010_mean,          bio12_sd = bio12_1981.2010_sd,          bio15_mean = bio15_1981.2010_mean,          bio15_sd = bio15_1981.2010_sd)  stats_table_future <- stats_table %>%   dplyr::select(!contains(\"1981\")) %>%   rename(bio1_mean = bio1_IPSL.CM6A.LR_ssp370_2041.2070_mean,          bio1_sd = bio1_IPSL.CM6A.LR_ssp370_2041.2070_sd,          bio12_mean = bio12_IPSL.CM6A.LR_ssp370_2041.2070_mean,          bio12_sd = bio12_IPSL.CM6A.LR_ssp370_2041.2070_sd,          bio15_mean = bio15_IPSL.CM6A.LR_ssp370_2041.2070_mean,          bio15_sd = bio15_IPSL.CM6A.LR_ssp370_2041.2070_sd)  # Clear up memory # rm(stats_table) ; gc() pseudoabs <- stats_table_present %>%   filter(!subcatchment_id %in% spdata_ids$subcatchment_id) %>%   sample_n(10000) head(pseudoabs) presence <- left_join(spdata_ids, stats_table_present, by = \"subcatchment_id\") data_model <- data.table::rbindlist(list(presence, pseudoabs), fill = TRUE) data_model$occurrence <- ifelse(!is.na(data_model$occurrence_ID), 1, 0) data_model$occurrence <- as.factor(data_model$occurrence) head(data_model) set.seed(17) #obtain stratified training sample train_idx <- sample(nrow(data_model), 2/3 * nrow(data_model)) data_train <- data_model[train_idx, ] data_test <- data_model[-train_idx, ] data_train <- data_model"},{"path":"/articles/case_study_cuba.html","id":"a-simple-sdm-using-random-forest","dir":"Articles","previous_headings":"","what":"A simple SDM using Random Forest","title":"Case study - Cuba","text":"use ranger random forest (RF) algorithm (Wright & Ziegler, 2017) build species distribution model. Random forest can handle huge quantities data therefore scalable larger datasets. dataset highly imbalanced towards pseudo-absence data, apply -sampling method presence-data modelling process (Valavi et al., 2021). -sampling works balancing training data, contain significantly absence presence points. Following approach, classification-RF model use number pseudo-absences presence points classification tree, sampling replacement (bootstrapping) full pseudo-absence set (Valavi et al., 2021). Let’s define sample size classification tree proportion whole training set, based number presences. Run inspect model. model can evaluated based OOB prediction error. metric lies fact data points used training tree can used test tree. errors OOB samples called --bag errors.","code":"# number of presence records: pres_num <- as.numeric(table(data_train$occurrence)[\"1\"]) sample_size <- c(\"0\" = pres_num / nrow(data_train),                  \"1\" = pres_num / nrow(data_train)) sample_size ##           0           1  ## 0.004281589 0.004281589 model <- ranger(data_train$occurrence ~ .,                  data = data_train[, 6:14],                  num.trees = 1000,                  mtry= 6,                  replace = T,                  sample.fraction = sample_size,                  oob.error = T,                  keep.inbag = T,                  num.threads = 4,                  importance ='impurity',                  probability = T)  model ## Ranger result ##  ## Call: ##  ranger(data_train$occurrence ~ ., data = data_train[, 6:14],      num.trees = 1000, mtry = 6, replace = T, sample.fraction = sample_size,      oob.error = T, keep.inbag = T, num.threads = 4, importance = \"impurity\",      probability = T)  ##  ## Type:                             Probability estimation  ## Number of trees:                  1000  ## Sample size:                      10043  ## Number of independent variables:  9  ## Mtry:                             6  ## Target node size:                 10  ## Variable importance mode:         impurity  ## Splitrule:                        gini  ## OOB prediction error (Brier s.):  0.09087151"},{"path":"/articles/case_study_cuba.html","id":"model-prediction","dir":"Articles","previous_headings":"","what":"Model prediction","title":"Case study - Cuba","text":"predict table sub-catchments across Cuba, including future Bioclim variables. can now reclassify sub-catchment raster based predicted probabilities occurrence, visualise potential suitable habitats species. purpose use ‘reclass_raster’ function. function requires table reclassification rules (.e., subc_id = predicted_occurrence), values need integers. First, let’s join probabilities presence sub-catchment corresponding sub-catchment IDs, create table reclassification rules multiply probability values 100, convert integers can now reclassify sub-catchment raster Let’s plot future habitat suitability map, add presence points  zoom habitat suitability map, can observe sub-catchments offer suitable habitat species detailed view.","code":"pred <- predict(model, data = stats_table_future[,!1]) prediction <- data.table(subcatchment_id = stats_table_future$subcatchment_id,                          pred_occur = as.numeric(pred$predictions[,2])) head(prediction) ##    subcatchment_id pred_occur ##              <int>      <num> ## 1:       399592461 0.02508095 ## 2:       399592865 0.02323095 ## 3:       399593700 0.03909881 ## 4:       399593701 0.03078929 ## 5:       399593817 0.02586667 ## 6:       399593818 0.08412738 prediction$pred_occur <- as.integer(round(prediction$pred_occur, 2) * 100) reclass_raster(   data = prediction,   rast_val = \"subcatchment_id\",   new_val = \"pred_occur\",   raster_layer = paste0(layer_dir, \"/sub_catchment.tif\"),   recl_layer = paste0(wdir, \"/prediction.tif\"),   read = FALSE) # Define colour palette num_pal <- colorNumeric(   viridisLite::inferno(256)   , domain = prediction$pred_occur   , na.color = \"transparent\" )  p <- leaflet() %>% addTiles() %>%   setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%     leafem::addGeotiff(   file = paste0(wdir, \"/prediction.tif\"), opacity = 1,    colorOptions = colorOptions(                   palette = hcl.colors(256, palette = \"inferno\"),                   na.color = \"transparent\"                   ) # read external raster file without loading it to R   ) %>%   leaflet::addCircles(data = spdata_vect, color = \"turquoise\", stroke = TRUE, # data = spdata                       weight = 5, opacity = 1) %>% # add points   addLegend(pal = num_pal,            values = prediction$pred_occur,            labels = palette(),            title = \"Future habitat suitability map<\/br>for Hypolestes trinitatis\",            position = \"bottomleft\", opacity = 1, labFormat = labelFormat(suffix = \"%\"))  # add a legend  p"},{"path":[]},{"path":"/articles/case_study_germany.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Case study - Germany","text":"Load required libraries Define working directory","code":"library(hydrographr) library(rgbif) library(data.table) library(dplyr) library(terra) library(tools) library(stringr) library(leaflet) library(leafem) library(sf) library(tidyr) # Define the \"data_germany\" directory, where you have downloaded all the data, # as the working directory wdir <- \"my/working/directory/data_germany\" setwd(wdir)  # Create a new folder in the working directory to store all the data dir.create(\"data\")"},{"path":"/articles/case_study_germany.html","id":"species-data","dir":"Articles","previous_headings":"","what":"Species data","title":"Case study - Germany","text":"first download occurrence data coordinates GBIF Let’s visualise species occurrences map Let’s define extent (bounding box) study area (xmin, ymin, xmax, ymax)","code":"# Once: Download species occurrence data based on the key of the dataset # and write out to working directory spdata_all <- occ_download_get(key=\"0004551-231002084531237\",                                overwrite = TRUE) %>%   occ_download_import  fwrite(spdata_all, paste0(wdir, \"/data/fish_germany_gbif.csv\"),        row.names = F, quote = F, sep = \"\\t\") # Import and clean the data spdata <- fread(paste0(wdir, \"/data/fish_germany_gbif.csv\"), sep = \"\\t\") %>%   select(gbifID, decimalLongitude, decimalLatitude, species, year) %>%   rename(\"longitude\" = \"decimalLongitude\",          \"latitude\" = \"decimalLatitude\") head(spdata) # Define the extent bbox <- c(min(spdata$longitude), min(spdata$latitude),           max(spdata$longitude), max(spdata$latitude)) # Define color palette for the different years of record factpal <- colorFactor(hcl.colors(unique(spdata$year)), spdata$year)  # Create leaflet plot spdata_plot <- leaflet(spdata) %>%   addProviderTiles('Esri.WorldShadedRelief') %>%   setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%   addCircles(lng = ~longitude, lat = ~ latitude,               color =  ~factpal(as.factor(year)),              opacity = 1) %>%   addLegend(pal = factpal, values = ~as.factor(year),             title = \"Year of record\") spdata_plot"},{"path":[]},{"path":"/articles/case_study_germany.html","id":"hydrography90m","dir":"Articles","previous_headings":"Abiotic variables data","what":"1. Hydrography90m","title":"Case study - Germany","text":"order download layers Hydrography90m, need know IDs 20°x20° tiles located. can obtain IDs using function get_tile_id(). function downloads uses auxiliary raster file contains regional units globally, thus requires active internet connection. Currently function returns tiles regional unit input points located. However, may far study area hence always needed steps. Please double check tile IDs relevant purpose using Tile map found . case, Germany spreads just one tile, ID “h18v02”, keep one. define names raster vector layers want download.","code":"tile_id <- get_tile_id(data = spdata,                        lon = \"longitude\", lat = \"latitude\")  # Get reg unit id to crop all the regular tile layers so that  # we have uninterrupted basins reg_unit_id <- get_regional_unit_id(data = spdata,                                     lon = \"longitude\", lat = \"latitude\") tile_id ## [1] \"h16v02\" \"h18v00\" \"h18v02\" tile_id <- \"h18v02\" # Define the raster layers vars_tif <- c(\"basin\", \"sub_catchment\", \"segment\", \"accumulation\", \"direction\",               \"outlet_dist_dw_basin\", \"outlet_dist_dw_scatch\",               \"channel_dist_up_seg\", \"order_strahler\") # Define the vector layers # The \"basin\" layer contains the polygons of the drainage basins while the # \"order_vect_segment\" layer is the stream network vector file vars_gpkg <- c(\"basin\", \"order_vect_segment\") # Extend timeout to 1000s to allow uninterrupted downloading options(timeout = 1000) # Download the .tif tiles of the desired variables download_tiles(variable = vars_tif, tile_id = tile_id, file_format = \"tif\",                download_dir = \"data\")  # Download the .gpkg tiles of the desired variables download_tiles(variable = vars_gpkg, tile_id = tile_id, file_format = \"gpkg\",                download_dir = \"data\")  # Download the raster mask of the regional unit download_tiles(variable = \"regional_unit\",                file_format = \"tif\",                reg_unit_id = reg_unit_id,                download_dir = \"data\")"},{"path":"/articles/case_study_germany.html","id":"elevation---merit-hydro","dir":"Articles","previous_headings":"Abiotic variables data","what":"2. Elevation - MERIT-HYDRO","title":"Case study - Germany","text":"download elevation files MERIT-HYDRO, visit https://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_Hydro/ define tiles need downloaded. download zipped tiles new directory called elv, unzip downloaded .tar file keep tiles need","code":"elv_dir <- paste0(wdir, \"/data/elv\")  dir.create(elv_dir)"},{"path":"/articles/case_study_germany.html","id":"climate---chelsa-bioclim","dir":"Articles","previous_headings":"Abiotic variables data","what":"3. Climate - CHELSA Bioclim","title":"Case study - Germany","text":"Finally, download three CHELSA present Bioclim variables. quick outlook bioclimatic variables can look .","code":"# Create download directory dir.create(paste0(wdir, \"/data/chelsa_bioclim\")) # Extend timeout to 1000s to allow uninterrupted downloading options(timeout = 1000)  # Download # Present, 1981-2010 download.file(\"https://os.zhdk.cloud.switch.ch/chelsav2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio12_1981-2010_V.2.1.tif \", destfile = \"data/chelsa_bioclim/bio12_1981-2010.tif\", mode = \"wb\") download.file(\"https://os.zhdk.cloud.switch.ch/chelsav2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio15_1981-2010_V.2.1.tif \", destfile = \"data/chelsa_bioclim/bio15_1981-2010.tif\", mode = \"wb\") download.file(\"https://os.zhdk.cloud.switch.ch/chelsav2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio1_1981-2010_V.2.1.tif \", destfile = \"data/chelsa_bioclim/bio1_1981-2010.tif\", mode = \"wb\")"},{"path":"/articles/case_study_germany.html","id":"cropping-the-raster-files","dir":"Articles","previous_headings":"","what":"Cropping the raster files","title":"Case study - Germany","text":"downloaded layers, need crop extent study area extended 500 km, basins split half. Let’s define extent (bounding box) study area (xmin, ymin, xmax, ymax) crop raster tiles extent using function crop_to_extent() loop","code":"# Define and create a directory for the study area study_area_dir <-  paste0(wdir, \"/data/study_area\") if(!dir.exists(study_area_dir)) dir.create(study_area_dir)   # Get the full paths of the raster tiles raster_tiles_watershed <- list.files(paste0(wdir, \"/data/r.watershed\"),                                      pattern = \".tif\", full.names = TRUE,                                       recursive = TRUE) raster_tiles_dist <- list.files(paste0(wdir, \"/data/r.stream.distance\"),                                 pattern = \".tif\", full.names = TRUE,                                  recursive = TRUE) raster_tiles_chan <- list.files(paste0(wdir, \"/data/r.stream.channel\"),                                 pattern = \".tif\", full.names = TRUE,                                  recursive = TRUE) raster_tiles_ord <- list.files(paste0(wdir, \"/data/r.stream.order\"),                                pattern = \".tif\", full.names = TRUE,                                 recursive = TRUE)  raster_tiles <- c(raster_tiles_watershed, raster_tiles_dist,                    raster_tiles_chan, raster_tiles_ord) bb <- c(0.256, 20, 45, 55.4325) for(itile in raster_tiles) {    crop_to_extent(raster_layer = itile,                  bounding_box = bb,                  out_dir = study_area_dir,                  file_name =  paste0(str_remove(basename(itile), \".tif\"),                                      \"_crop.tif\"),                  quiet = FALSE,                  compression = \"high\",                  bigtiff = TRUE,                  read = FALSE) }"},{"path":"/articles/case_study_germany.html","id":"filtering-the-sub-catchment-and-basin--gpkg-files","dir":"Articles","previous_headings":"","what":"Filtering the sub-catchment and basin .gpkg files","title":"Case study - Germany","text":"case don’t work server, suggest download output files chunk following links copy folder study_area_dir: order_vect_segment_h18v02_crop.gpkg basin_h18v02_crop.gpkg","code":"# !! Only run this chunk on a machine with more than 16 GB RAM,  # as the input files are really big !!  # Load the cropped stream and basin raster layer of the study area. # The stream raster can be used interchangeably with the sub_catchment raster,  # because the stream IDs are the same as the sub-catchment IDs.  # Here we use the stream raster because it's smaller in size.   stream_layer <- rast(paste0(study_area_dir, \"/segment_h18v02_crop.tif\")) basin_layer <- rast(paste0(study_area_dir, \"/basin_h18v02_crop.tif\"))  # Get all sub-catchment and basin IDs of the study area subc_ids <- terra::unique(stream_layer) basin_ids <- terra::unique(basin_layer)  # Get the full path of the stream order segment GeoPackage tile order_tile <- list.files(wdir, pattern = \"order.+_h[v0-8]+.gpkg$\",                          full.names = TRUE, recursive = TRUE) basin_gpkg_tile <- list.files(wdir, pattern = \"bas.+_h[v0-8]+.gpkg$\",                               full.names = TRUE, recursive = TRUE)  # Filter the sub-catchment IDs from the GeoPackage of the order_vector_segment # tiles (sub-catchment ID = stream ID) # Save the stream segments of the study area filtered_stream <- read_geopackage(order_tile,                                    import_as = \"sf\",                                    subc_id = subc_ids$segment_h18v02_crop,                                    name = \"stream\")  sf::write_sf(filtered_stream, paste(study_area_dir,                               paste0(str_remove(basename(order_tile), \".gpkg\"),                                      \"_crop.gpkg\"), sep=\"/\"))  filtered_bas <- read_geopackage(basin_gpkg_tile,                                 import_as = \"sf\",                                 subc_id = basin_ids$basin_h18v02_crop,                                 name = \"ID\")  sf::write_sf(filtered_bas, paste(study_area_dir,                                  paste0(str_remove(basename(basin_gpkg_tile), \".gpkg\"),                                         \"_crop.gpkg\"), sep=\"/\"))"},{"path":"/articles/case_study_germany.html","id":"merging-the-elevation-tiles","dir":"Articles","previous_headings":"","what":"Merging the elevation tiles","title":"Case study - Germany","text":"Finally, crop CHELSA Bioclim layers. define directory containing layers cropped list file names","code":"# These are the elevation tiles that include our study area elv_tiles <- c(\"n45e000_elv.tif\", \"n50e010_elv.tif\", \"n60e000_elv.tif\",                 \"n45e005_elv.tif\", \"n50e015_elv.tif\", \"n60e005_elv.tif\",                 \"n45e010_elv.tif\", \"n55e000_elv.tif\", \"n60e010_elv.tif\",                 \"n45e015_elv.tif\", \"n55e005_elv.tif\", \"n60e015_elv.tif\",                 \"n50e000_elv.tif\", \"n55e010_elv.tif\", \"n50e005_elv.tif\",                 \"n55e015_elv.tif\")  merge_tiles(tile_dir = elv_dir,              tile_names = elv_tiles,             out_dir = study_area_dir,              file_name = \"elv_study_area.tif\",             compression = \"high\",             bigtiff = TRUE,             quiet = FALSE)   # crop to our extent crop_to_extent(raster_layer = paste0(study_area_dir, \"/elv_study_area.tif\"),                bounding_box = bb,                out_dir = study_area_dir,                file_name =  \"elv_study_area_crop.tif\",                quiet = FALSE,                compression = \"high\",                 bigtiff = TRUE,                 read = FALSE) dir_chelsa <- paste0(wdir, \"/data/chelsa_bioclim\") files_chelsa <- list.files(dir_chelsa, pattern = \".tif\", full.names = TRUE) for(ifile in files_chelsa) {     crop_to_extent(       raster_layer = ifile,       bounding_box = bb,       out_dir = study_area_dir,       file_name = basename(ifile),       read = FALSE,       quiet = TRUE) }"},{"path":"/articles/case_study_germany.html","id":"extracting-sub-catchment-ids","dir":"Articles","previous_headings":"","what":"Extracting sub-catchment IDs","title":"Case study - Germany","text":"Extract IDs sub-catchments points located. step crucial, many functions later use require vector sub-catchment IDs input. Note function extract_ids() can used extract values specific points raster file provided argument subc_layer. can safely used query large raster files, loaded R. species data now corresponding sub-catchment ids","code":"spdata_ids <- extract_ids(data = spdata,                           id = \"gbifID\",                           lon = \"longitude\",                            lat = \"latitude\",                           basin_layer = paste0(study_area_dir,                                                 \"/basin_h18v02_crop.tif\"),                           subc_layer = paste0(study_area_dir,                                                \"/sub_catchment_h18v02_crop.tif\"))"},{"path":"/articles/case_study_germany.html","id":"snapping-points-to-the-network","dir":"Articles","previous_headings":"","what":"Snapping points to the network","title":"Case study - Germany","text":"can calculate distance along stream network species occurrences, need snap coordinates sites stream network. Recorded coordinates point locations usually exactly overlap digital stream network , therefore, need slightly corrected. hydrographr package offers two different snapping functions, snap_to_network snap_to_subc_segment. first function uses defined distance radius flow accumulation threshold, second function snaps point stream segment sub-catchment point originally located . case study use function snap_to_network able define certain flow accumulation threshold ensure fish occurrences snapped headwater stream (first order stream) also higher order stream next .  function implemented -loop starts searching streams high flow accumulation 400,000 km² short distance slowly decreases flow accumulation 100 km². still sites left snapped stream segment, distance increase 10 30 cells. can write result snapping species data attributed new coordinates cases new sub-catchment id (“subc_id_snap_accu”)","code":"# Define full paths of raster layers stream_rast <- paste0(study_area_dir, \"/segment_h18v02_crop.tif\") flow_rast <- paste0(study_area_dir, \"/accumulation_h18v02_crop.tif\") # We need to shorten the gbifIDs because they are too long for GRASS-GIS # We will delete the first 2 characters (\"40\") from all IDs spdata_ids$gbifID_tmp <- str_replace(spdata_ids$gbifID, \"40\", \"\") # Define thresholds for the flow accumulation of the stream segment, where # the point location should be snapped to accu_threshold <- c(400000, 300000, 100000, 50000, 10000, 5000, 1000, 500, 100)  # Define the distance radius dist_radius <- c(10, 20, 30)  # Create a temporary data.table point_locations_tmp <- spdata_ids  # Note: The for loop takes about 9 minutes first <- TRUE for (idist in dist_radius) {         # If the distance increases to 20 cells only a flow accumulation of 100 km²    # will be used    if (idist == 20) {     # Set accu_threshold to 100     accu_threshold <- c(100)    }       for (iaccu in accu_threshold) {     # Snap point locations to the stream network     point_locations_snapped_tmp <- snap_to_network(data = point_locations_tmp,                                     id = \"gbifID\",                                     lon = \"longitude\", lat = \"latitude\",                                     stream_layer = stream_rast,                                     accu_layer = flow_rast,                                     method = \"accumulation\",                                     distance = idist,                                     accumulation = iaccu,                                     quiet = FALSE)           # Keep point location with NAs for the next loop     point_locations_tmp <- point_locations_snapped_tmp %>%        filter(is.na(subc_id_snap_accu))      if (first == TRUE) {     # Keep the point locations with the new coordinates and remove rows with NA     point_locations_snapped <- point_locations_snapped_tmp %>%      filter(!is.na(subc_id_snap_accu))     first <- FALSE   } else {     # Bind the new data.frame to the data.frame of the loop before     # and remove the NA     point_locations_snapped <- point_locations_snapped %>%        bind_rows(point_locations_snapped_tmp) %>%        filter(!is.na(subc_id_snap_accu))        }      }      } fwrite(point_locations_snapped, paste0(wdir, \"/data/spdata_snapped.csv\"), sep = \",\",                        row.names = FALSE, quote = FALSE) head(point_locations_snapped)"},{"path":"/articles/case_study_germany.html","id":"calculating-distances-between-points","dir":"Articles","previous_headings":"","what":"Calculating distances between points","title":"Case study - Germany","text":"calculate distance point locations. following chunks computationally heavy, suggest run server.","code":"# Load as graph stream_graph <- read_geopackage(   gpkg = paste0(study_area_dir, \"/order_vect_segment_h18v02_crop.gpkg\"),   import_as = \"graph\")  # Get the network distance (in meters) between all input pairs. # We provide the subcatchment ids of the snapped points to the argument \"subc_id\" subc_distances <- get_distance_graph(stream_graph,                             subc_id = point_locations_snapped$subc_id_snap_accu,                             variable = \"length\",                             distance_m = TRUE) head(subc_distances)"},{"path":"/articles/case_study_germany.html","id":"obtaining-network-centrality-indices","dir":"Articles","previous_headings":"","what":"Obtaining network centrality indices","title":"Case study - Germany","text":"now calculate centrality indices using directed stream network graph function get_centrality want consider upstream connected segments, set mode = \"\". following chunks computationally heavy, suggest run server. can reclassify stream segment sub-catchment raster layers based centrality value stream, using function reclass_raster. Essentially, create two new raster files, stream pixel centrality value, instead unique ID stream sub-catchment. ’s new segment sub-catchment layers look like QGIS. Yellow sub-catchments highest values, whereas dark blue lowest values different metrics. node betweenness (roughly) defined number geodesics (shortest paths) going node:  degree node number adjacent edges:  Farness centrality sum length shortest paths node nodes. reciprocal closeness (Altermatt, 2013):  eccentricity node shortest path distance farthest node graph (West, 1996): can add centrality metrics’ values table points, using sub-catchment id: Bonus! want include centrality indices stream network .gpkg, can import .gpkg merge centrality table","code":"centrality <- get_centrality(stream_graph, index = \"all\", mode = \"in\") # This data.frame includes some NAs in some of the columns, because some centrality metrics by default cannot be defined for some the subcatchments. We have to discard subcatchments with NAs before reclassifying. centrality <- drop_na(centrality)  # Then we convert the values in the centrality data.frame into integers centrality <- centrality %>% mutate_if(is.numeric, as.integer)  reclass_raster(data = centrality,                              rast_val = \"subc_id\",                              new_val = \"betweeness\",                              raster_layer = paste0(study_area_dir, \"/segment_h18v02_crop.tif\"),                              recl_layer = paste0(study_area_dir,\"/segment_h18v02_betweeness_all.tif\"),                              read = F)  reclass_raster(data = centrality,                 rast_val = \"subc_id\",                 new_val = \"degree\",                 raster_layer = paste0(study_area_dir, \"/sub_catchment_h18v02_crop.tif\"),                recl_layer = paste0(study_area_dir,\"/sub_catchment_h18v02_degree_all.tif\"),                 read = F)  reclass_raster(data = centrality,                 rast_val = \"subc_id\",                 new_val = \"betweeness\",                 raster_layer = paste0(study_area_dir, \"/sub_catchment_h18v02_crop.tif\"),                recl_layer = paste0(study_area_dir,\"/sub_catchment_h18v02_betweeness_all.tif\"),                 read = F)  reclass_raster(data = centrality, rast_val = \"subc_id\",                 new_val = \"farness\",                 raster_layer = paste0(study_area_dir, \"/sub_catchment_h18v02_crop.tif\"),                recl_layer = paste0(study_area_dir,\"/sub_catchment_h18v02_farness_all.tif\"),                 read = F)  reclass_raster(data = centrality,                 rast_val = \"subc_id\",                 new_val = \"eccentricity\",                 raster_layer = paste0(study_area_dir, \"/sub_catchment_h18v02_crop.tif\"),                recl_layer = paste0(study_area_dir,\"/sub_catchment_h18v02_eccentricity_all.tif\"),                 read = F) point_locations_snapped <- left_join(point_locations_snapped, centrality,                            by = c('subc_id_snap_accu'= 'subc_id')) head(point_locations_snapped) # Load stream network .gpkg as vector stream_vect <- read_geopackage(paste0(study_area_dir, \"/order_vect_segment_h18v02_crop.gpkg\"),                                import_as = \"SpatVect\")  # Merge the centrality table with the vector stream_vect <- terra::merge(stream_vect, spdata_centr,                          by.x = c('stream'), by.y=\"subc_id\")  # Write out the stream gpkg including the centrality indices writeVector(stream_vect, paste0(study_area_dir, \"/order_vect_segment_centr.gpkg\"))"},{"path":"/articles/case_study_germany.html","id":"extracting-zonal-statistics-of-topographic-variables","dir":"Articles","previous_headings":"","what":"Extracting zonal statistics of topographic variables","title":"Case study - Germany","text":"calculate zonal statistics Hydrography90m, MERIT-HYDRO DEM CHELSA Bioclim variables sub-catchments species points. Caution, don’t increase number cores 3 can cause memory problems. However, highly depends number sub-catchments well. recommend test function different parameters find works best case. Let’s first define input var_layers extract_zonal_stat function good practice aggregating variables check NoData values: function also reports NoData values used calculation zonal statistics variable. keep mean sd variable stats_table: values original raster files scaled, need re-scale proceeding analyses. define following functions: … apply rescale values","code":"list.files(study_area_dir) var_layers <- c(\"bio1_1981-2010.tif\", \"bio12_1981-2010.tif\",                 \"bio15_1981-2010.tif\", \"elv_study_area_crop.tif\",                 \"outlet_dist_dw_basin_h18v02_crop.tif\") report_no_data(data_dir = study_area_dir, var_layer = var_layers) #                                 Raster                  NoData # 1                   bio1_1981-2010.tif                  -99999 # 2                  bio15_1981-2010.tif 3.40282346600000016e+38 # 3 outlet_dist_dw_basin_h18v02_crop.tif                   -9999 # 4                  bio12_1981-2010.tif                  -99999 # 5              elv_study_area_crop.tif                   -9999 # Run the function that returns the zonal statistics. # We provide the subcatchment ids of the fish points to the argument 'subc_id'. stats_table_zon <- extract_zonal_stat(                     data_dir = study_area_dir,                     subc_layer = paste0(study_area_dir, \"/sub_catchment_h18v02_crop.tif\"),                     subc_id = point_locations_snapped$subc_id_snap_accu,                     var_layer = var_layers,                     out_dir = paste0(wdir, \"/data\"),                     file_name = \"zonal_stats.csv\",                     n_cores = 3) stats_table_zon <- stats_table_zon %>%   dplyr::select(contains(\"subc\") |  ends_with(\"_mean\") | ends_with(\"_sd\")) %>%   rename('subcatchment_id' = 'subc_id') clim_scale <- function(x, na.rm = F) (x * 0.1) offset <- function(x, na.rm = F) (x - 273.15) stats_table_zon <- stats_table_zon  %>%   mutate(across(starts_with(\"bio\"), clim_scale))  %>%   mutate(across(matches(\"bio1_.*_mean\"), offset)) head(stats_table_zon)"},{"path":"/articles/case_study_germany.html","id":"putting-together-the-final-table","dir":"Articles","previous_headings":"","what":"Putting together the final table","title":"Case study - Germany","text":"now perform left joins match zonal statistics table original species data.","code":"data_fin <- left_join(point_locations_snapped, stats_table_zon,            by = c('subc_id_snap_accu'= 'subcatchment_id'))  data_fin$gbifID <- as.character(data_fin$gbifID) data_fin <- left_join(spdata_ids, data_fin)  # Convert the gbifIDs back to the original values data_fin$gbifID <- str_c(\"40\", data_fin$gbifID) spdata$gbifID <- as.character(spdata$gbifID)  data_fin <- left_join(spdata, data_fin) head(data_fin)"},{"path":[]},{"path":"/articles/example_other_stream_networks.html","id":"download-nhdplushr-data","dir":"Articles","previous_headings":"","what":"Download NHDPlusHR data","title":"Usage of hydrographr with other stream networks","text":"Data NHDPlusHR dataset can downloaded using R package nhdplusTools USGS website: https://apps.nationalmap.gov/downloader/ Check directory structure","code":"# Define the working directory for the region Hawaii wdir <- \"my/working/directory/data_hawaii\" data_dir <- paste0(wdir, \"/data\") rast_dir <- paste0(data_dir, \"/raster\") vect_dir <- paste0(data_dir, \"/vector\") sp_dir <- paste0(data_dir, \"/species\") out_dir  <- paste0(wdir, \"/output\")  # Create a new folder in the working directories to store all the data dir.create(data_dir) dir.create(rast_dir) dir.create(vect_dir) dir.create(sp_dir) dir.create(out_dir) # Download NHDPlus raster data for Hawaii  download_nhdplusv2(outdir = rast_dir,                    url = paste0(                      \"https://prd-tnm.s3.amazonaws.com/StagedProducts/\",                                 \"Hydrography/NHDPlusHR/Beta/GDB/\",                                 \"NHDPLUS_H_2006_HU4_RASTER.7z\"),                    progress = TRUE)  # Unzip -- can also be done manually # When called with default arguments extracts all files in the archive. archive_extract(archive = paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER.7z\"),                 dir = paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER\") ) # Download NHDPlus vector data for Hawaii  download_nhdplusv2(outdir = vect_dir,                    url = paste0(                      \"https://prd-tnm.s3.amazonaws.com/StagedProducts/\",                       \"Hydrography/NHDPlusHR/Beta/GDB/\",                       \"NHDPLUS_H_2006_HU4_GDB.zip\"),                    progress = TRUE)  # Unzip -- can also be done manually archive_extract(archive = paste0(vect_dir, \"/NHDPLUS_H_2006_HU4_GDB.zip\"),                 dir = paste0(vect_dir, \"/NHDPLUS_H_2006_HU4_GDB\") ) list.files(paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER\"), recursive = T,             pattern = \".tif$\")"},{"path":"/articles/example_other_stream_networks.html","id":"species-data","dir":"Articles","previous_headings":"","what":"Species data","title":"Usage of hydrographr with other stream networks","text":"Download occurrence records Stenogobius hawaiiensis GBIF.org using R package rgbif.","code":"# Download occurrence data with coordinates from GBIF gbif_data <- occ_data(scientificName = \"Stenogobius hawaiiensis\",                        hasCoordinate = TRUE)  # To cite the data use: # gbif_citation(gbif_data)  # Clean the data spdata <- gbif_data$data %>%    select(decimalLongitude, decimalLatitude, species, occurrenceStatus,           country, stateProvince, year) %>%    filter(!is.na(year),          stateProvince == \"Hawaii\") %>%    distinct() %>%    mutate(occurrence_id = 1:nrow(.)) %>%    rename(\"longitude\" = \"decimalLongitude\",          \"latitude\" = \"decimalLatitude\") %>%    select(8, 1:7)  # Save the data write.csv(spdata, paste0(sp_dir, \"/stenogobius_hawaiiensis.csv\"), row.names = F,           quote = F)  spdata"},{"path":"/articles/example_other_stream_networks.html","id":"visualise-species-data","dir":"Articles","previous_headings":"","what":"Visualise species data","title":"Usage of hydrographr with other stream networks","text":"inspecting NHDPlusHR data, observe coordinate reference system WGS84, EPSG:26904. Therefore need convert species points’ coordinates EPSG:26904, match spatial layers.","code":"m <- leaflet() %>%   addProviderTiles('Esri.WorldShadedRelief') %>%   addCircles(data = spdata, color = \"purple\")   m rast(paste0(rast_dir,              \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006/cat.tif\")) # Convert the coordinate columns to a matrix spdata_coord <- spdata %>%    select(longitude, latitude) %>%    as.matrix()  # Project to the coordinate reference system of the NHDPlus dataset spdata_coord <- project(spdata_coord,                          from = \"+proj=longlat +datum=WGS84\",                          to = \"EPSG:26904\")  # Replace the WGS84 coordinates with the EPSG:26904 coordinates in the species  # points' data frame spdata <- spdata %>%    mutate(longitude = spdata_coord[,1],          latitude = spdata_coord[,2])"},{"path":"/articles/example_other_stream_networks.html","id":"crop-and-merge-raster-data","dir":"Articles","previous_headings":"","what":"Crop and merge raster data","title":"Usage of hydrographr with other stream networks","text":"","code":"# The coordinates of the bounding box should also be in the NAD_1983_UTM_Zone_4N projection system bbox <- c(582638, 2361564, 594373, 2370255) crop_to_extent(raster_layer = paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006/cat.tif\"),                 bounding_box = bbox,                out_dir = out_dir,                file_name = \"cat_crop.tif\",                compression = \"high\"               )  # We can crop to another bounding box, and then try to merge the two cropped  # layers bbox2 <- c(595603, 2371564, 604373, 2380255) crop_to_extent(raster_layer = paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006/cat.tif\"),                 bounding_box = bbox2,                out_dir = out_dir,                file_name = \"cat_crop2.tif\",                compression = \"high\"               ) # Merge the two cropped layers merge_tiles(tile_dir = out_dir,              tile_names = c(\"cat_crop.tif\", \"cat_crop2.tif\"),             out_dir = out_dir,             file_name = \"cat_merged.tif\",             compression = \"high\")"},{"path":"/articles/example_other_stream_networks.html","id":"extract-ids-report-no-data-value-set-no-data-value","dir":"Articles","previous_headings":"","what":"Extract IDs, Report no data value, Set no data value","title":"Usage of hydrographr with other stream networks","text":"","code":"spdata_ids <- extract_ids(data = spdata,              lon = \"longitude\", lat = \"latitude\",             id = \"occurrence_id\",             basin_layer  = paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006/cat.tif\"))   spdata_ids  # We observe that some points obtain basin_id=NA, because they fall out of the  # extent of the downloaded raster layer. We will filter these points out,  # keeping only those which overlap with the raster.  spdata_ids <- spdata_ids %>%     filter(!is.na(basin_id))  spdata_ids  # On the other hand, some other points obtain basin_id=-32768. This should be  # the no data value of the raster layer, meaning a value attributed to the sea  # cells. We can double-check this using the report_no_data function: no_data_val <- report_no_data(data_dir = paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006\"), var_layer = \"cat.tif\")  no_data_val # We can filter out these points, too, so that we only keep the points sampled  # in rivers.  spdata_ids <- spdata_ids %>%     filter(basin_id != no_data_val$NoData) spdata_ids # Don't run: # If we wanted to change the no data value of a layer, we could do it using the  # function: set_no_data(data_dir = paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006\"),              var_layer = \"cat.tif\",              no_data = -32768)  # Remember to set it back to the original value!"},{"path":"/articles/example_other_stream_networks.html","id":"extract-zonal-statistics","dir":"Articles","previous_headings":"","what":"Extract zonal statistics","title":"Usage of hydrographr with other stream networks","text":"extract zonal statistics raster layer using NHDPlus basins zones, first need download crop extent example raster layer. use CHELSA Bioclim 12 (BIO12) layer 1981-2010 example. operation, also need convert projection system cat.tif raster EPSG:4326 using ‘terra’ package, compatible coordinate system CHELSA Bioclim files.","code":"# Download bio12 in the rast_dir download.file(   \"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio12_1981-2010_V.2.1.tif\", destfile = paste0(rast_dir, \"/bio12_1981-2010.tif\"), mode = \"wb\")   # Crop bio12 to bounding box # We first need to convert the bounding box coordinates to WGS84, that is the  # coordinate system of the CHELSA Bioclim layers. bbox_wgs84 <- project(ext(bbox), from = \"EPSG:26904\", to = \"EPSG:4326\") # Now crop crop_to_extent(raster_layer = paste0(rast_dir, \"/bio12_1981-2010.tif\"),                 bounding_box = bbox_wgs84,                out_dir = out_dir,                file_name = \"bio12_crop.tif\",                compression = \"high\"               )  # Load the cat.tif layer cat_rast <- rast(paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006/cat.tif\"))  # Reproject to WGS84 and write out the file with a new name cat_rast <- project(cat_rast,                    y = \"epsg:4326\",                    filename = paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006/cat_wgs84.tif\"))  # Extract the zonal statistics of bio12 based on the NHDPlus basins zonal_stats <- extract_zonal_stat(data_dir = out_dir,                 subc_id = \"all\",                  subc_layer =  paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006/cat_wgs84.tif\"),                 var_layer = \"bio12_crop.tif\",                 quiet = F)  # Same as before, we need to filter out the no data values to get a better  # grasp of the resulting table zonal_stats <- zonal_stats %>%    filter(bio12_crop_data_cells !=0 ) zonal_stats"},{"path":"/articles/example_other_stream_networks.html","id":"reclassify-raster","dir":"Articles","previous_headings":"","what":"Reclassify raster","title":"Usage of hydrographr with other stream networks","text":"","code":"# Create example dataframe with reclassification rules to reclassify the value  # of all basins containing species data to 1, and all the rest to -99999. reclass_rules <- data.frame(cat = as.integer(spdata_ids$basin_id),                              new_val = as.integer(1))  # Apply reclassification reclass_raster(   data = reclass_rules,   rast_val = \"cat\",   new_val = \"new_val\",   raster_layer = paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006/cat.tif\"),   recl_layer = paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006/cat_reclass.tif\"),   read = FALSE,    no_data = no_data_val$NoData)"},{"path":[]},{"path":"/articles/example_other_stream_networks.html","id":"read-a--gpkg","dir":"Articles","previous_headings":"","what":"Read a .GPKG","title":"Usage of hydrographr with other stream networks","text":"See [Download NHDPlus data] download NHDPlus vector data Hawaii.","code":"gdb_dir <- paste0(vect_dir, \"/NHDPLUS_H_2006_HU4_GDB\")  # Write out a .gpkg file based on the .gdb files get_nhdplushr(gdb_dir, file.path(gdb_dir, \"nhdplus_2006-HU4.gpkg\"))   # Import flow line attribute table as a data.table flow_line <- read_geopackage(gpkg = paste0(gdb_dir, \"/nhdplus_2006-HU4.gpkg\"),                  layer_name = \"NHDFlowline\")   # To import flow line as a graph, first import the attribute table as a  # data.table and then transform to a graph. Note that the first two columns  # need to be the \"from\" and \"to\" columns  setcolorder(flow_line, c(\"FromNode\", \"ToNode\"))  # Create the graph flow_line_graph  <- graph_from_data_frame(flow_line, directed = TRUE)  # Import the catchment layer as a spatial vector catchment_sp <- read_geopackage(                 gpkg = paste0(gdb_dir, \"/nhdplus_2006-HU4.gpkg\"),                  layer_name = \"NHDPlusCatchment\",                 name = \"REACHCODE\",                 import_as = \"SpatVect\")"},{"path":"/articles/example_other_stream_networks.html","id":"snap-points-to-the-network","dir":"Articles","previous_headings":"","what":"Snap points to the network","title":"Usage of hydrographr with other stream networks","text":"Note: columns “subc_id_snap_dist” “subc_id_snap_accu” table “point_locations_snapped” meaningful case stream flow accumulation raster layers derived Hydrography90m.","code":"# Define full path to the stream network raster layer stream_rast <- paste0(rast_dir, \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006/swnet.tif\")  # Define full path to the flow accumulation raster layer flow_rast <- paste0(rast_dir,                      \"/NHDPLUS_H_2006_HU4_RASTER/HRNHDPlusRasters2006/fac.tif\")  # Define thresholds for the flow accumulation of the stream segment, where # the point location should be snapped to accu_threshold <- 700 # Define the distance radius dist_radius <- 20  # Snap point locations to the stream network point_locations_snapped <- snap_to_network(data = spdata_ids,                                              lon = \"longitude\",                                              lat = \"latitude\",                                              id = \"occurrence_id\",                                              stream_layer = stream_rast,                                              accu_layer = flow_rast,                                              method = \"both\",                                              distance = dist_radius,                                              accumulation = accu_threshold,                                              quiet = FALSE)"},{"path":[]},{"path":"/articles/hydrographr.html","id":"system-requirements","dir":"Articles","previous_headings":"","what":"System requirements","title":"Getting started with hydrographr","text":"work smoothly hydrographr package, GRASS GIS, GDAL, pktools need installed. can find installation guideline operating systems: Linux Windows macOS","code":""},{"path":"/articles/hydrographr.html","id":"loading-hydrographr","dir":"Articles","previous_headings":"","what":"Loading hydrographr","title":"Getting started with hydrographr","text":"can install hydrographr GitHub repository. install R package yet, install remotes::install_github(). start exploring package load hydrographr.","code":"# If the package remotes is not installed run first: install.packages(\"remotes\")  remotes::install_github(\"glowabio/hydrographr\") library(hydrographr) #> Warning: replacing previous import 'dplyr::as_data_frame' by #> 'igraph::as_data_frame' when loading 'hydrographr'"},{"path":"/articles/linux_system_setup.html","id":"installation-of-the-required-gis-tools","dir":"Articles","previous_headings":"","what":"Installation of the required GIS tools","title":"Setting up the package requirements on Linux","text":"installed GIS tools, please, install using hydrographr. use Ubuntu can copy-paste commands . use another Linux distribution, please, see linked software webpage installation instruction. Warning! Sometimes installation work within VPN connection. fail fetch data online repositories installation, please try deactivating VPN connection. Add Ubuntugis repository First, add “Ubuntugis” Personal Package Archive (PPA) system’s software sources able install GIS tools available . Next, need tell system pull latest list software archive knows , including PPA just added: Now ready installGDAL, GRASS GIS GNU parallel. Copy paste commands console follow instructions install tools. GDAL GDAL translator library raster vector geospatial data formats comes variety useful command line utilities data translation processing. information check GDAL website. GRASS GIS GRASS GIS powerful raster, vector, geospatial processing engine including tools terrain ecosystem modeling, hydrology processing satellite aerial imagery. detailed installation instructions check GRASS GIS users wiki GRASS GIS website. Please, make sure grass8.2 installed. GRASS GIS addons Copy-paste commands install required addons GRASS GIS. GNU parallel GNU parallel shell tool executing jobs parallel multiple cores. faster processing, hydrographr functions snap_to_subc_segment() extract_zonal_stat() GNU parallel implemented. information check GNU parallel website. Now type ctrl+z exit GNU parallel environment. GNU bc GNU bc (Basic Calculator) arbitrary precision numeric processing language, used function snap_to_subc_segment(). information check GNU bc website. recommend reboot computer installation.","code":"# Add the Ubuntugis PPA sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable # Update the packages list sudo apt update # Install GDAL sudo apt install gdal-bin python3-gdal # Install GRASS GIS sudo apt-get install grass grass-core grass-dev grass-gui grass-doc sudo apt install make  # Install GRASS GIS addons export GRASSEXEC=\"grass --exec\" $GRASSEXEC  g.extension  extension=r.stream.distance $GRASSEXEC  g.extension  extension=r.stream.order $GRASSEXEC  g.extension  extension=r.stream.snap $GRASSEXEC  g.extension  extension=r.stream.basins # Download the latest version of GNU parallel  wget http://ftp.gnu.org/gnu/parallel/parallel-latest.tar.bz2  # Unzip the the .tar file and create a new folder parallel-yyyymmdd sudo tar xjf parallel-latest.tar.bz2 # Check folders and files in the directory and see the date of  # latest version e.g. parallel-20221122 ls # Move to the new created folder parallel-yyyymmdd cd parallel-20221122  sudo ./configure && make sudo make install parallel --citation  #> Academic tradition requires you to cite works you base your article on. #> If you use programs that use GNU Parallel to process data for an article in a #> scientific publication, please cite: #>  #> @software{tange_2022_7347980, #>       author       = {Tange, Ole}, #>       title        = {GNU Parallel 20221122 ('Херсо́н')}, #>       month        = Nov, #>       year         = 2022, #>       note         = {{GNU Parallel is a general parallelizer to run #>                        multiple serial command line programs in parallel #>                        without changing them.}}, #>       publisher    = {Zenodo}, #>       doi          = {10.5281/zenodo.7347980}, #>       url          = {https://doi.org/10.5281/zenodo.7347980} #> }  #> (Feel free to use \\nocite{tange_2022_7347980})  #> This helps funding further development; AND IT WON'T COST YOU A CENT. #> If you pay 10000 EUR you should feel free to use GNU Parallel without citing.  #> More about funding GNU Parallel and the citation notice: #> https://lists.gnu.org/archive/html/parallel/2013-11/msg00006.html #> https://www.gnu.org/software/parallel/parallel_design.html#citation-notice #> https://git.savannah.gnu.org/cgit/parallel.git/tree/doc/citation-notice-faq.txt  #> If you send a copy of your published article to tange@gnu.org, it will be #> mentioned in the release notes of next version of GNU Parallel. #>  #> Type: 'will cite' and press enter.  will cite  #> Thank you for your support: You are the reason why there is funding to #> continue maintaining GNU Parallel. On behalf of future versions of #> GNU Parallel, which would not exist without your support:  #>  THANK YOU SO MUCH  #> It is really appreciated. The citation notice is now silenced. # Install bc sudo apt install bc"},{"path":"/articles/macos_system_setup.html","id":"installation-of-the-required-gis-tools","dir":"Articles","previous_headings":"","what":"Installation of the required GIS tools","title":"Setting up the package requirements on MacOS","text":"installed GIS tools, please, install using hydrographr. Warning! Sometimes installation work within VPN connection. fail fetch data online repositories installation, please try deactivating VPN connection. GDAL GDAL translator library raster vector geospatial data formats comes variety useful command line utilities data translation processing. information check GDAL website. install GDAL, follow installation guide GRASS GIS GRASS GIS powerful raster, vector, geospatial processing engine including tools terrain ecosystem modeling, hydrology processing satellite aerial imagery. detailed installation instructions check GRASS GIS users wiki GRASS GIS website. Please, make sure grass8.2 installed. First, need download latest stable release GRASS GIS GRASS repository. Afterwards: 1. Download unzip *.dmg installation package 2. Drag GRASS app /Applications folder (work properly folder) IMPORTANT: INSTALLATION LOCATION new fully bundled binaries MUST installed run /Applications folder. run another folder even subfolder /Applications. build environment. GNU parallel GNU parallel shell tool executing jobs parallel multiple cores. faster processing, hydrographr functions snap_to_subc_segment() extract_zonal_stat() GNU parallel implemented. information check GNU parallel website. Copy paste commands console: Now type ctrl+z exit GNU parallel environment. GNU bc GNU bc (Basic Calculator) arbitrary precision numeric processing language, used function snap_to_subc_segment(). information check GNU bc website. recommend reboot computer installation.","code":"brew install parallel  parallel --citation  #> Academic tradition requires you to cite works you base your article on. #> If you use programs that use GNU Parallel to process data for an article in a #> scientific publication, please cite: #>  #> @software{tange_2022_7347980, #>       author       = {Tange, Ole}, #>       title        = {GNU Parallel 20221122 ('Херсо́н')}, #>       month        = Nov, #>       year         = 2022, #>       note         = {{GNU Parallel is a general parallelizer to run #>                        multiple serial command line programs in parallel #>                        without changing them.}}, #>       publisher    = {Zenodo}, #>       doi          = {10.5281/zenodo.7347980}, #>       url          = {https://doi.org/10.5281/zenodo.7347980} #> }  #> (Feel free to use \\nocite{tange_2022_7347980})  #> This helps funding further development; AND IT WON'T COST YOU A CENT. #> If you pay 10000 EUR you should feel free to use GNU Parallel without citing.  #> More about funding GNU Parallel and the citation notice: #> https://lists.gnu.org/archive/html/parallel/2013-11/msg00006.html #> https://www.gnu.org/software/parallel/parallel_design.html#citation-notice #> https://git.savannah.gnu.org/cgit/parallel.git/tree/doc/citation-notice-faq.txt  #> If you send a copy of your published article to tange@gnu.org, it will be #> mentioned in the release notes of next version of GNU Parallel. #>  #> Type: 'will cite' and press enter.  will cite  #> Thank you for your support: You are the reason why there is funding to #> continue maintaining GNU Parallel. On behalf of future versions of #> GNU Parallel, which would not exist without your support:  #>  THANK YOU SO MUCH  #> It is really appreciated. The citation notice is now silenced. # Install bc brew install bc"},{"path":"/articles/windows_system_setup.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"Setting up the package requirements on Windows","text":"Make sure least 4 GB free disk space. need administrator privileges Windows. must running Windows 10 version 2004 higher Windows 11, otherwise WSL available.","code":""},{"path":[]},{"path":"/articles/windows_system_setup.html","id":"option-1--install-windows-subsystem-for-linux-through-the-microsoft-store-preferable","dir":"Articles","previous_headings":"Installation of the Windows Subsystem for Linux with Ubuntu","what":"Option 1. Install Windows Subsystem for Linux through the Microsoft Store (preferable)","title":"Setting up the package requirements on Windows","text":"Visit Windows store, search Ubuntu, install restart. (information installation: https://aka.ms/wslstore)","code":""},{"path":"/articles/windows_system_setup.html","id":"option-2--install-windows-subsystem-for-linux-through-the-command-line","dir":"Articles","previous_headings":"Installation of the Windows Subsystem for Linux with Ubuntu","what":"Option 2. Install Windows Subsystem for Linux through the command line","title":"Setting up the package requirements on Windows","text":"Open Windows Command Prompt (cmd.exe) administrator mode clicking Start button typing “cmd” search bar.right-click “Command Prompt” select “Run Administrator” highlight result arrow keys press Ctrl+Shift+Enter open command prompt administrative privileges. Enter wsl --install enable features necessary run WSL also directly install Ubuntu distribution Linux, default option WSL. installation complete, restart machine. open “Start” menu click “Ubuntu” open Ubuntu console. first time open Ubuntu via “Start” menu prompted create username password Linux distribution. default user account administrator Ubuntu. purpose guideline use hydrographr, can pick name want. Please note whilst entering password, nothing appear screen won’t see typing, completely normal. Window automatically update Linux distribution. update upgrade packages Ubuntu, please copy paste command Ubuntu terminal enter password. done regularly basis. run command sudo system ask password UNIX user account. details installation WSL see . details setting username password see .","code":"# Enable the features necessary to run WSL and  # install the Ubuntu distribution of Linux.  wsl --install  # If the above doesn't work, then try: wsl --install -d Ubuntu  #> Installing: Virtual Machine Platform #> Virtual Machine Platform has been installed. #> Installing: Windows Subsystem for Linux #> Windows Subsystem for Linux has been installed. #> Downloading: WSL Kernel #> Installing: WSL Kernel #> WSL Kernel has been installed. #> Downloading: Ubuntu #> The requested operation is successful. Changes will not be effective until the system is rebooted. #> Installing, this may take a few minutes... #> Please create a default UNIX user account. The username does not need to match your Windows username. #> For more information visit: https://aka.ms/wslusers #> Enter new UNIX username: hydrographr #> New password: #> Retype new password: #> passwd: password updated successfully #> Installation successful! #> To run a command as administrator (user \"root\"), use \"sudo <command>\". #> See \"man sudo_root\" for details. # Update and upgrade packages on Ubuntu sudo apt update && sudo apt upgrade"},{"path":"/articles/windows_system_setup.html","id":"installation-of-the-required-gis-tools-on-the-wsl","dir":"Articles","previous_headings":"","what":"Installation of the required GIS tools on the WSL","title":"Setting up the package requirements on Windows","text":"Next, need install required GIS tools WSL Ubuntu system. can use Ubuntu terminal, PowerShell Windows Command Prompt start installation. use PowerShell Windows Command Prompt use command wsl enter WSL first, exit go back Windows OS installation GIS tools. Warning! Sometimes installation work within VPN connection. fail fetch data online repositories installation, please try deactivating VPN connection. Add Ubuntugis repository First, add “Ubuntugis” Personal Package Archive (PPA) system’s software sources able install GIS tools available . Next, need tell system pull latest list software archive knows , including PPA just added: Now ready installGDAL, GRASS GIS GNU parallel. Copy paste commands console follow instructions install tools. GDAL GDAL translator library raster vector geospatial data formats comes variety useful command line utilities data translation processing. information check GDAL website. GRASS GIS GRASS GIS powerful raster, vector, geospatial processing engine including tools terrain ecosystem modeling, hydrology processing satellite aerial imagery. detailed installation instructions check GRASS GIS users wiki GRASS GIS website. GRASS GIS addons Copy-paste commands install required addons GRASS GIS. GNU parallel GNU parallel shell tool executing jobs parallel multiple cores. faster processing, hydrographr functions snap_to_subc_segment() extract_zonal_stat() GNU parallel implemented. information check GNU parallel website. Now type ctrl+z exit GNU parallel environment. GNU bc GNU bc (Basic Calculator) arbitrary precision numeric processing language, used function snap_to_subc_segment(). information check GNU bc website. dos2unix Windows systems uses different text file line endings Linux systems. Dos2Unix package contains commands converting line endings text file Windows Linux vice versa. recommend reboot computer installation.","code":"# Add the Ubuntugis PPA sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable # Update the packages list sudo apt update && sudo apt upgrade # Install GDAL sudo apt install gdal-bin python3-gdal # Install GRASS GIS sudo apt-get install grass grass-core grass-dev grass-gui grass-doc sudo apt install make  # Install GRASS GIS addons export GRASSEXEC=\"grass --exec\" $GRASSEXEC  g.extension  extension=r.stream.distance $GRASSEXEC  g.extension  extension=r.stream.order $GRASSEXEC  g.extension  extension=r.stream.snap $GRASSEXEC  g.extension  extension=r.stream.basins # Download the latest version of GNU parallel  wget http://ftp.gnu.org/gnu/parallel/parallel-latest.tar.bz2  # Install the software that unzips the .tar file sudo apt-get install bzip2  # Unzip the the .tar file and create a new folder parallel-yyyymmdd sudo tar xjf parallel-latest.tar.bz2 # Check folders and files in the directory and see the date of  # latest version e.g. parallel-20221122 ls # Move to the new created folder parallel-yyyymmdd cd parallel-20221122  sudo ./configure && make sudo make install parallel --citation  #> Academic tradition requires you to cite works you base your article on. #> If you use programs that use GNU Parallel to process data for an article in a #> scientific publication, please cite: #>  #> @software{tange_2022_7347980, #>       author       = {Tange, Ole}, #>       title        = {GNU Parallel 20221122 ('Херсо́н')}, #>       month        = Nov, #>       year         = 2022, #>       note         = {{GNU Parallel is a general parallelizer to run #>                        multiple serial command line programs in parallel #>                        without changing them.}}, #>       publisher    = {Zenodo}, #>       doi          = {10.5281/zenodo.7347980}, #>       url          = {https://doi.org/10.5281/zenodo.7347980} #> }  #> (Feel free to use \\nocite{tange_2022_7347980})  #> This helps funding further development; AND IT WON'T COST YOU A CENT. #> If you pay 10000 EUR you should feel free to use GNU Parallel without citing.  #> More about funding GNU Parallel and the citation notice: #> https://lists.gnu.org/archive/html/parallel/2013-11/msg00006.html #> https://www.gnu.org/software/parallel/parallel_design.html#citation-notice #> https://git.savannah.gnu.org/cgit/parallel.git/tree/doc/citation-notice-faq.txt  #> If you send a copy of your published article to tange@gnu.org, it will be #> mentioned in the release notes of next version of GNU Parallel. #>  #> Type: 'will cite' and press enter.  will cite  #> Thank you for your support: You are the reason why there is funding to #> continue maintaining GNU Parallel. On behalf of future versions of #> GNU Parallel, which would not exist without your support:  #>  THANK YOU SO MUCH  #> It is really appreciated. The citation notice is now silenced. # Install bc sudo apt install bc # Install dos2unix sudo apt install -y dos2unix"},{"path":"/articles/windows_system_setup.html","id":"general-instructions-regarding-paths","dir":"Articles","previous_headings":"","what":"General instructions regarding paths","title":"Setting up the package requirements on Windows","text":"Make sure data stored C:, U: G: drives, otherwise can’t read WSL. Additionaly, avoid using whitespaces paths.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marlene Schürz. Author, maintainer. Afroditi Grigoropoulou. Author. Jaime Garcia Marquez. Author. Yusdiel Torres Cambas. Author. Christoph Schürz. Author. Mathieu Floury. Author. Thomas Tomiczek. Author. Vanessa Bremerich. Author. Merret Buurman. Author. Giuseppe Amatulli. Author. Sami Domisch. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Schürz M, Grigoropoulou , Márquez JG, Torres-Cambas Y, Tomiczek T, Floury M, Bremerich V, Schürz C, Amatulli G, Grossart H, Domisch S (2023). “hydrographr: R package scalable hydrographic data processing.” Methods Ecology Evolution, 0(0), 1-11. doi:10.1111/2041-210X.14226.","code":"@Article{,   title = {hydrographr: An R package for scalable hydrographic data processing},   author = {Marlene Schürz and Afroditi Grigoropoulou and Jaime García Márquez and Yusdiel Torres-Cambas and Thomas Tomiczek and Mathieu Floury and Vanessa Bremerich and Christoph Schürz and Giuseppe Amatulli and Hans-Peter Grossart and Sami Domisch},   journal = {Methods in Ecology and Evolution},   year = {2023},   volume = {0},   number = {0},   pages = {1-11},   doi = {https://doi.org/10.1111/2041-210X.14226}, }"},{"path":"/index.html","id":"hydrographr-","dir":"","previous_headings":"","what":"Scalable Hydrographic Data Processing in R","title":"Scalable Hydrographic Data Processing in R","text":"hydrographr provides collection R function wrappers GDAL GRASS-GIS functions efficiently work Hydrography90m spatial biodiversity data. easy--use functions process large raster vector data directly disk parallel, memory R get overloaded. allows creating scalable data processing analysis workflows R, even though data processed directly R. package contains currently functions: invite users test package provide feedback. Please notify us possible issues, bugs feature requests issues tab top page.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Scalable Hydrographic Data Processing in R","text":"Please see installation guide required tools https://glowabio.github.io/hydrographr/articles/hydrographr.html. Afterwards, use following lines install package R: {r} install.packages(\"remotes\") remotes::install_github(\"glowabio/hydrographr\") library(hydrographr) pdf manual hydrographr package can downloaded .","code":""},{"path":"/index.html","id":"publication","dir":"","previous_headings":"","what":"Publication","title":"Scalable Hydrographic Data Processing in R","text":"can find details package applications recently published paper Methods Ecology Evolution. Please cite hydrographr package : Schürz, M., Grigoropoulou, ., Garcia Marquez, J.R., Tomiczek, T., Floury, M., Schürz, C., Amatulli, G., Grossart, H.-P., Domisch, S. (2023). hydrographr: R package scalable hydrographic data processing. Methods Ecology Evolution, doi:10.1111/2041-210X.14226. Please also cite Hydrography90m data: Amatulli, G., Garcia Marquez, J.R., Sethi, T., Kiesel, J., Grigoropoulou, ., Üblacker, M., Shen, L., Domisch, S. (2022). Hydrography90m: new high-resolution global hydrographic dataset. Earth System Science Data, 14, 4525–4550, doi:10.5194/essd-14-4525-2022. ’re using environmental data, please also cite Environment90m data: Garcia Marquez, J., Amatulli, G., Grigoropoulou, ., Schürz, M., Tomiczek, T., Buurman, M., Bremerich, V., Bego, K. Domisch, S.: Global datasets aggregated environmental variables sub-catchment scale freshwater biodiversity modeling, prep. (Please contact authors --date citation info.) thank NFDI4Biodiversity NFDI4Earth providing funding helped us getting hydrographr package together!","code":""},{"path":"/reference/check_tiles_filesize.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function that checks the size of single files before downloading. It is called and inherits arguments by the function 'download_tiles()' — check_tiles_filesize","title":"Internal function that checks the size of single files before downloading. It is called and inherits arguments by the function 'download_tiles()' — check_tiles_filesize","text":"Internal function checks size single files downloading. called inherits arguments function 'download_tiles()'","code":""},{"path":"/reference/check_tiles_filesize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function that checks the size of single files before downloading. It is called and inherits arguments by the function 'download_tiles()' — check_tiles_filesize","text":"","code":"check_tiles_filesize(   variable,   file_format = \"tif\",   tile_id = NULL,   global = FALSE,   h90m_varnames,   h90m_tile_id,   h90m_file_names,   file_size_table )"},{"path":"/reference/check_tiles_filesize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function that checks the size of single files before downloading. It is called and inherits arguments by the function 'download_tiles()' — check_tiles_filesize","text":"variable character vector variable names. file_format character. Format requested file (\"tif\" \"gpkg\"). tile_id character. ID requested tile regional unit. global logical. TRUE, global extent file downloaded. Default FALSE. h90m_varnames character vector. valid names hydrography90m files available download, (inherited 'download_tiles()'). h90m_tile_id character vector. valid IDs hydrography90m. regular tiles available download (inherited 'download_tiles()'). h90m_file_names character vector. names files available download. Used verify whether requested format valid (inherited 'download_tiles()'). file_size_table data.frame. Lookup table including file names sizes (inherited 'download_tiles()').","code":""},{"path":"/reference/check_wsl.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if WSL and Ubuntu is installed on Windows — check_wsl","title":"Check if WSL and Ubuntu is installed on Windows — check_wsl","text":"Check WSL Ubuntu installed Windows","code":""},{"path":"/reference/check_wsl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if WSL and Ubuntu is installed on Windows — check_wsl","text":"","code":"check_wsl()"},{"path":"/reference/crop_to_extent.html","id":null,"dir":"Reference","previous_headings":"","what":"Crop raster to extent — crop_to_extent","title":"Crop raster to extent — crop_to_extent","text":"function crops input raster layer (.tif) given bounding box (xmin, ymin, xmax, ymax coordinates, spatial object extract bounding box) boundary polygon vector layer (cutline source). cropping performed directly disk, .e. input layer need loaded R. output always written disk, can optionally loaded R SpatRaster (terra package) object (using read = TRUE).","code":""},{"path":"/reference/crop_to_extent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Crop raster to extent — crop_to_extent","text":"","code":"crop_to_extent(   raster_layer,   vector_layer = NULL,   bounding_box = NULL,   out_dir,   file_name,   compression = \"low\",   bigtiff = TRUE,   read = TRUE,   quiet = TRUE )"},{"path":"/reference/crop_to_extent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Crop raster to extent — crop_to_extent","text":"raster_layer character. Full path input raster .tif layer. vector_layer character. Full path vector layer used cutline data source (similar mask operation). bounding_box numeric vector coordinates corners bounding box (xmin, ymin, xmax, ymax), SpatRaster, SpatVector, spatial object. out_dir character. directory output stored. file_name character. Name cropped output raster .tif file. compression character. Compression written output file. Compression levels can defined \"none\", \"low\", \"high\". Default \"low\". bigtiff logical. Define whether output file expected BIGTIFF (file size larger 4 GB). FALSE size > 4GB file written. Default TRUE. read logical. TRUE, cropped raster .tif layer gets read R. FALSE, layer stored disk. Default TRUE. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/crop_to_extent.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Crop raster to extent — crop_to_extent","text":"function returns always .tif raster file written disk. Optionally, SpatRaster (terra object) can loaded R read = TRUE.","code":""},{"path":"/reference/crop_to_extent.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Crop raster to extent — crop_to_extent","text":"Yusdiel Torres-Cambas","code":""},{"path":"/reference/crop_to_extent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Crop raster to extent — crop_to_extent","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Define full path to the input raster .tif layer ans vector layer spi_raster <- paste0(my_directory, \"/hydrography90m_test_data\",                      \"/spi_1264942.tif\") basin_vector <- paste0(my_directory, \"/hydrography90m_test_data\",                       \"/basin_59.gpkg\")  # Crop the Stream Power Index to the basin spi_basin <- crop_to_extent(raster_layer = spi_raster,                             vector_layer = basin_vector,                             out_dir = my_directory,                             file_name = \"spi_basin_cropped.tif\",                             read = TRUE)"},{"path":"/reference/download_env90m_tables.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Environment90m tables — download_env90m_tables","title":"Download Environment90m tables — download_env90m_tables","text":"various `download_something_tables()` functions allow retrieve Environment90m variable names download data Environment90m datasets, split 20°x20° tiles. basically 3 usages: (1) functions called without arguments (.e. without specifying variable names tiles), available variable names returned. (2) subset variables tile IDs specified, download size resulting download computed. (3) subset variables tile IDs specified, `download` set `TRUE`, requested tables downloaded either left zipped files unzipped text files. Multiple regular tiles, e.g. belonging regional units, can downloaded single request. tile IDs can obtained using function [get_tile_id()].","code":""},{"path":"/reference/download_env90m_tables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Environment90m tables — download_env90m_tables","text":"","code":"download_soil_tables(   subset = NULL,   tile_ids = NULL,   download = FALSE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE,   ignore_missing = FALSE,   tempdir = NULL,   quiet = FALSE )  download_flo1k_tables(   subset = NULL,   tile_ids = NULL,   download = FALSE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE,   ignore_missing = FALSE,   tempdir = NULL,   quiet = FALSE )  download_cgiar_tables(   subset = NULL,   tile_ids = NULL,   download = FALSE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE,   ignore_missing = FALSE,   tempdir = NULL,   quiet = FALSE )  download_merit_dem_tables(   subset = NULL,   tile_ids = NULL,   download = FALSE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE,   ignore_missing = FALSE,   tempdir = NULL,   quiet = FALSE )  download_hydrography90m_tables(   subset = NULL,   tile_ids = NULL,   download = FALSE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE,   ignore_missing = FALSE,   tempdir = NULL,   quiet = FALSE )  download_observed_climate_tables(   subset = NULL,   tile_ids = NULL,   download = FALSE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE,   ignore_missing = FALSE,   tempdir = NULL,   quiet = FALSE )  download_projected_climate_tables(   base_vars = NULL,   time_periods = NULL,   models = NULL,   scenarios = NULL,   versions = NULL,   subset = NULL,   tile_ids = NULL,   download = FALSE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE,   ignore_missing = FALSE,   tempdir = NULL,   quiet = FALSE )  download_landcover_tables(   base_vars = NULL,   years = NULL,   subset = NULL,   tile_ids = NULL,   download = FALSE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE,   ignore_missing = FALSE,   tempdir = NULL,   quiet = FALSE )"},{"path":"/reference/download_env90m_tables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Environment90m tables — download_env90m_tables","text":"subset Vector variable names downloaded (string \"\" available variables). tile_ids Vector containing tile ids tiles (e.g. \"h10v04\") downloaded, whose download availability size checked(string \"\" available tiles). download logical. TRUE, `tile_ids` specified, files downloaded IGB server. FALSE, `tile_ids` specified, download size computed. FALSE, `tile_ids` NULL, variable names returned user. download_dir Directory downloads stored. Defaults current working directory \".\". Ignored `download=FALSE`. file_format File format tables, either \"txt\" \"zip\". \"txt\", zipped tables unzipped. \"zip\", downloaded zipped files left . Default \"txt\", means zip files unzipped downloading. Note take space disk zips. delete_zips logical. boolean `FALSE`, downloaded zip files deleted unzipping. Defaults TRUE. ignored request file format zip. ignore_missing logical. requested variables /tile_ids available, frequently caused typo variable name. TRUE, missing misspelled ones ignored others downloaded (warning given ). FALSE, function fail allow user check variable names spelling. Defaults FALSE. tempdir Optional (rarely needed). Path directory store/look temporary various file size tables various Environment90m datasets, required downloaded functions. passed, defaults output [base::tempdir()]. quiet logical. FALSE, informative messages printed. Default FALSE. base_vars (`download_projected_climate_tables()` `download_landcover_tables()`) Vector desired base variables, e.g. landcover variable \"c20_1992\" can expressed base variable \"c20\" year \"1992\". time_periods (`download_projected_climate_...`) Vector desired time periods (leave `NULL` specify `\"\"` available time periods). models (`download_projected_climate_...`) Vector desired models (leave `NULL` specify `\"\"` available models). scenarios (`download_projected_climate_...`) Vector desired scenarios (leave `NULL` specify `\"\"` available scenarios). versions (`download_projected_climate_...`) Vector desired versions (leave `NULL` specify `\"\"` available versions). January 2025, available version \"V.2.1\". years (`download_landcover_...`) Vector desired years (leave `NULL` specify `\"\"` available years).","code":""},{"path":"/reference/download_env90m_tables.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Environment90m tables — download_env90m_tables","text":"named list : * variable names, * components variable names (applies), * dataset name, * requested tile_ids (applies), * download size (`tile_id` specified), * path downloaded files stored (`download=TRUE`), * etc.","code":""},{"path":"/reference/download_env90m_tables.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download Environment90m tables — download_env90m_tables","text":"following table can find variables included Environment90m dataset. column \"Variable\" includes variable names used input parameter \"variable\" function. Likewise, column \"File format\" contains input given \"file_format\" parameter. Environment90m dataset comprises data landcover dataset ESA Land Cover (esa_cci_landcover_v2_1_1), CHELSA v2.1 (chelsa_bioclim_v2_1), SOILGRIDS (soilgrids250m_v2_0) Hydrography90m (hydrography90m_v1_0). visualisations available tiles, details variables Hydrography90m dataset, please refer https://hydrography.org/hydrography90m/hydrography90m_layers/. details bioclimatic variables, especially details scale unit values, please refer http://chelsa-climate.org/. details ESA Land Cover variables, please refer https://www.climatologylab.org/terraclimate.html. Please note values dataset aggregated similar classes (see Environment90m publication). details Soil data, please refer https://soilgrids.org.","code":""},{"path":"/reference/download_env90m_tables.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Download Environment90m tables — download_env90m_tables","text":"download_soil_tables(): Download SOILGRIDS tables (soilgrids250m_v2_0) download_flo1k_tables(): Download flow tables (flo1k_v1_0) download_cgiar_tables(): Download CGIAR-CSI tables (cgiar_csi_v3) download_merit_dem_tables(): Download MERIT-DEM tables (Multi-Error-Removed Improved-Terrain Digital Elevation Model, merit_dem_v1_0_3) download_hydrography90m_tables(): Download Hydrography90m tables (hydrography90m_v1_0) download_observed_climate_tables(): Download CHELSA bioclimatic variables tables, except projections (Climatologies high resolution earth’s land surface areas, chelsa_bioclim_v2_1) download_projected_climate_tables(): Download CHELSA bioclimatic variables, tables, projections (Climatologies high resolution earth’s land surface areas, chelsa_bioclim_v2_1) download_landcover_tables(): Download ESA Land Cover tables (esa_cci_landcover_v2_1_1)","code":""},{"path":"/reference/download_env90m_tables.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Download Environment90m tables — download_env90m_tables","text":"Garcia Marquez J., Amatulli, G., Grigoropoulou, ., Schürz, M., Tomiczek, T., Buurman, M., Bremerich, V., Bego, K. Domisch, S.: Global datasets aggregated environmental variables sub-catchment scale freshwater biodiversity modeling, prep. Please contact authors --date citation info.","code":""},{"path":[]},{"path":"/reference/download_env90m_tables.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Download Environment90m tables — download_env90m_tables","text":"Merret Buurman","code":""},{"path":"/reference/download_env90m_tables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Environment90m tables — download_env90m_tables","text":"","code":"### Soil: soilgrids250m_v2_0 ### # Show all available soil variable names: download_soil_tables()  # Compute download size of all soil variables, for one tile: download_soil_tables(   subset = \"ALL\",   tile_ids = c(\"h00v04\"),   download = FALSE)  # Download one soil variable (Clay content), for two tiles: if (FALSE) { # \\dontrun{ download_soil_tables(   subset = c(\"clyppt\"),   tile_ids = c(\"h00v04\", \"h10v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"zip\") } # }  # Download one soil variable (Clay content), for one tile, # unzip, and delete the zips: if (FALSE) { # \\dontrun{ download_soil_tables(   subset = c(\"clyppt\"),   tile_ids = c(\"h00v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE)  } # }   ### Flow (mean): flo1k_v1_0 ### # Show all available flo1k variable names: download_flo1k_tables()  # Compute download size of the only flo1k_v1_0 variable (mean flow), # for one tile: if (FALSE) { # \\dontrun{ download_flo1k_tables(   subset = \"ALL\",   tile_ids = c(\"h00v04\"),   download = FALSE) } # }  # Download the only flo1k_v1_0 variable (flo1k), for two tiles: if (FALSE) { # \\dontrun{ download_soil_tables(   subset = c(\"flo1k\"),   tile_ids = c(\"h00v04\", \"h10v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"zip\") } # }  # Download the only flo1k_v1_0 variable (flo1k), for one tile, # unzip, and delete the zips: if (FALSE) { # \\dontrun{ download_soil_tables(   subset = c(\"flo1k\"),   tile_ids = c(\"h00v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE) } # }   ### CGIAR CSI dataset: cgiar_csi_v3 ### # Show all available cgiar variable names download_cgiar_tables()  # Compute download size of all cgiar variables, for one tile: download_cgiar_tables(   subset = \"ALL\",   tile_ids = c(\"h00v04\"),   download = FALSE)  # Download one cgiar variable (Global Aridity Index), for two tiles: if (FALSE) { # \\dontrun{ download_cgiar_tables(   subset = c(\"garid\"),   tile_ids = c(\"h00v04\", \"h10v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"zip\") } # }  # Download two cgiar variables (Global Aridity Index, Potential # Evapotranspiration), for one tile, unzip, and delete the zips: if (FALSE) { # \\dontrun{ download_cgiar_tables(   subset = c(\"garid\", \"gevapt\"),   tile_ids = c(\"h00v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE) } # }   ### Digital Elevation Model: merit_dem_v1_0_3 ### # (Multi-Error-Removed Improved-Terrain Digital Elevation Model) # Show all available merit_dem_v1_0_3 variable names download_merit_dem_tables()  # Compute download size of the only merit_dem_v1_0_3 variable # (Mean elevation), for one tile: download_merit_dem_tables(   subset = \"ALL\",   tile_ids = c(\"h00v04\"),   download = FALSE)  # Download the only merit_dem_v1_0_3 variable (mean elevation), # for two tiles: if (FALSE) { # \\dontrun{ download_merit_dem_tables(   subset = c(\"elev\"), # or \"ALL\"   tile_ids = c(\"h00v04\", \"h10v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"zip\") } # }  # Download the only merit_dem_v1_0_3 variable (mean elevation), # for one tile, unzip, and delete the zips: if (FALSE) { # \\dontrun{ download_merit_dem_tables(   subset = c(\"elev\"),   tile_ids = c(\"h00v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE) } # }   ### Hydrography90m: hydrography90m_v1_0 ### # Show all available hy90m variable names download_hydrography90m_tables()  # Compute download size of all hy90m variables, for one tile: download_hydrography90m_tables(   subset = \"ALL\",   tile_ids = c(\"h00v04\"),   download = FALSE)  # Download one hy90m variable (Strahler’s stream order), for two tiles: if (FALSE) { # \\dontrun{ download_hydrography90m_tables(   subset = c(\"stream_strahler\"),   tile_ids = c(\"h00v04\", \"h10v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"zip\") } # }  # Download one hy90m variable (Strahler’s stream order), for one tile, # unzip, and delete the zips: if (FALSE) { # \\dontrun{ download_hydrography90m_tables(   subset = c(\"stream_strahler\"),   tile_ids = c(\"h00v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE)  } # }  ### Bioclimatic Variables: chelsa_bioclim_v2_1 ### ### (excluding projections)                    ### # Show all available bioclim variable names # (excluding projections): download_observed_climate_tables()  # Compute download size of all bioclim variables, for one tile: download_observed_climate_tables(   subset = \"ALL\",   tile_ids = c(\"h00v04\"),   download = FALSE)  # Download one bioclim variable (Annual mean temperature), for two tiles: if (FALSE) { # \\dontrun{ download_observed_climate_tables(   subset = c(\"bio1\"),   tile_ids = c(\"h00v04\", \"h10v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"zip\") } # }  # Download one bioclim variable (Annual mean temperature), for one tile, # unzip, and delete the zips: if (FALSE) { # \\dontrun{ download_observed_climate_tables(   subset = c(\"bio1\"),   tile_ids = c(\"h00v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE) } # }   ### Bioclimatic Variables: chelsa_bioclim_v2_1 ### ### (projections only)                         ### # Show all available projected bioclim variable names download_projected_climate_tables()  # Compute download size of all variables, for one tile: download_projected_climate_tables(   subset = \"ALL\",   tile_ids = c(\"h00v04\"),   download = FALSE)  # Download one variable (Annual mean temperature), for two tiles: if (FALSE) { # \\dontrun{ download_projected_climate_tables(   subset = c(\"bio1\"),   tile_ids = c(\"h00v04\", \"h10v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"zip\") } # }  # Download one variable (Annual mean temperature), for one tile, # unzip, and delete the zips: if (FALSE) { # \\dontrun{ download_projected_climate_tables(   subset = c(\"bio1_2071-2100_ukesm1-0-ll_ssp585_V.2.1\"),   tile_ids = c(\"h00v04\"),   download = TRUE,   download_dir = \".\",   file_format = \"txt\",   delete_zips = TRUE)  } # }   ### Landcover: esa_cci_landcover_v2_1_1 ### # Show all available landcover variable names: download_landcover_tables()  # Compute download size of two landcover base variables (Cropland, rainfed, # and Grassland) and two years, for all tiles:   vars <- download_landcover_tables(     base_vars=c(\"c10\", \"c130\"),     years=c(1992, 1993),     tile_ids=\"ALL\")  # Download two base variables and one year, for two tiles: if (FALSE) { # \\dontrun{   vars <- download_landcover_tables(     base_vars=c(\"c10\", \"c130\"),     years=c(1992),     tile_ids=c(\"h00v04\", \"h10v04\"),     download=TRUE,     download_dir=\"/tmp\",     file_format=\"zip\",     delete_zips=FALSE) } # }"},{"path":"/reference/download_test_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Download test data — download_test_data","title":"Download test data — download_test_data","text":"Download test data package, includes Hydrography90m species point observation data small geographic extent, test functions. test data automatically downloaded unzipped function desired path, can alternatively downloaded https://drive.google.com/file/d/1kYNWXmtVm6X7MZLISOePGpvxB1pk1scD/view?usp=share_link.","code":""},{"path":"/reference/download_test_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download test data — download_test_data","text":"","code":"download_test_data(download_dir = \".\")"},{"path":"/reference/download_test_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download test data — download_test_data","text":"download_dir character. directory files downloaded. Default location working directory.","code":""},{"path":"/reference/download_test_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download test data — download_test_data","text":"Downloads test data Hydrography90m dataset","code":""},{"path":"/reference/download_test_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Download test data — download_test_data","text":"Amatulli, G., Garcia Marquez, J., Sethi, T., Kiesel, J., Grigoropoulou, ., Üblacker, M. M., Shen, L. Q., Domisch, S.: Hydrography90m: new high-resolution global hydrographic dataset, Earth Syst. Sci. Data, 14, 4525–4550, https://doi.org/10.5194/essd-14-4525-2022, 2022. Amatulli G., Garcia Marquez J., Sethi T., Kiesel J., Grigoropoulou ., Üblacker M., Shen L. & Domisch S. (2022-08-09 ). Hydrography90m: new high-resolution global hydrographic dataset. IGB Leibniz-Institute Freshwater Ecology Inland Fisheries. dataset. https://doi.org/10.18728/igb-fred-762.1","code":""},{"path":"/reference/download_test_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Download test data — download_test_data","text":"Afroditi Grigoropoulou","code":""},{"path":"/reference/download_test_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download test data — download_test_data","text":"","code":"# Download the test data to the current working directory download_test_data()  # Download the data to a specific (existing) directory download_test_data(\"path/to/your/directory\")"},{"path":"/reference/download_tiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Download files of the Hydrography90m dataset — download_tiles","title":"Download files of the Hydrography90m dataset — download_tiles","text":"function downloads data Hydrography90m dataset, split 20°x20° tiles. tile ID specified, selected layers (variable) downloaded. addition, Hydrography90m organized non-interrupted drainage basins called regional units. regional unit ID (reg_unit_id) specified, raster mask drainage basin downloaded (useful later processing). Multiple regular tiles, e.g. belonging regional units, can downloaded single request. tile regional unit IDs can obtained using functions \"get_tile_id\" \"get_regional_unit_id\", respectively. files stored locally folder architecture, similar data repository, available https://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path=%2F.","code":""},{"path":"/reference/download_tiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download files of the Hydrography90m dataset — download_tiles","text":"","code":"download_tiles(   variable,   file_format = \"tif\",   tile_id = NULL,   reg_unit_id = NULL,   global = FALSE,   download_dir = \".\" )"},{"path":"/reference/download_tiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download files of the Hydrography90m dataset — download_tiles","text":"variable character vector variable names. See Details variable names. file_format character. Format requested file (\"tif\" \"gpkg\"). See Details. tile_id character vector. IDs requested tiles. reg_unit_id character vector. IDs requested regional units. global logical. TRUE, global extent file downloaded. Default FALSE. download_dir character. directory files downloaded. Default working directory.","code":""},{"path":"/reference/download_tiles.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download files of the Hydrography90m dataset — download_tiles","text":"following table can find variables included Hydrography90m dataset. column \"Variable\" includes variable names used input parameter \"variable\" function. Likewise, column \"File format\" contains input given \"file_format\" parameter. details visualisations spatial layers, please refer https://hydrography.org/hydrography90m/hydrography90m_layers/.","code":""},{"path":"/reference/download_tiles.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Download files of the Hydrography90m dataset — download_tiles","text":"error download file (likely case files bigger 3-4GB), can try manually download file pasting link returned error message browser.","code":""},{"path":"/reference/download_tiles.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Download files of the Hydrography90m dataset — download_tiles","text":"Amatulli G., Garcia Marquez J., Sethi T., Kiesel J., Grigoropoulou ., Üblacker M., Shen L. & Domisch S. (2022-08-09 ) Hydrography90m: new high-resolution global hydrographic dataset. IGB Leibniz-Institute Freshwater Ecology Inland Fisheries. dataset. https://doi.org/10.18728/igb-fred-762.1","code":""},{"path":"/reference/download_tiles.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Download files of the Hydrography90m dataset — download_tiles","text":"Afroditi Grigoropoulou","code":""},{"path":"/reference/download_tiles.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download files of the Hydrography90m dataset — download_tiles","text":"","code":"# Download data for two variables in three regular tiles # to the current working directory download_tiles(variable = c(\"sti\", \"stream_dist_up_farth\"),                file_format = \"tif\",                tile_id = c(\"h00v02\",\"h16v02\", \"h16v04\"))  # Download the global .tif layer for the variable \"direction\" # into the temporary R folder or define a different directory # Define directory my_directory <- tempdir() # Download layer download_tiles(variable = \"direction\",                file_format = \"tif\",                global = TRUE,                download_dir = my_directory)  # Download the raster mask of two regional units # to the current working directory. download_tiles(variable = \"regional_unit\",                file_format = \"tif\",                reg_unit_id = c(\"33\",\"34\"))  # Download the raster mask of all regional units # to the current working directory. download_tiles(variable = \"regional_unit\",                file_format = \"tif\",                global = TRUE)"},{"path":"/reference/download_tiles_base.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function that downloads a single file from https://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path= GDrive folder. It is called and inherits arguments by the function 'download_tiles()'. — download_tiles_base","title":"Internal function that downloads a single file from https://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path= GDrive folder. It is called and inherits arguments by the function 'download_tiles()'. — download_tiles_base","text":"Internal function downloads single file https://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path= GDrive folder. called inherits arguments function 'download_tiles()'.","code":""},{"path":"/reference/download_tiles_base.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function that downloads a single file from https://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path= GDrive folder. It is called and inherits arguments by the function 'download_tiles()'. — download_tiles_base","text":"","code":"download_tiles_base(   variable,   file_format = \"tif\",   tile_id = NULL,   global = FALSE,   download_dir = \".\",   file_size_table = NULL,   server_url = NULL )"},{"path":"/reference/download_tiles_base.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function that downloads a single file from https://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path= GDrive folder. It is called and inherits arguments by the function 'download_tiles()'. — download_tiles_base","text":"variable character vector variable names. file_format character. Format requested file (\"tif\" \"gpkg\"). tile_id character. ID requested tile regional unit. global logical. TRUE, global extent file downloaded. Default FALSE. download_dir character. directory files downloaded. Default working directory. file_size_table data.frame. Lookup table including file names sizes (inherited 'download_tiles()'). server_url character. url home download folder either Nimbus GDrive (inherited 'download_tiles()').","code":""},{"path":"/reference/extract_from_gpkg.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract values from the stream order .gpkg files. — extract_from_gpkg","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"function reads attribute table stream network GeoPackage file (.gpkg) stored disk extracts data one () input sub-catchment (.e. stream segment) IDs. output data.table, output loaded R.","code":""},{"path":"/reference/extract_from_gpkg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"","code":"extract_from_gpkg(   data_dir,   subc_id,   subc_layer,   var_layer,   out_dir = NULL,   file_name = NULL,   n_cores = NULL,   quiet = TRUE )"},{"path":"/reference/extract_from_gpkg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"data_dir character. Path directory containing input data. subc_id numeric vector sub-catchment IDs \"\". \"\", attribute table extracted stream segments input .gpkg layer. stream segment IDs sub-catchment IDs. vector sub-catchment IDs can acquired extract_ids() function, sub-setting resulting data.frame. subc_layer character. Full path sub-catchment ID .tif layer var_layer character vector .gpkg files disk, e.g. \"order_vect_point_h18v04.gpkg\". out_dir character. directory output stored. out_dir specified, attribute tables stored .csv files location, named input variable vector files (e.g. \"/path//stats_order_vect_point_h18v04.csv\"). NULL, output loaded R stored disk. file_name character. Name .csv file output table stored. out_dir also specified purpose. n_cores numeric. Number cores used parallelization, case multiple .gpkg files provided var_layer. NULL, available cores - 1 used. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/extract_from_gpkg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"following attributes stored stream network .gpkg files (produced GRASS GIS function r.stream.order: cat - category stream - sub-catchment / stream segment ID (equal cat) next_stream - downstream sub-catchment / stream segment ID prev_streams; two uptstream sub-catchment / stream segment IDs strahler - Strahler's stream order horton - Hortons's stream order shreve - Shreve's stream magnitude hack - Hack's main streams Gravelius order topo_dim - Topological dimension streams order scheidegger - Scheidegger's Consisted Associated Integers drwal - Drwal's stream hierarchy length - length stream segment stright - length stream segment stright line sinusoid - fractal dimension: stream segment length / stright stream segment length; cum_length - length stream source flow_accum - flow accumulation within sub-catchment stream segment out_dist - distance current stream initialisation outlet source_elev - elevation stream segment initialisation outlet_elev - elevation stream segment outlet elev_drop difference source_elev outlet_elev + drop outlet out_drop - drop outlet stream segment gradient - drop/length","code":""},{"path":"/reference/extract_from_gpkg.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"https://grass.osgeo.org/grass82/manuals/v..ogr.html https://grass.osgeo.org/grass82/manuals/addons/r.stream.order.html","code":""},{"path":"/reference/extract_from_gpkg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"Afroditi Grigoropoulou, Jaime Garcia Marquez, Marlene Schürz","code":""},{"path":"/reference/extract_from_gpkg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"","code":"# Download test data into temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Define path to the directory containing all input data test_data <- paste0(my_directory, \"/hydrography90m_test_data\")  # Define sub-catchment ID layer subc_raster <- paste0(my_directory, \"/hydrography90m_test_data\",                   \"/subcatchment_1264942.tif\")  # Extract the attribute table of the file order_vect_59.gpkg for all the # sub-catchment IDs of the subcatchment_1264942.tif raster layer attribute_table <- extract_from_gpkg(data_dir = test_data,                                      subc_id = \"all\",                                      subc_layer = subc_raster,                                      var_layer = \"order_vect_59.gpkg\",                                      n_cores = 1)  # Show the output table attribute_table"},{"path":"/reference/extract_ids.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract sub-catchment and/or basin IDs — extract_ids","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"Extracts ID value basin /sub-catchment raster layer given point locations. Can also used point-based extraction .tif layer specifying layer \"basin\" parameter. function can used extract sub-catchment IDs present sub-catchment raster well, providing sub-catchment raster layer input.","code":""},{"path":"/reference/extract_ids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"","code":"extract_ids(   data = NULL,   lon,   lat,   id = NULL,   basin_layer = NULL,   subc_layer = NULL,   quiet = TRUE )"},{"path":"/reference/extract_ids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"data data.frame data.table contains columns regarding longitude / latitude coordinates WGS84. data missing sub-catchment raster layer provided, function extract ID value present layer. lon character. name column longitude coordinates. lat character. name column latitude coordinates. id character. name column containing unique IDs row \"data\" (e.g., occurrence site IDs). basin_layer character. Full path .tif layer basin ID. subc_layer character. Full path .tif layer sub-catchment ID. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/extract_ids.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"extraction value given point location basin /sub-catchment raster layer Hydrography90m dataset, GDAL function 'gdallocationinfo' used. point locations defined coordinates WGS84 reference system. function can also used extract value given raster layer WGS84 projection, environmental information stored input raster file. extraction ID values sub-catchment raster layer done terra::unique.","code":""},{"path":"/reference/extract_ids.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"Duplicated rows removed.","code":""},{"path":"/reference/extract_ids.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"https://gdal.org/programs/gdallocationinfo.html","code":""},{"path":"/reference/extract_ids.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"Afroditi Grigoropoulou, Marlene Schürz","code":""},{"path":"/reference/extract_ids.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Example 1: Extracts the ID value at given point locations  # Load occurrence data species_occurrence <- read.table(paste0(my_directory,                                         \"/hydrography90m_test_data\",                                         \"/spdata_1264942.txt\"),                                  header = TRUE)  # Define full path to the basin and sub-catchments raster layer basin_raster <- paste0(my_directory,                      \"/hydrography90m_test_data/basin_1264942.tif\") subc_raster <- paste0(my_directory,                     \"/hydrography90m_test_data/basin_1264942.tif\")  # Extract basin and sub-catchment IDs from the Hydrography90m layers hydrography90m_ids <- extract_ids(data = species_occurrence,                                   lon = \"longitude\",                                   lat = \"latitude\",                                   id = \"occurrence_id\",                                   subc_layer = subc_raster,                                   basin_layer = basin_raster)  # Show the output table hydrography90m_ids  # Example 2: Extract ID values of all subcatchments subc_raster <- paste0(my_directory, \"/hydrography90m_test_data/subcatchment_1264942.tif\") hydrography90m_ids <- extract_ids(subc_layer = subc_raster) fwrite(hydrography90m_ids, paste0(my_directory, '/subc_IDs.txt'))"},{"path":"/reference/extract_zonal_stat.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate zonal statistics — extract_zonal_stat","title":"Calculate zonal statistics — extract_zonal_stat","text":"Calculate zonal statistics based one environmental variable raster .tif layers. function aggregates data 12 summary statistics (mean, min, max, range, ...) selected sub-catchments input file. sub-catchment raster (.tif) input file read directly disk. output data.table loaded R. function can also used zonal statistic calculation specifying raster layer zones subc_layer parameter optionally, also target zone IDs subc_id parameter.","code":""},{"path":"/reference/extract_zonal_stat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate zonal statistics — extract_zonal_stat","text":"","code":"extract_zonal_stat(   data_dir,   subc_id,   subc_layer,   var_layer,   out_dir = NULL,   file_name = NULL,   n_cores = NULL,   quiet = TRUE )"},{"path":"/reference/extract_zonal_stat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate zonal statistics — extract_zonal_stat","text":"data_dir character. Path directory containing input data. subc_id Vector sub-catchment IDs \"\". \"\", zonal statistics calculated sub-catchments given sub-catchment raster layer. vector sub-catchment IDs can acquired extract_ids() function, sub-setting resulting data.frame. subc_layer character. Full path sub-catchment ID .tif layer. var_layer character vector variable raster layers disk, e.g. \"slope_grad_dw_cel_h00v00.tif\". Note variable name appears output table columns (e.g. slope_grad_dw_cel_mean). speed processing, selected variable raster layers can cropped extent sub-catchment layer, e.g. crop_to_extent(). out_dir character. directory output stored. out_dir file_name specified, output table stored .csv file location. NULL, output loaded R stored disk. file_name character. Name .csv file output table stored. out_dir also specified purpose. n_cores numeric. Number cores used parallelization, case multiple .tif files provided var_layer. Default 1. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/extract_zonal_stat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate zonal statistics — extract_zonal_stat","text":"Returns table sub-catchment ID (subc_id) number cells value (data_cells) number cells NoData value (nodata_cells) minimum value (min) maximum value (max) value range (range) arithmetic mean (mean) arithmetic mean absolute values (mean_abs) standard deviation (sd) variance (var) coefficient variation (cv) sum (sum) sum absolute values (sum_abs).","code":""},{"path":"/reference/extract_zonal_stat.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate zonal statistics — extract_zonal_stat","text":"https://grass.osgeo.org/grass82/manuals/r.univar.html","code":""},{"path":[]},{"path":"/reference/extract_zonal_stat.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate zonal statistics — extract_zonal_stat","text":"Afroditi Grigoropoulou, Jaime Garcia Marquez, Marlene Schürz","code":""},{"path":"/reference/extract_zonal_stat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate zonal statistics — extract_zonal_stat","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Define full path to the sub-catchment ID .tif layer subc_raster <-  paste0(my_directory, \"/hydrography90m_test_data\",                        \"/subcatchment_1264942.tif\")  # Define the directory where the output will be stored output_folder <- paste0(my_directory, \"/hydrography90m_test_data/output\") # Create output folder if it doesn't exist if(!dir.exists(output_folder)) dir.create(output_folder)  # Calculate the zonal statistics for all sub-catchments for two variables stat <- extract_zonal_stat(data_dir = paste0(my_directory,                                              \"/hydrography90m_test_data\"),                            subc_id = c(513837216, 513841103,                                        513850467, 513868394,                                        513870312),                            subc_layer = subc_raster,                            var_layer = c(\"spi_1264942.tif\",                                          \"sti_1264942.tif\"),                            out_dir = output_folder,                            file_name = \"zonal_statistics.csv\",                            n_cores = 2) # Show output table stat"},{"path":"/reference/fix_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Fix path for WSL on Windows — fix_path","title":"Fix path for WSL on Windows — fix_path","text":"Fix path WSL Windows","code":""},{"path":"/reference/fix_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fix path for WSL on Windows — fix_path","text":"","code":"fix_path(path)"},{"path":"/reference/fix_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fix path for WSL on Windows — fix_path","text":"path Full Windows path.","code":""},{"path":"/reference/get_all_upstream_distances.html","id":null,"dir":"Reference","previous_headings":"","what":"Get all upstream distances for all subc_id — get_all_upstream_distances","title":"Get all upstream distances for all subc_id — get_all_upstream_distances","text":"Calculates upstream distance subc_id upstream subc_id. output can directly used spatial prioritization analyses e.g. Marxan, Gurobi etc. specify longitudinal connectivity. Note stream segment sub-catchment IDs identical, consistency, use term \"subc_id\". Note distance can extremely long subsequent spatial prioritization analyses might want consider setting cap certain distance.","code":""},{"path":"/reference/get_all_upstream_distances.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get all upstream distances for all subc_id — get_all_upstream_distances","text":"","code":"get_all_upstream_distances(network_table = network_table, n_cores = 1)"},{"path":"/reference/get_all_upstream_distances.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get all upstream distances for all subc_id — get_all_upstream_distances","text":"network_table data.table includes columns c(stream, next_stream, out_dist, latter specifying distance outlet (included Hydrography90m vector attribute table). n_cores numeric. Number cores used parallelisation case multiple stream segments / s. Default 1. Currently, parallelisation process requires copying data core. case graph large, many segments used input, setting n_cores higher value can speed computation. comes however cost possible RAM limitations even slower processing since large data copied core. Hence consider testing n_cores = 1 first. Optional.","code":""},{"path":"/reference/get_all_upstream_distances.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get all upstream distances for all subc_id — get_all_upstream_distances","text":"data.table reports distance (meters) subc_id upstream subc_ids.","code":""},{"path":"/reference/get_all_upstream_distances.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Get all upstream distances for all subc_id — get_all_upstream_distances","text":"Currently attributes provided  (selected subc_id segment). attributes also needed outlet subc_id, next downstream sub_id can selected (enlarge study area)","code":""},{"path":"/reference/get_all_upstream_distances.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get all upstream distances for all subc_id — get_all_upstream_distances","text":"Csardi G, Nepusz T: igraph software package complex network research, InterJournal, Complex Systems 1695. 2006. https://igraph.org","code":""},{"path":[]},{"path":"/reference/get_all_upstream_distances.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get all upstream distances for all subc_id — get_all_upstream_distances","text":"Sami Domisch","code":""},{"path":"/reference/get_all_upstream_distances.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get all upstream distances for all subc_id — get_all_upstream_distances","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load stream network as a graph my_graph <- read_geopackage(gpkg = paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                            import_as = \"graph\")  # Pick a random subc_id subc_id = \"513867228\" # Get the upstream catchment as a data.table network_table <- hydrographr::get_catchment_graph(g = my_graph ,                                                   subc_id = subc_id,                                                   mode = \"in\",                                                   use_outlet = FALSE,                                                   as_graph = FALSE,                                                   n_cores = 1)  ## Condense the table supplied to the function to save RAM keep_these <- c(\"stream\", \"next_stream\", \"out_dist\") network_table <- network_table[, ..keep_these]  ## Change to integers network_table$stream <- as.integer(network_table$stream) network_table$next_stream <- as.integer(network_table$next_stream)   ## Calculate the network distance (in meter) from each subc_id to ## all upstream subc_id using four CPUs for the parallelization result <- get_all_upstream_distances(network_table = network_table,                                       n_cores = 4)"},{"path":"/reference/get_catchment_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"Get catchment from stream network graph — get_catchment_graph","title":"Get catchment from stream network graph — get_catchment_graph","text":"Subset stream network graph extracting upstream, downstream entire catchment, one multiple stream segments. function return either one data.tables graph objects input stream segment. Note stream segment sub-catchment IDs identical, consistency, use term \"subc_id\". switching mode either \"\", \"\" \"\", upstream, downstream connected segments returned, respectively. function read_geopackage() can used create input network graph.","code":""},{"path":"/reference/get_catchment_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get catchment from stream network graph — get_catchment_graph","text":"","code":"get_catchment_graph(   g,   subc_id = NULL,   use_outlet = FALSE,   mode = NULL,   as_graph = FALSE,   n_cores = 1,   max_size = 1500 )"},{"path":"/reference/get_catchment_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get catchment from stream network graph — get_catchment_graph","text":"g igraph object. directed graph. subc_id numeric vector single multiple IDs, e.g (c(ID1, ID2, ID3, ...). sub-catchment (equivalent stream segment) IDs delineate upstream drainage area. empty, outlets used sub-catchment IDs (use_outlet = TRUE). Note can browse entire network online https://geo.igb-berlin.de/maps/351/view left hand side, select \"Stream segment ID\"  layer click map get ID. Optional. use_outlet logical. TRUE, outlets given network graph used additional input subc_ids. Outlets identified internally stream segments downstream connected segment. Default FALSE. mode character. One \"\", \"\" \"\". \"\" returns upstream catchment, \"\" returns downstream catchment (catchments reachable given input segment), \"\" returns . as_graph logical. TRUE, output new graph list new graphs original attributes. FALSE, output  new data.table list data.tables. List objects named subc_ids. Default FALSE. n_cores numeric. Number cores used parallelisation case multiple stream segments / s. Default 1. Currently, parallelisation process requires copying data core. case graph large, many segments used input, setting n_cores higher value can speed computation. comes however cost possible RAM limitations even slower processing since large data copied core. Hence consider testing n_cores = 1 first. Optional. max_size numeric. Specifies maximum size data passed parallel back-end MB. Default 1500 (1.5 GB). Consider higher value large study areas (one 20°x20° tile). Optional.","code":""},{"path":"/reference/get_catchment_graph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get catchment from stream network graph — get_catchment_graph","text":"graph data.table reports subc_ids. case multiple input segments, results stored list.","code":""},{"path":"/reference/get_catchment_graph.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Get catchment from stream network graph — get_catchment_graph","text":"Currently attributes provided  (selected subc_id segment). attributes also needed outlet subc_id, next downstream sub_id can selected (enlarge study area)","code":""},{"path":"/reference/get_catchment_graph.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get catchment from stream network graph — get_catchment_graph","text":"Csardi G, Nepusz T: igraph software package complex network research, InterJournal, Complex Systems 1695. 2006. https://igraph.org","code":""},{"path":[]},{"path":"/reference/get_catchment_graph.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get catchment from stream network graph — get_catchment_graph","text":"Sami Domisch","code":""},{"path":"/reference/get_catchment_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get catchment from stream network graph — get_catchment_graph","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load stream network as a graph my_graph <- read_geopackage(gpkg = paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                            import_as = \"graph\")  # Pick a random subc_id subc_id = \"513855877\" # Get the upstream catchment as a graph g_up <- get_catchment_graph(g = my_graph, subc_id = subc_id, mode = \"in\",                             use_outlet = FALSE, as_graph = TRUE, n_cores = 1)  # Get the downstream segments as a data.table, g_down <- get_catchment_graph(g = my_graph, subc_id = subc_id, mode = \"out\",                               use_outlet = FALSE, as_graph = FALSE, n_cores = 1)  # Get the catchments of all outlets in the study area as a graph g_all <- get_catchment_graph(g = my_graph, mode = \"in\", use_outlet = TRUE,                              as_graph = TRUE, n_cores = 1)"},{"path":"/reference/get_centrality.html","id":null,"dir":"Reference","previous_headings":"","what":"Get centrality indexes from stream network graph — get_centrality","title":"Get centrality indexes from stream network graph — get_centrality","text":"Calculate centrality indexes directed stream network graph. switching mode either \"\", \"\" \"\", upstream, downstream connected segments considered, respectively. function read_geopackage() can used create input network graph.","code":""},{"path":"/reference/get_centrality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get centrality indexes from stream network graph — get_centrality","text":"","code":"get_centrality(g, index = \"all\", mode = NULL)"},{"path":"/reference/get_centrality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get centrality indexes from stream network graph — get_centrality","text":"g igraph object. directed graph. index character. One \"\", \"closeness\", \"farness\", \"betweenness\", \"degree\", \"eccentricity\". See @Details mode character. One \"\", \"\" \"\". Defines whether shortest paths (upstream) (downstream) given segments/sub-catchments calculated. \"\", downstream segments considered. \"\", upstream segments considered. \"\", flow direction ignored streams considered.","code":""},{"path":"/reference/get_centrality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get centrality indexes from stream network graph — get_centrality","text":"data.table reports subc_id centrality values.","code":""},{"path":"/reference/get_centrality.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get centrality indexes from stream network graph — get_centrality","text":"degree node number adjacent edges. Closeness centrality measures many steps required access every node given node Farness centrality sum length shortest paths node nodes. reciprocal closeness (Altermatt, 2013). eccentricity node shortest path distance farthest node graph (West, 1996). node betweenness (roughly) defined number geodesics (shortest paths) going node.","code":""},{"path":"/reference/get_centrality.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get centrality indexes from stream network graph — get_centrality","text":"Csardi G, Nepusz T: igraph software package complex network research, InterJournal, Complex Systems 1695. 2006. https://igraph.org","code":""},{"path":[]},{"path":"/reference/get_centrality.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get centrality indexes from stream network graph — get_centrality","text":"Afroditi Grigoropoulou","code":""},{"path":"/reference/get_centrality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get centrality indexes from stream network graph — get_centrality","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load stream network as a graph my_graph <- read_geopackage(gpkg = paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                            import_as = \"graph\")  # Get the all the centrality indexes centrality <- get_centrality(g = my_graph, index = \"all\", mode = \"in\")  # Load stream network as a vector stream_vect <- read_geopackage(gpkg = paste0(my_directory,                                              \"/hydrography90m_test_data\",                                              \"/order_vect_59.gpkg\"),                                import_as = \"SpatVect\")  # Merge the centrality table with the vector stream_vect <- terra::merge(stream_vect, centrality,                             by.x = c('stream'), by.y=\"subc_id\")  # Write out the stream network vector including the centrality indices writeVector(stream_vect, paste0(my_directory,                                 \"/hydrography90m_test_data\",                                 \"/order_vect_59_centr.gpkg\"))"},{"path":"/reference/get_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate euclidean or along the network distance between points located in one basin — get_distance","title":"Calculate euclidean or along the network distance between points located in one basin — get_distance","text":"Calculate euclidean along--network distance (meters) points located one basin. calculate distance along network, point coordinates need snapped stream network using function snap_to_network() snap_to_subc_segment().","code":""},{"path":"/reference/get_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate euclidean or along the network distance between points located in one basin — get_distance","text":"","code":"get_distance(   data,   lon,   lat,   id,   stream_layer = NULL,   distance = \"both\",   n_cores = 1,   quiet = TRUE )"},{"path":"/reference/get_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate euclidean or along the network distance between points located in one basin — get_distance","text":"data data.frame data.table contains columns regarding longitude / latitude coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates. id character. name column containing unique IDs row \"data\" (e.g., occurrence site IDs). unique IDs need numeric less 10 characters long. stream_layer character. Full path stream network .gpkg file. Needs defined calculate distance along network. (case Hydrography90m, relevant files format \"order_vect_segment_h??v??.gpkg\") distance character. One \"euclidean\", \"network\", \"\". \"euclidean\", euclidean distances pairs points calculated. \"network\", shortest path along network pairs points calculated. (see \"Details\" information). method set \"\", distance measures calculated. Distances given meters. Default \"\". n_cores numeric. Number cores used parallelisation. Default 1. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/get_distance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate euclidean or along the network distance between points located in one basin — get_distance","text":"distance='euclidean', distance matrix, meters, euclidean distances pairs points (object class data.frame) returned. distance='network', data.frame three columns: from_id, to_id, dist returned. 'dist' column includes distance, meters, shortest path along network point \"from_id\" point \"to_id\". distance='', list containing objects returned.","code":""},{"path":"/reference/get_distance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate euclidean or along the network distance between points located in one basin — get_distance","text":"calculate euclidean distance pairs points function uses v.distance command GRASS GIS, set produce square matrix distances. calculation distances along stream network implemented command v.net.allpairs GRASS GIS. along--network distance calculation done pairs points located within basin. points located different basins, function get_distance_parallel() used.","code":""},{"path":"/reference/get_distance.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate euclidean or along the network distance between points located in one basin — get_distance","text":"https://grass.osgeo.org/grass82/manuals/v.net.allpairs.html https://grass.osgeo.org/grass82/manuals/v.distance.html","code":""},{"path":[]},{"path":"/reference/get_distance.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate euclidean or along the network distance between points located in one basin — get_distance","text":"Afroditi Grigoropoulou, Marlene Schürz, Jaime Garcia Marquez","code":""},{"path":"/reference/get_distance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate euclidean or along the network distance between points located in one basin — get_distance","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load occurrence data species_occurrence <- read.table(paste0(my_directory,                             \"/hydrography90m_test_data/spdata_1264942.txt\"),                               header = TRUE)  basin_rast <- paste0(my_directory,                      \"/hydrography90m_test_data/basin_1264942.tif\")  # Define full path to the sub-catchment raster layer subc_rast <- paste0(my_directory,                     \"/hydrography90m_test_data/subcatchment_1264942.tif\")  # Define full path to the vector file of the stream network stream_vect <- paste0(my_directory,                       \"/hydrography90m_test_data/order_vect_59.gpkg\")  # Automatically extract the basin and sub-catchment IDs and # snap the data points to the stream segment snapped_coordinates <- snap_to_subc_segment(data = species_occurrence,                                             lon = \"longitude\",                                             lat = \"latitude\",                                             id = \"occurrence_id\",                                             basin_layer = basin_rast,                                             subc_layer = subc_rast,                                             stream_layer = stream_vect,                                             n_cores = 2) # Show head of output table head(snapped_coordinates)  # Get the euclidean distance and the distance along the network between all # pairs of points distance_table <- get_distance(data = snapped_coordinates,                                lon = \"lon_snap\",                                lat = \"lat_snap\",                                id = \"occurrence_id\",                                stream_layer = stream_vect,                                distance = \"network\") # Show table distance_table"},{"path":"/reference/get_distance_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the distance in meters between stream segments along a network graph — get_distance_graph","title":"Get the distance in meters between stream segments along a network graph — get_distance_graph","text":"Given set input sub-catchment IDs, function calculate network distance meters input pairs. Alternatively, number stream segments (sub-catchment) along paths reported. Note stream segment sub-catchment IDs identical, consistency, use term \"subc_id\". See example code \"Note\" explains use standard igraph functions obtain actual stream segment IDs along path. function read_geopackage() can used create input network graph, function get_catchment_graph() can used subset network graph. Note graph-based function includes also entire length \"\" \"\" stream segment, opposed get_distance() , depending snapping method, includes part \"\" \"\" segement points located, resulting minor differences.","code":""},{"path":"/reference/get_distance_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the distance in meters between stream segments along a network graph — get_distance_graph","text":"","code":"get_distance_graph(   g,   subc_id = NULL,   variable = \"length\",   distance_m = TRUE,   max_size = 1500 )"},{"path":"/reference/get_distance_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the distance in meters between stream segments along a network graph — get_distance_graph","text":"g igraph object. directed graph. subc_id numeric. Vector single multiple IDs, e.g (c(ID1, ID2, ID3, ...). sub-catchment (equivalent stream segment) IDs calculating distances. Note can browse entire network online https://geo.igb-berlin.de/maps/351/view left hand side, select \"Stream segment ID\" layer click map get ID. variable character. Specify attribute / column name graph object cumulated along network path. Default \"length\" used Hydrography90m dataset. needed using \"distance_m = FALSE\". distance_m logical. TRUE, case Hydrography90m dataset, length (meters) network segment along path cumulated total length pairs reported data.table. FALSE, number segments traversed reported output matrix. subc_ids actual path needed, please see example code \"Note\". Default TRUE. max_size numeric. Specifies maximum size data passed parallel back-end MB. Default 1500 (1.5 GB). Consider higher value large study areas (one 20°x20° tile). Optional.","code":""},{"path":"/reference/get_distance_graph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the distance in meters between stream segments along a network graph — get_distance_graph","text":"data.table reports either distance number segments sub_ids.","code":""},{"path":"/reference/get_distance_graph.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Get the distance in meters between stream segments along a network graph — get_distance_graph","text":"getting actual IDs path two sub-catchments, can use igraph function \"all_shortest_paths\": Specify subc_ids: from_subc_id = 513866854 to_subc_id = 513867238 subc_path <- all_shortest_paths(graph = my_graph, = .character(from_subc_id), = .character(to_subc_id), mode = \"\") Extract subc_ids output: subc_path <- .numeric(as_ids(subc_path$res[1])) can attach environmental data sub-catchments along path, using functions read_geopackage extract_zonal_statistics","code":""},{"path":"/reference/get_distance_graph.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get the distance in meters between stream segments along a network graph — get_distance_graph","text":"Csardi G, Nepusz T: igraph software package complex network research, InterJournal, Complex Systems 1695. 2006. https://igraph.org","code":""},{"path":[]},{"path":"/reference/get_distance_graph.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the distance in meters between stream segments along a network graph — get_distance_graph","text":"Sami Domisch","code":""},{"path":"/reference/get_distance_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the distance in meters between stream segments along a network graph — get_distance_graph","text":"","code":"# Download test data into the temporary R folder # or define a different directory  my_directory <- tempdir() download_test_data(my_directory)  # Load stream network as a graph my_graph <- read_geopackage(gpkg = paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                            import_as = \"graph\")  # Assume we have some point data in the following sub-catchment IDs: subc_id <- c(\"513863746\", \"513866851\", \"513867238\")  # ... if you have some point data, use the extract_ids() function: # (the coordinates correspond to the same three subc_ids) my_points <- data.frame(longitude = c(8.885996642821286,                                       8.873352770456831,                                       8.898060225276105),                         latitude = c(42.26533822791161,                                      42.26307509726402,                                      42.25513157316764),                         occurrence_id = c(1, 2, 3))  # Define the path of the sub-catchment raster subc_raster <- paste0(my_directory,                       \"/hydrography90m_test_data/subcatchment_1264942.tif\")   # Extract the sub-catchment IDs for the points: my_points_subc_id <- extract_ids(data = my_points,                                  lon = \"longitude\",                                  lat = \"latitude\",                                  id = \"occurrence_id\",                                  subc_layer = subc_raster)   # Get a vector of the sub-catchment IDs: subc_id <- my_points_subc_id$subcatchment_id  # Get the network distance (in meters) between all input pairs: subc_distances <- get_distance_graph(my_graph,                                      subc_id = subc_id,                                      variable = \"length\",                                      distance_m = TRUE) subc_distances  # Get the number of stream segments that are along the network path: number_segments <- get_distance_graph(my_graph,                                       subc_id = subc_id,                                       variable = \"length\",                                       distance_m = FALSE) number_segments"},{"path":"/reference/get_distance_parallel.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate euclidean or along the network distance between points — get_distance_parallel","title":"Calculate euclidean or along the network distance between points — get_distance_parallel","text":"Calculate euclidean along--network distance (meters) points. calculate distance along network, point coordinates need snapped stream network using function snap_to_network() snap_to_subc_segment().","code":""},{"path":"/reference/get_distance_parallel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate euclidean or along the network distance between points — get_distance_parallel","text":"","code":"get_distance_parallel(   data,   lon,   lat,   id,   basin_id = NULL,   basin_layer = NULL,   stream_layer = NULL,   distance = \"both\",   n_cores = 1,   quiet = TRUE )"},{"path":"/reference/get_distance_parallel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate euclidean or along the network distance between points — get_distance_parallel","text":"data data.frame data.table contains columns regarding longitude / latitude coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates. id character. name column containing unique IDs row \"data\" (e.g., occurrence site IDs). unique IDs need numeric less 10 characters long. basin_id character. name column basin IDs. NULL distance set 'network' '', basin IDs extracted automatically. Default NULL. basin_layer character. Full path basin ID .tif layer. Needs defined calculate distance along network. stream_layer character. Full path stream network .gpkg file. Needs defined calculate distance along network. distance character. One \"euclidean\", \"network\", \"\". \"euclidean\", euclidean distances pairs points calculated. \"network\", shortest path along network pairs points calculated. (see \"Details\" information). method set \"\", distance measures calculated. Distances given meters. Default \"\". n_cores numeric. Number cores used parallelisation. Default 1. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/get_distance_parallel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate euclidean or along the network distance between points — get_distance_parallel","text":"distance='euclidean', distance matrix, meters, euclidean distances pairs points (object class data.frame) returned. distance='network', data.frame three columns: from_id, to_id, dist returned. 'dist' column includes distance, meters, shortest path along network point \"from_id\" point \"to_id\". distance='', list containing objects returned.","code":""},{"path":"/reference/get_distance_parallel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate euclidean or along the network distance between points — get_distance_parallel","text":"calculate euclidian distance pairs points function uses v.distance command GRASS GIS, set produce square matrix distances. calculation distances along stream network implemented command v.net.allpairs GRASS GIS. along--network distance calculation done pairs points located within basin. points located different basins, function can run parallel (.e., core distance calculations points within one basin). distance points located different basins zero connected network.","code":""},{"path":"/reference/get_distance_parallel.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate euclidean or along the network distance between points — get_distance_parallel","text":"https://grass.osgeo.org/grass82/manuals/v.net.allpairs.html https://grass.osgeo.org/grass82/manuals/v.distance.html","code":""},{"path":[]},{"path":"/reference/get_distance_parallel.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate euclidean or along the network distance between points — get_distance_parallel","text":"Afroditi Grigoropoulou, Marlene Schürz, Jaime Garcia Marquez","code":""},{"path":"/reference/get_distance_parallel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate euclidean or along the network distance between points — get_distance_parallel","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load occurrence data species_occurrence <- read.table(paste0(my_directory,                             \"/hydrography90m_test_data/spdata_1264942.txt\"),                               header = TRUE)  basin_rast <- paste0(my_directory,                      \"/hydrography90m_test_data/basin_1264942.tif\")  # Define full path to the sub-catchment raster layer subc_rast <- paste0(my_directory,                     \"/hydrography90m_test_data/subcatchment_1264942.tif\")  # Define full path to the vector file of the stream network stream_vect <- paste0(my_directory,                       \"/hydrography90m_test_data/order_vect_59.gpkg\")  # Automatically extract the basin and sub-catchment IDs and # snap the data points to the stream segment snapped_coordinates <- snap_to_subc_segment(data = species_occurrence,                                             lon = \"longitude\",                                             lat = \"latitude\",                                             id = \"occurrence_id\",                                             basin_layer = basin_rast,                                             subc_layer = subc_rast,                                             stream_layer = stream_vect,                                             n_cores = 2) # Show head of output table head(snapped_coordinates)  # Get the euclidean distance and the distance along the network between all # pairs of points distance_table <- get_distance_parallel(data = snapped_coordinates,                                lon = \"lon_snap\",                                lat = \"lat_snap\",                                id = \"occurrence_id\",                                basin_id = \"basin_id\",                                basin_layer = basin_rast,                                stream_layer = stream_vect,                                distance = \"network\") # Show table distance_table"},{"path":"/reference/get_modelfit_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Get environmental variables at each occurrence and pseudo-absence point location — get_modelfit_table","title":"Get environmental variables at each occurrence and pseudo-absence point location — get_modelfit_table","text":"Get environmental variables species occurrences pseudo-absences given point locations extracting environmental information prediction table produced get_predict_table function see also help(get_predict_table).","code":""},{"path":"/reference/get_modelfit_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get environmental variables at each occurrence and pseudo-absence point location — get_modelfit_table","text":"","code":"get_modelfit_table(   data,   spec,   lon,   lat,   pseudoabs = NULL,   subc,   predict_table,   mod_fit_table,   read = TRUE,   quiet = TRUE )"},{"path":"/reference/get_modelfit_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get environmental variables at each occurrence and pseudo-absence point location — get_modelfit_table","text":"data data.frame data.table contains columns regarding species name longitude / latitude coordinates WGS84. spec character. name column species names. lon character. name column longitude coordinates. lat character. name column latitude coordinates. pseudoabs integer. number pseudo-absences subc character. Full path sub-catchment .tif file sub-catchment ID. predict_table character. Full path predict.csv table (.e., output get_predict_table); see also help(get_predict_table) mod_fit_table character. Full path output.csv table, .e., model fit table file. read logical. TRUE, model .csv table gets read R data.table data.frame. FALSE, table stored disk. Default FALSE. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/get_modelfit_table.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get environmental variables at each occurrence and pseudo-absence point location — get_modelfit_table","text":"Jaime Garcia Marquez, Thomas Tomiczek","code":""},{"path":"/reference/get_modelfit_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get environmental variables at each occurrence and pseudo-absence point location — get_modelfit_table","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load occurrence data  # Define full path to the sub-catchments raster layer  # Define full path to the prediction table predict_tbl <- paste0(my_directory,                      \"/hydrography90m_test_data/projectionTB.csv\")  # Define full path to the output model fit table model_fit <- paste0(my_directory,                      \"/hydrography90m_test_data/model_table.csv\")  # Get table with environmental variables at each occurrence # and pseudo-absence point location modelfit_table <- get_modelfit_table(data = species_occurrence,                                   spec = \"species\",                                   lon = \"long\",                                   lat = \"lat\",                                   pseudoabs = 10000,                                   subc = subc_raster,                                   predict_table =  predict_tbl,                                   mod_fit_table = model_fit)"},{"path":"/reference/get_os.html","id":null,"dir":"Reference","previous_headings":"","what":"Identify the operating system. The function was written by Will Lowe and was copied from here: https://conjugateprior.org/2015/06/identifying-the-os-from-r/ — get_os","title":"Identify the operating system. The function was written by Will Lowe and was copied from here: https://conjugateprior.org/2015/06/identifying-the-os-from-r/ — get_os","text":"Identify operating system. function written Lowe copied : https://conjugateprior.org/2015/06/identifying--os--r/","code":""},{"path":"/reference/get_os.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identify the operating system. The function was written by Will Lowe and was copied from here: https://conjugateprior.org/2015/06/identifying-the-os-from-r/ — get_os","text":"","code":"get_os()"},{"path":"/reference/get_pfafstetter_basins.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Pfafstetter sub-basins — get_pfafstetter_basins","title":"Get Pfafstetter sub-basins — get_pfafstetter_basins","text":"Subset basin catchment nine smaller sub-basins following Pfafstetter basin delineation scheme. functions takes network graph input splits smaller sub-basins following hierarchical topological coding scheme (see Verdin & Verdin (1999) details), using flow accumulation basis. user define sub-catchment (stream segment) ID serves outlet basin. Note can stream segment upstream catchment. input graph can created read_geopackage() get_catchment_graph().","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Pfafstetter sub-basins — get_pfafstetter_basins","text":"","code":"get_pfafstetter_basins(   g,   subc_raster,   out_dir,   file_name,   data_table = FALSE,   n_cores = NULL )"},{"path":"/reference/get_pfafstetter_basins.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Pfafstetter sub-basins — get_pfafstetter_basins","text":"g igraph object. directed graph basin one outlet. outlet can stream / sub-catchment upstream basin split smaller sub-basins. input graph can created read_geopackage() get_catchment_graph(). subc_raster character. Full path sub-catchment raster file basin. need cropped / masked basin, IDs sub-catchments need match input graph. out_dir character. path output directory Pfafstetter raster layer written. needed data.table=FALSE. file_name character. filename extension Pfafstetter raster layer (e.g. 'pfafstetter_raster.tif\"). needed data.table=FALSE. data_table Logical. TRUE, result loaded R 2-column data.table (sub-catchment ID Pfafstetter code). FALSE, result loaded raster (terra object) R written disk. Default FALSE. n_cores numeric. Number cores used parallelisation. Default NULL (= detectCores(logical=FALSE)-1). Optional.","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Pfafstetter sub-basins — get_pfafstetter_basins","text":"Either data.table, raster (terra object) loaded R. case result raster, .tif file written disk.","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Get Pfafstetter sub-basins — get_pfafstetter_basins","text":"can use online map https://geo.igb-berlin.de/maps/351/view identify ID stream segment (use \"Stream segment ID\" layer left)","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get Pfafstetter sub-basins — get_pfafstetter_basins","text":"Verdin, K.L. & Verdin, J.P. (1999). topological system delineation codification Earth’s river basins. Journal Hydrology, 218(1-2), 1-12. doi:10.1016/s0022-1694(99)00011-6","code":""},{"path":[]},{"path":"/reference/get_pfafstetter_basins.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get Pfafstetter sub-basins — get_pfafstetter_basins","text":"Sami Domisch","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Pfafstetter sub-basins — get_pfafstetter_basins","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Import the stream network as a graph # Load stream network as a graph my_graph <- read_geopackage(gpkg = paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                            import_as = \"graph\")  # Subset the graph such that it contains only one basin. You can use # a random ID, i.e. it does not need to be the real outlet of the basin. g_subset <- get_catchment_graph(g = my_graph,                          subc_id = \"513867227\",                          use_outlet = FALSE,                          mode = \"in\",                          as_graph = TRUE)  # Specify the sub-catchment raster file subc_raster <- paste0(my_directory,\"/hydrography90m_test_data\",                      \"/subcatchment_1264942.tif\")  # Specify the output directory out_dir <- my_directory  # Calculate the Pfafstetter sub-basins and write the raster layer to disk ( # and import into R) pfafstetter <- get_pfafstetter_basins(g = g_subset ,                                       subc_raster = subc_raster,                                       out_dir = out_dir,                                       file_name = \"pfafstetter_raster.tif\",                                       data_table = FALSE,                                       n_cores = 4)"},{"path":"/reference/get_predict_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Get predict table — get_predict_table","title":"Get predict table — get_predict_table","text":"function creates table environmental variables specific subset subcatchments.","code":""},{"path":"/reference/get_predict_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get predict table — get_predict_table","text":"","code":"get_predict_table(   variable,   statistics = \"ALL\",   tile_id,   input_var_path,   subcatch_id,   out_file_path,   n_cores = NULL,   read = TRUE,   quiet = TRUE,   tempdir = NULL,   overwrite = FALSE )"},{"path":"/reference/get_predict_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get predict table — get_predict_table","text":"variable character vector variable names. Possible values : variables Env90m dataset, can bew viewed calling 'download__tables()'. details, see '?download_env90m_tables'. statistics character vector statistics names. Possible values \"sd\", \"mean\", \"range\" \"\". Default \"\". tile_id character. IDs tiles interest. input_var_path path directory contains table environmental variables entire tiles. Tables may subdirectories provided directory. subcatch_id path text file subcatchment ids, numeric vector containing subcatchment ids. out_file_path character. path output file created. n_cores numeric. Number cores used parallelization. read logical. TRUE, table environmental variables gets read R. FALSE, table stored disk. Default TRUE. quiet logical. FALSE, standard output printed. Default TRUE. tempdir String. Path directory store/look file size table. passed, defaults output base::tempdir(). overwrite logical. TRUE, output file overwritten . already exists. Useful repeated testing. Default FALSE.","code":""},{"path":"/reference/get_predict_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get predict table — get_predict_table","text":"function returns table sub-catchment ID (subcID) column descriptive statistic variable (eg. bio1_mean: mean variable bio1)","code":""},{"path":"/reference/get_predict_table.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get predict table — get_predict_table","text":"Jaime García Márquez, Yusdiel Torres-Cambas","code":""},{"path":"/reference/get_predict_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get predict table — get_predict_table","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory) # TODO make test data available for download!  # Define variable and tile: var <- c(\"bio1\") tile_id <- c(\"h18v02\")  # Point to input data in_path <- paste0(my_directory, '/hydrography90m_test_data') subc_ids <- paste0(my_directory, '/hydrography90m_test_data/subc_IDs.txt') output <- paste0(my_directory, '/hydrography90m_test_data/predictTB.csv')  # Run the function with 2 cores and calculate all statistics: get_predict_table(variable = var,                   statistics = c(\"ALL\"),                   tile_id = tile_id,                   input_var_path = in_path,                   subcatch_id = subc_ids,                   out_file_path = output,                   read = FALSE, quiet = FALSE,                   n_cores = 2)  # Now you can see the result in /tmp/.../hydrography90m_test_data/predictTB.csv"},{"path":"/reference/get_regional_unit_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Hydrography90m regional unit IDs — get_regional_unit_id","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"Given coordinates input points (WGS84), function identifies IDs regional units Hydrography90m points located. Input point data frame. regional units refer non-interrupted basins (opposed 20°x20° tiles). IDs can used download Hydrography90m regional unit raster mask(s) using download_tiles().","code":""},{"path":"/reference/get_regional_unit_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"","code":"get_regional_unit_id(data, lon, lat, quiet = TRUE)"},{"path":"/reference/get_regional_unit_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"data data.frame data.table contains columns regarding longitude / latitude coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/get_regional_unit_id.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"https://gdal.org/programs/gdallocationinfo.html","code":""},{"path":"/reference/get_regional_unit_id.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"Afroditi Grigoropoulou","code":""},{"path":"/reference/get_regional_unit_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Read the species data species_occurrence <- read.table(paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/spdata_1264942.txt\"),                               header = TRUE)  # Get the regional unit ID get_regional_unit_id(species_occurrence, lon = \"longitude\",                     lat = \"latitude\")"},{"path":"/reference/get_segment_neighbours.html","id":null,"dir":"Reference","previous_headings":"","what":"Get stream segment neighbours — get_segment_neighbours","title":"Get stream segment neighbours — get_segment_neighbours","text":"input stream segment, function reports upstream, downstream, -downstream segments connected input segment(s) within specified neighbour order, option summarize attributes across segments. Note stream segment sub-catchment IDs identical, consistency, use term \"subc_id\". input graph can created read_geopackage() get_catchment_graph(). function can used obtain neighbour stream segments / sub-catchments spatially explicit models spatial prioritization analyses (e.g. Marxan/Gurobi). selecting wider radius, spatial dependency neighbouring segments / sub-catchments can increased. See also get_all_upstream_distances() creates input addressing longitudinal connectivity spatial prioritization.","code":""},{"path":"/reference/get_segment_neighbours.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get stream segment neighbours — get_segment_neighbours","text":"","code":"get_segment_neighbours(   g,   subc_id = NULL,   var_layer = NULL,   stat = NULL,   attach_only = FALSE,   order = 5,   mode = \"in\",   n_cores = 1,   max_size = 1500 )"},{"path":"/reference/get_segment_neighbours.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get stream segment neighbours — get_segment_neighbours","text":"g igraph object. directed graph. subc_id numeric vector input sub-catchment IDs (=stream segment IDs) search connected segments. var_layer character vector. One attributes (variable layers) input graph reported output segment_id (\"to_stream\"). Optional. Default NULL. stat one functions mean, median, min, max, sd (without quotes). Aggregates (summarizes) variables neighbourhood input segment (e.g., average land cover next five upstream segments sub-catchments). Default NULL. attach_only logical. TRUE, selected variables attached segment without aggregation. Default FALSE. order numeric. neighbouring order igraph::ego. Order = 1 immediate neighbours input sub-catchment IDs, order = 2 order 1 plus immediate neighbours sub-catchment IDs order 1, . mode character. One \"\", \"\", \"\". \"\" returns upstream neighbouring segments, \"\" returns downstream segments, \"\" returns . n_cores numeric. Number cores used parallelisation case multiple stream segments / outlets. Default 1. Note parallelisation process requires copying data core. case graph large, many segments used input, setting n_cores higher value can speed computation. comes however cost possible RAM limitations even slower processing since large data copied core. Hence consider testing first n_cores = 1. Optional. max_size numeric. Specifies maximum size data passed parallel back-end MB. Default 1500 (1.5 GB). Consider higher value large study areas (one 20°x20° tile). Optional.","code":""},{"path":"/reference/get_segment_neighbours.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get stream segment neighbours — get_segment_neighbours","text":"data.table indicating connected segments (stream  | to_stream), data.table summarizes attributes neighbours contributing segment. #' @note Currently attributes provided outlet (selected subc_id segment). attributes also needed outlet subc_id, next downstream sub_id can selected (enlarge study area) using e.g. get_catchment_graph().","code":""},{"path":"/reference/get_segment_neighbours.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get stream segment neighbours — get_segment_neighbours","text":"Csardi G, Nepusz T: igraph software package complex network research, InterJournal, Complex Systems 1695. 2006. https://igraph.org","code":""},{"path":[]},{"path":"/reference/get_segment_neighbours.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get stream segment neighbours — get_segment_neighbours","text":"Sami Domisch","code":""},{"path":"/reference/get_segment_neighbours.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get stream segment neighbours — get_segment_neighbours","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load the stream network as graph my_graph <- read_geopackage(gpkg= paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                             import_as = \"graph\")  # Subset the graph and get a smaller catchment my_graph <- get_catchment_graph(g = my_graph, subc_id = 513866048, mode = \"in\",                                 use_outlet = FALSE, as_graph = TRUE, n_cores = 1)   # Get a vector of all segment IDs library(igraph) subc_id <- as_ids(V(my_graph))   # Get all (up-and downstream) directly adjacent neighbours of each segment get_segment_neighbours(g = my_graph, subc_id = subc_id,                        order = 1, mode = \"all\")  # Get the upstream segment neighbours in the 5th order # and report the length and source elevation # for the neighbours of each input segment get_segment_neighbours(g = my_graph, subc_id = subc_id,                        order = 5, mode = \"in\", n_cores = 1,                        var_layer = c(\"length\", \"source_elev\"),                        attach_only = TRUE)  # Get the downstream segment neighbours in the 5th order # and calculate the median length and source elevation # across the neighbours of each input segment get_segment_neighbours(g = my_graph, subc_id = subc_id,                        order = 2, mode =\"out\", n_cores = 1,                        var_layer = c(\"length\", \"source_elev\"),                        stat = median)  # Get the up-and downstream segment neighbours in the 5th order # and report the median length and source elevation # for the neighbours of each input segment get_segment_neighbours(g = my_graph, subc_id = subc_id, order = 2,                        mode = \"all\", n_cores = 1,                        var_layer = c(\"length\", \"source_elev\"),                        stat = mean, attach_only = TRUE)"},{"path":"/reference/get_tile_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Hydrography90m 20°x20° tile ID — get_tile_id","title":"Get the Hydrography90m 20°x20° tile ID — get_tile_id","text":"Identifies 20°x20° tile IDs Hydrography90m data input points located. IDs can used download data using download_tiles(). input data frame point coordinates. orientation, please also see tiles https://hydrography.org/hydrography90m/hydrography90m_layers","code":""},{"path":"/reference/get_tile_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Hydrography90m 20°x20° tile ID — get_tile_id","text":"","code":"get_tile_id(data, lon, lat)"},{"path":"/reference/get_tile_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Hydrography90m 20°x20° tile ID — get_tile_id","text":"data data.frame data.table contains columns regarding longitude / latitude coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates.","code":""},{"path":"/reference/get_tile_id.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the Hydrography90m 20°x20° tile ID — get_tile_id","text":"Afroditi Grigoropoulou","code":""},{"path":"/reference/get_tile_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the Hydrography90m 20°x20° tile ID — get_tile_id","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load species occurrence data species_occurrence <- read.table(paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/spdata_1264942.txt\"),                                  header = TRUE)  # Get the tile ID get_tile_id(data = species_occurrence,             lon = \"longitude\", lat = \"latitude\")"},{"path":"/reference/get_upstream_catchment.html","id":null,"dir":"Reference","previous_headings":"","what":"Delineate the upstream catchment — get_upstream_catchment","title":"Delineate the upstream catchment — get_upstream_catchment","text":"Delineates upstream catchment given point, point considered outlet catchment.","code":""},{"path":"/reference/get_upstream_catchment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delineate the upstream catchment — get_upstream_catchment","text":"","code":"get_upstream_catchment(   data,   id,   lon,   lat,   direction_layer = NULL,   out_dir = NULL,   n_cores = NULL,   compression = \"low\",   bigtiff = TRUE,   quiet = TRUE )"},{"path":"/reference/get_upstream_catchment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delineate the upstream catchment — get_upstream_catchment","text":"data data.frame data.table contains columns regarding longitude / latitude coordinates WGS84. Note points need snapped stream network snap_to_network() snap_to_subc_segment(). id character. name column containing unique IDs row \"data\" (e.g., occurrence site IDs). unique IDs need numeric less 10 characters long. lon character. name column longitude coordinates. lat character. name column latitude coordinates. direction_layer character. Full path flow direction raster file. out_dir Full path directory output(s) stored. output file name includes \"id\" helps identifying  upstream corresponding catchment. n_cores numeric. Number cores used parallelisation. NULL, available cores - 1 used. Default NULL. compression character. Compression written output file. Compression levels can defined \"none\", \"low\", \"high\". Default \"low\", referring compression type \"DEFLATE\" compression level 2. \"high\" refers compression level 9. bigtiff logical. Define whether output file expected BIGTIFF (file size larger 4 GB). FALSE size > 4GB file written. Default TRUE. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/get_upstream_catchment.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Delineate the upstream catchment — get_upstream_catchment","text":"https://grass.osgeo.org/grass82/manuals/r.water.outlet.html https://grass.osgeo.org/grass82/manuals/r.region.html","code":""},{"path":[]},{"path":"/reference/get_upstream_catchment.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Delineate the upstream catchment — get_upstream_catchment","text":"Jaime Garcia Marquez, Afroditi Grigoropoulou, Marlene Schürz","code":""},{"path":"/reference/get_upstream_catchment.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Delineate the upstream catchment — get_upstream_catchment","text":"","code":"# Download test data into temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Before running the function get_upstream_catchment(), snap the points to # to the stream segment. There are multiple ways to snap the points. Here is # one example:  # Load occurrence data species_occurrence <- read.table(paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/spdata_1264942.txt\"),                               header = TRUE)  # Define full path to the basin and sub-catchments raster layer basin_raster <- paste0(my_directory,                        \"/hydrography90m_test_data/basin_1264942.tif\") subc_raster <- paste0(my_directory,                       \"/hydrography90m_test_data/subcatchment_1264942.tif\")  # Define full path to the vector file of the stream network stream_vector <- paste0(my_directory,                         \"/hydrography90m_test_data/order_vect_59.gpkg\")  # Automatically extract the basin and sub-catchment IDs and # snap the data points to the stream segment snapped_coordinates <- snap_to_subc_segment(data = species_occurrence,                                             lon = \"longitude\",                                             lat = \"latitude\",                                             id = \"occurrence_id\",                                             basin_layer = basin_raster,                                             subc_layer = subc_raster,                                             stream_layer = stream_vector,                                             n_cores = 2)  # Define full path to the direction .tif direction_raster <- paste0(my_directory,                            \"/hydrography90m_test_data/direction_1264942.tif\") # Define the path for the output file(s) output_folder <-  paste0(my_directory, \"/upstream_catchments\") if(!dir.exists(output_folder)) dir.create(output_folder) # Get the upstream catchment for each point location get_upstream_catchment(snapped_coordinates,                        lon = \"lon_snap\",                        lat = \"lat_snap\",                        id = \"occurrence_id\",                        direction_layer = direction_raster,                        out_dir = output_folder,                        n_cores = 2)"},{"path":"/reference/get_upstream_variable.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate upstream variables for each sub-catchment — get_upstream_variable","title":"Calculate upstream variables for each sub-catchment — get_upstream_variable","text":"input sub-catchment, function calculates upstream mean, median, min, max, sd sum one variables. Note stream segment sub-catchment IDs identical. input graph can created read_geopackage() get_catchment_graph(). function can used obtain upstream stream segments / sub-catchments species distribution modelling, spatial prioritization analyses (e.g. Marxan/Gurobi) specify connectivity (case \"length\" attribute can used). function accepts input graph one ore outlets (e.g. entire tile many drainage basins). variable aggregated upstream prepared data.table, e.g. extract_zonal_stat().","code":""},{"path":"/reference/get_upstream_variable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate upstream variables for each sub-catchment — get_upstream_variable","text":"","code":"get_upstream_variable(   g,   variable_table = NULL,   subc_id = NULL,   var_layer = NULL,   upstream_stat = NULL,   include_focal = FALSE,   save_up_conn = NULL,   load_up_conn = NULL,   n_cores = 1,   max_size = 3000 )"},{"path":"/reference/get_upstream_variable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate upstream variables for each sub-catchment — get_upstream_variable","text":"g igraph object. directed graph. variable_table data.table includes stream column (corresponding subc_id) well attributes aggregated across upstream network subc_id. Default NULL. subc_id vector subc_id upstream variables calculated. Optional; default use subc_id input graph. var_layer character vector. One attributes (variable layers) variable_table reported output subc_id. Default NULL. upstream_stat one functions mean, median, min, max, sd, sum (without quotes). function used aggregate (summarize) upstream variables subc_id (e.g., average land cover across entire upstream area). Default NULL. include_focal Whether focal subc_id included aggregation include_focal = TRUE default. Set FALSE focal subc_id included upstream aggregation given sub-catchment. save_up_conn character. Provide name .RData file written disk (getwd()), includes intermediate result consisting data.table includes upstream connections subc_id. Useful large study areas avoids re-running possibly time-consuming pre-processing obtain metrics (e.g. mean, sum) variables. data.table called upstream_dt columns stream base. Default FALSE. load_up_conn Optional, used, upstream_dt. case file upstream connections previously written disk (using e.g. save_up_conn = \"my_file\"), data.table can first loaded load(paste0(getwd(), \"/my_file.RData\")), loads upstream_dt data.table columns c(\"stream\", \"base\"). Use  load_up_conn = upstream_dt skip pre-processing. Optional, default NULL. n_cores numeric. Number cores used parallelisation. case graph large, many segments used input, setting n_cores higher value can speed computation. Note however parallelisation process requires copying input graph core. may result possible RAM limitations even slower processing. Hence consider testing first lower number cores. Default n_cores = 1. Optional. max_size numeric. Specifies maximum size data passed parallel back-end MB. Default 1500 (1.5 GB). Consider higher value large study areas (one 20°x20° tile). Optional.","code":""},{"path":"/reference/get_upstream_variable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate upstream variables for each sub-catchment — get_upstream_variable","text":"data.table indicating \"stream\" (=subc_id) upstream variables column names var_layer argument. intermediate output can saved disk (requires setting save_up_conn = \"my_file\").","code":""},{"path":"/reference/get_upstream_variable.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate upstream variables for each sub-catchment — get_upstream_variable","text":"Csardi G, Nepusz T: igraph software package complex network research, InterJournal, Complex Systems 1695. 2006. https://igraph.org","code":""},{"path":[]},{"path":"/reference/get_upstream_variable.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate upstream variables for each sub-catchment — get_upstream_variable","text":"Sami Domisch","code":""},{"path":"/reference/get_upstream_variable.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate upstream variables for each sub-catchment — get_upstream_variable","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() setwd(my_directory) download_test_data(my_directory)  # Load the stream network as graph my_graph <- read_geopackage(gpkg = paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                                           import_as = \"graph\")  # Subset the graph and get a smaller catchment my_graph <- get_catchment_graph(g = my_graph,                                 subc_id = 513867228,                                 mode = \"in\",                                 use_outlet = FALSE,                                 as_graph = TRUE,                                 n_cores = 1)   ## Prepare the variables that should be accumulated. ## Load the table variable_table <- read_geopackage(gpkg = paste0(my_directory,                                                \"/hydrography90m_test_data\",                                               \"/order_vect_59.gpkg\"),                                               import_as = \"data.table\")  ## Specify the layers for the upstream aggregation var_layer= c(\"length\", \"flow_accum\")  ## Subset the table keep_these <- c(\"stream\", var_layer) variable_table <- variable_table[, ..keep_these]   ## Get the upstream sum of the variables \"length\" and \"flow_accum\" for ## single subc_id result <- get_upstream_variable(my_graph,                                 variable_table = variable_table,                                 var_layer = var_layer,                                 upstream_stat=sum,                                 subc_id = c(513861908, 513864129),                                 include_focal = TRUE)   ## Get the upstream sum of the variables \"length\" and \"flow_accum\" across the ## entire network result <- get_upstream_variable(my_graph,                                 variable_table = variable_table,                                 var_layer = var_layer,                                 upstream_stat=sum,                                 include_focal = TRUE,                                 n_cores = 4,                                 save_up_conn = \"my_file\")   ## Alternatively, load the previously generated upstream connections ## and use it directly, skipping the pre-processing load(paste0(getwd(), \"/my_file.RData\"))  result <- get_upstream_variable(my_graph,                                 variable_table = variable_table,                                 var_layer = var_layer,                                 upstream_stat=max,                                 include_focal = TRUE,                                 load_up_conn = upstream_dt)    ## Map the new variable across the network  ## Specify tif-layer for reclassification subc_raster <- paste0(my_directory, \"/hydrography90m_test_data/subcatchment_1264942.tif\") recl_raster <- paste0(my_directory, \"/upstream_sum.tif\")  ## Set columns as integer result <- result[, names(result) := lapply(.SD, as.integer)]  ### Create raster - select the \"to\" column which represents the unique subc_id r <- reclass_raster(data = result,                    rast_val = \"stream\",                    new_val = \"flow_accum\",                    raster_layer = subc_raster,                    recl_layer = recl_raster,                    read = TRUE)  ## Plot the map terra::plot(r, background = \"grey\")"},{"path":"/reference/make_sh_exec.html","id":null,"dir":"Reference","previous_headings":"","what":"Make bash scripts executable — make_sh_exec","title":"Make bash scripts executable — make_sh_exec","text":"Make bash scripts executable","code":""},{"path":"/reference/make_sh_exec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make bash scripts executable — make_sh_exec","text":"","code":"make_sh_exec()"},{"path":"/reference/merge_tiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge raster or vector objects — merge_tiles","title":"Merge raster or vector objects — merge_tiles","text":"Merge multiple raster spatial vector objects disk form new raster spatial vector object larger spatial extent. directory least two raster .tif spatial vector geopackage files provided. Depending input, output .tif .gpkg file (saved out_dir). read = TRUE, output read R SpatRaster (terra package) object case .tif files, SpatVector (terra package) object case .gpkg files.","code":""},{"path":"/reference/merge_tiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge raster or vector objects — merge_tiles","text":"","code":"merge_tiles(   tile_dir,   tile_names,   out_dir,   file_name,   name = \"stream\",   compression = \"low\",   bigtiff = TRUE,   read = FALSE,   quiet = TRUE )"},{"path":"/reference/merge_tiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge raster or vector objects — merge_tiles","text":"tile_dir character. directory containing raster spatial vectors tiles, merged. tile_names character. names files merged, including file extension (.tif .gpkg). out_dir character. directory output stored. file_name character. Name merged output file, including file extension (.tif .gpkg). name character. attribute table column name stream segment (\"stream\"), sub-catchment (\"ID\"), basin (\"ID\") outlet (\"ID\") column used merging GeoPackages. Default \"stream\". compression character. Compression written output file. Compression levels can defined \"none\", \"low\", \"high\". Default \"low\", referring compression type \"DEFLATE\" compression level 2. \"high\" refers compression level 9. bigtiff logical. Define whether output file expected BIGTIFF (file size larger 4 GB). FALSE size > 4GB file written. Default TRUE. read logical. TRUE, merged layer gets read R. FALSE, layer stored disk. Default FALSE. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/merge_tiles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge raster or vector objects — merge_tiles","text":".tif raster file .gpkg spatial vector object always written disk, optionally loaded R.","code":""},{"path":"/reference/merge_tiles.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Merge raster or vector objects — merge_tiles","text":"https://gdal.org/programs/gdalbuildvrt.html https://gdal.org/programs/gdal_translate.html https://gdal.org/programs/ogrmerge.html#ogrmerge https://gdal.org/programs/ogr2ogr.html","code":""},{"path":"/reference/merge_tiles.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Merge raster or vector objects — merge_tiles","text":"Thomas Tomiczek, Jaime Garcia Marquez, Afroditi Grigoropoulou","code":""},{"path":"/reference/merge_tiles.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge raster or vector objects — merge_tiles","text":"","code":"# Download tiles into the temporary R folder # or define a different directory my_directory <- tempdir() download_tiles(variable = \"basin\",                file_format = \"tif\",                tile_id = c(\"h22v08\",\"h22v10\"),                download_dir = my_directory)  # Define folder containing only the tiles, which should me merged tiles_folder <- paste0(my_directory, \"/r.watershed/basin_tiles20d\") # Define output folder output_folder <- paste0(my_directory, \"/merged_tiles\") # Create output folder if it doesn't exist if(!dir.exists(output_folder)) dir.create(output_folder)   # Merge tiles merged_tiles <- merge_tiles(tile_dir = tiles_folder,                             tile_names = c(\"basin_h22v08.tif\", \"basin_h22v10.tif\"),                             out_dir = output_folder,                             file_name = \"basin_merged.tif\",                             read = TRUE)"},{"path":"/reference/read_geopackage.html","id":null,"dir":"Reference","previous_headings":"","what":"Read a GeoPackage file — read_geopackage","title":"Read a GeoPackage file — read_geopackage","text":"Reads entire, subset GeoPackage vector file disk either table (data.table), directed graph object (igraph), spatial dataframe (sf) SpatVect object (terra).","code":""},{"path":"/reference/read_geopackage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read a GeoPackage file — read_geopackage","text":"","code":"read_geopackage(   gpkg,   import_as = \"data.table\",   layer_name = NULL,   subc_id = NULL,   name = \"stream\" )"},{"path":"/reference/read_geopackage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read a GeoPackage file — read_geopackage","text":"gpkg character. Full path GeoPackage file. import_as character. \"data.table\", \"graph\", \"sf\", \"SpatVect\". \"data.table\" imports data data.table. \"graph\" imports layer directed graph (igraph object). option possible network layer (e.g. stream network) needs contain attributes \"stream\" \"next_stream\". \"sf\" imports layer  spatial data frame (sf object). \"SpatVect\" imports layer SpatVector (terra object). Default \"data.table\". layer_name character. Name specific data layer import GeoPackage. specific data layer needs defined GeoPackage contains multiple layers. see available layers function st_layers() R package 'sf' can used. Optional. Default NULL. subc_id numeric. Vector sub-catchment (stream segment) IDs form (c(ID1, ID2, ...) spatial objects attributes GeoPackage imported. Optional. Default NULL. name character. attribute table column name stream segment (\"stream\"), sub-catchment (\"ID\"), basin (\"ID\") outlet (\"ID\") column used subsetting GeoPackage prior importing. Optional. Default \"stream\".","code":""},{"path":"/reference/read_geopackage.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Read a GeoPackage file — read_geopackage","text":"Sami Domisch, Marlene Schürz","code":""},{"path":"/reference/read_geopackage.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read a GeoPackage file — read_geopackage","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)   # Read the stream network as a graph my_graph <- read_geopackage(gpkg = paste0(my_directory,                                           \"/hydrography90m_test_data\",                                           \"/order_vect_59.gpkg\"),                                           import_as = \"graph\")  # Read the stream network as a data.table my_dt <- read_geopackage(gpkg = paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/order_vect_59.gpkg\"))  # Read the stream network as a data.table for specific IDs my_dt <- read_geopackage(gpkg = paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/order_vect_59.gpkg\"),                                        subc_id = c(513833203, 513833594))  # Read the sub-catchments as a SF-object my_sf <- read_geopackage(gpkg = paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/sub_catchment_59.gpkg\"),                                        import_as = \"sf\",                                        layer_name = \"sub_catchment\")  # Read a subset of sub-catchments as a SF-object my_sf <- read_geopackage(gpkg = paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/sub_catchment_59.gpkg\"),                                        import_as = \"sf\",                                        subc_id = c(513833203, 513833594),                                        name = \"ID\")  # Read the basin as SpatVect object my_sv <- read_geopackage(gpkg = paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/basin_59.gpkg\"),                                        import_as = \"SpatVect\")"},{"path":"/reference/reclass_raster.html","id":null,"dir":"Reference","previous_headings":"","what":"Reclassify a raster layer — reclass_raster","title":"Reclassify a raster layer — reclass_raster","text":"Reclassifies raster .tif layer based look-table, output raster contains new values. function uses r.reclass function GRASS GIS. Note input raster needs type integer. input raster layer floating point values, can multiply factor (e.g. 1000) achieve integer values, otherwise GRASS GIS r.reclass round raster values next integer always desired.","code":""},{"path":"/reference/reclass_raster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reclassify a raster layer — reclass_raster","text":"","code":"reclass_raster(   data,   rast_val,   new_val,   raster_layer,   recl_layer,   no_data = -9999,   type = \"Int32\",   compression = \"low\",   bigtiff = TRUE,   read = FALSE,   quiet = TRUE )"},{"path":"/reference/reclass_raster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reclassify a raster layer — reclass_raster","text":"data data.frame data.table original new values written raster. rast_val character. name column original raster values. new_val character. name column new raster values, need integer values. case floating point values, consider multiplying values e.g. 1000 keep three decimals. raster_layer Full path input raster .tif layer. recl_layer character. Full path output .tif layer, .e., reclassified raster file. no_data numeric. no_data value new .tif layer. Default -9999. type character. Data type; Options Byte, Int16, UInt16, Int32, UInt32,CInt16, CInt32. Default Int32. Note integer values allowed. compression character. Compression written output file. Compression levels can defined \"none\", \"low\", \"high\". Default \"low\", referring compression type \"DEFLATE\" compression level 2. \"high\" refers compression level 9. bigtiff logical. Define whether output file expected BIGTIFF (file size larger 4 GB). FALSE size > 4GB file written. Default TRUE. read logical. TRUE, reclassified raster .tif layer gets read R SpatRaster (terra object). FALSE, layer stored disk. Default FALSE. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/reclass_raster.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Reclassify a raster layer — reclass_raster","text":"https://grass.osgeo.org/grass82/manuals/r.reclass.html","code":""},{"path":"/reference/reclass_raster.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Reclassify a raster layer — reclass_raster","text":"Marlene Schürz","code":""},{"path":"/reference/reclass_raster.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reclassify a raster layer — reclass_raster","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Read the stream order for each sub-catchment as a data.table my_dt <- read_geopackage(gpkg= paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                          import_as = \"data.table\")   # Select the stream segment ID and and the Strahler stream order str_ord <- my_dt[,c(\"stream\", \"strahler\")]  # Define input and output raster layer stream_raster <- paste0(my_directory,                         \"/hydrography90m_test_data/stream_1264942.tif\")  recl_raster <- paste0(my_directory,                       \"/hydrography90m_test_data/reclassified_raster.tif\")  # Reclassify the stream network to obtain the Strahler stream order raster str_ord_rast <- reclass_raster(data = str_ord,                                rast_val = \"stream\",                                new_val = \"strahler\",                                raster_layer = stream_raster,                                recl_layer = recl_raster)"},{"path":"/reference/report_no_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Report NoData value — report_no_data","title":"Report NoData value — report_no_data","text":"function reports defined NoData value raster layer. NoData value raster layer represents absence data. computations NoData value can treated different ways. Either NoData value reported Nodata value ignored value computed available values specified location.","code":""},{"path":"/reference/report_no_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Report NoData value — report_no_data","text":"","code":"report_no_data(data_dir, var_layer, n_cores = NULL)"},{"path":"/reference/report_no_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Report NoData value — report_no_data","text":"data_dir character. Path directory containing input data. var_layer character vector variable raster layers disk, e.g. \"slope_grad_dw_cel_h00v00.tif\". n_cores numeric. Number cores used parallelization, case multiple .tif files provided var_layer.","code":""},{"path":"/reference/report_no_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Report NoData value — report_no_data","text":"https://gdal.org/programs/gdalinfo.html","code":""},{"path":"/reference/report_no_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Report NoData value — report_no_data","text":"Afroditi Grigoropoulou, Marlene Schürz","code":""},{"path":"/reference/report_no_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Report NoData value — report_no_data","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Report the NoData value report_no_data(data_dir = paste0(my_directory, \"/hydrography90m_test_data\"),                var_layer = c(\"subcatchment_1264942.tif\", \"flow_1264942.tif\",                              \"spi_1264942.tif\"),                n_core = 2)"},{"path":"/reference/set_no_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Set no data value — set_no_data","title":"Set no data value — set_no_data","text":"Change set NoData value raster layer. change happens -place, meaning original file overwritten disk.","code":""},{"path":"/reference/set_no_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set no data value — set_no_data","text":"","code":"set_no_data(data_dir, var_layer, no_data, quiet = TRUE)"},{"path":"/reference/set_no_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set no data value — set_no_data","text":"data_dir character. Path directory containing input data. var_layer character vector variable layers disk, e.g. c(\"sti_h16v02.tif\", \"slope_grad_dw_cel_h00v00.tif\"). original files overwritten. no_data numeric. desired NoData value. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/set_no_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Set no data value — set_no_data","text":"https://gdal.org/programs/gdal_edit.html","code":""},{"path":"/reference/set_no_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Set no data value — set_no_data","text":"Afroditi Grigoropoulou, Marlene Schürz","code":""},{"path":"/reference/set_no_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set no data value — set_no_data","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Define no data value set_no_data(data_dir = paste0(my_directory, \"/hydrography90m_test_data\"),             var_layer = \"cti_1264942.tif\",             no_data = -9999)"},{"path":"/reference/snap_to_network.html","id":null,"dir":"Reference","previous_headings":"","what":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"Snap points next stream segment within defined radius (map pixels) minimum flow accumulation.","code":""},{"path":"/reference/snap_to_network.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"","code":"snap_to_network(   data,   lon,   lat,   id,   stream_layer,   accu_layer = NULL,   method = \"distance\",   distance = 500,   accumulation = 0.5,   quiet = TRUE )"},{"path":"/reference/snap_to_network.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"data data.frame data.table contains columns regarding longitude / latitude coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates. id character. name column containing unique IDs row \"data\" (e.g., occurrence site IDs). unique IDs need numeric less 10 characters long. stream_layer character. Full path stream network .tif file accu_layer character. Full path flow accumulation .tif file. Needed point snapped next stream segment accumulation value higher flow accumulation threshold (set 'accumulation'). prevents points snapped small stream tributaries. Optional. Default NULL. method character. One \"distance\", \"accumulation\", \"\". Defines points snapped using distance flow accumulation (see \"Details\" information). method set \"\" output contain new coordinates calculations. Default \"distance\" (map pixels). distance numeric. Maximum radius map pixels. points snapped next stream within radius. Default 500. accumulation numeric. Minimum flow accumulation. Points snapped next stream flow accumulation equal higher given value. Default 0.5. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/snap_to_network.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"Returns data.frame snapped coordinates sub-catchment ID snapped stream segment. sub-catchment ID NA, stream segment found within given distance (method = \"distance\") stream segment wad found within given distance flow accumulation equal higher given threshold (method = \"accumulation\"). \"-bbox\" means provided coordinates within extend (bounding box) provided stream network layer.","code":""},{"path":"/reference/snap_to_network.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"function makes use r.stream.snap function available GRASS GIS simultaneously snap number points stream network. distance threshold can specified points snapped stream segment within distance radius (map pixels). However, avoid snapping small tributaries, accumulation threshold can used snapping occurs stream segment equal higher accumulation threshold within given distance radius.","code":""},{"path":"/reference/snap_to_network.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"Duplicated rows removed input data.","code":""},{"path":"/reference/snap_to_network.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"https://grass.osgeo.org/grass78/manuals/addons/r.stream.snap.html","code":""},{"path":"/reference/snap_to_network.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"Marlene Schürz, Jaime Garcia Marquez","code":""},{"path":"/reference/snap_to_network.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load occurrence data species_occurrence <- read.table(paste0(my_directory,                             \"/hydrography90m_test_data/spdata_1264942.txt\"),                               header = TRUE)  # Define full path to stream network and flow accumulation stream_raster <- paste0(my_directory,                      \"/hydrography90m_test_data/stream_1264942.tif\") flow_raster <- paste0(my_directory,                      \"/hydrography90m_test_data/flow_1264942.tif\")  # To calculate the new (snapped) coordinates for a radius and a flow snapped_coordinates <- snap_to_network(data = species_occurrence,                                        lon = \"longitude\",                                        lat = \"latitude\",                                        id = \"occurrence_id\",                                        stream_layer = stream_raster,                                        accu_layer = flow_raster,                                        method = \"both\",                                        distance = 300,                                        accumulation = 0.8)  # Show head of output table head(snapped_coordinates)"},{"path":"/reference/snap_to_subc_segment.html","id":null,"dir":"Reference","previous_headings":"","what":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"Move points stream segment within sub-catchment point located.","code":""},{"path":"/reference/snap_to_subc_segment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"","code":"snap_to_subc_segment(   data,   lon,   lat,   id,   basin_id = NULL,   subc_id = NULL,   basin_layer,   subc_layer,   stream_layer,   n_cores = 1,   quiet = TRUE )"},{"path":"/reference/snap_to_subc_segment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"data data.frame data.table contains columns regarding longitude / latitude coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates. id character. name column containing unique IDs row \"data\" (e.g., occurrence site IDs). unique IDs need numeric less 10 characters long. basin_id character. name column basin IDs. NULL, basin IDs extracted automatically. Optional. Default NULL subc_id character. name column sub-catchment IDs. NULL, sub-catchment IDs extracted automatically. Optional. Default NULL. basin_layer character. Full path basin ID .tif layer. subc_layer character. Full path sub-catchment ID .tif layer. stream_layer character. Full path stream network .gpkg file. n_cores numeric. Number cores used parallelisation. Default 1. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/snap_to_subc_segment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"data.table original new coordinates, along sub-catchment ID.","code":""},{"path":"/reference/snap_to_subc_segment.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"function uses network module GRASS GIS (v.net), connect vector line map (stream network) point map (occurrence/sampling points). masking stream segment sub-catchment target point located, connect operation snaps point stream segment using distance threshold. threshold automatically calculated longest distance two points within sub-catchment. way snapping always take place. new location, function extracts new snapped coordinates.","code":""},{"path":"/reference/snap_to_subc_segment.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"https://grass.osgeo.org/grass82/manuals/v.net.html","code":""},{"path":[]},{"path":"/reference/snap_to_subc_segment.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"Jaime Garcia Marquez, Marlene Schürz","code":""},{"path":"/reference/snap_to_subc_segment.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load occurrence data species_occurrence <- read.table(paste0(my_directory,                             \"/hydrography90m_test_data/spdata_1264942.txt\"),                               header = TRUE) basin_rast <- paste0(my_directory,                      \"/hydrography90m_test_data/basin_1264942.tif\") subc_rast <- paste0(my_directory,                     \"/hydrography90m_test_data/subcatchment_1264942.tif\")  # Define full path to the vector file of the stream network stream_vect <- paste0(my_directory,                       \"/hydrography90m_test_data/order_vect_59.gpkg\")  hydrography90m_ids <- extract_ids(data = species_occurrence,                                   lon = \"longitude\",                                   lat = \"latitude\",                                   id = \"occurrence_id\",                                   subc_layer = subc_rast,                                   basin_layer = basin_rast)  # Snap data points to the stream segment of the provided sub-catchment ID snapped_coordinates <- snap_to_subc_segment(data = hydrography90m_ids,                                             lon = \"longitude\",                                             lat = \"latitude\",                                             id = \"occurrence_id\",                                             basin_id = \"basin_id\",                                             subc_id = \"subcatchment_id\",                                             basin_layer = basin_rast,                                             subc_layer = subc_rast,                                             stream_layer = stream_vect,                                             n_cores = 2) # Show head of output table head(snapped_coordinates)  # OR # Automatically extract the basin and sub-catchment IDs and # snap the data points to the stream segment snapped_coordinates <- snap_to_subc_segment(data = species_occurrence,                                             lon = \"longitude\",                                             lat = \"latitude\",                                             id = \"occurrence_id\",                                             basin_layer = basin_rast,                                             subc_layer = subc_rast,                                             stream_layer = stream_vect,                                             n_cores = 2) # Show head of output table head(snapped_coordinates)"},{"path":"/reference/split_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Split table — split_table","title":"Split table — split_table","text":"Split table along set number rows multiple parts equal length. Note duplicated rows removed.","code":""},{"path":"/reference/split_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split table — split_table","text":"","code":"split_table(data, split = NULL, split_tbl_path, read = FALSE, quiet = TRUE)"},{"path":"/reference/split_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split table — split_table","text":"data data.frame data.table split numeric. number rows selected split table split_tbl_path character. Full path store split tables read logical. TRUE, split data tables get read R data tables. FALSE, tables stored disk. Default FALSE. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/split_table.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Split table — split_table","text":"Jaime Garcia Marquez, Thomas Tomiczek","code":""},{"path":"/reference/split_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split table — split_table","text":"","code":"# Create data table  df <- data.frame(matrix(ncol = 2, nrow = 10000)) colnames(df) <- c('var1', 'var2') # or with real data my_directory <- tempdir() download_test_data(my_directory) df <- fread(paste0(my_directory, '/projectionTB.csv'), fill=TRUE)  # Define full path to store split data tables split_tbl_path <- paste0(my_directory,                      \"/hydrography90m_test_data/\") # Split data table hydrography90m_ids <- split_table(df, split = 20000, split_tbl_path)  # Show the output table hydrography90m_ids"}]
