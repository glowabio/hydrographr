[{"path":"/articles/benchmark.html","id":"introduction-and-scope","dir":"Articles","previous_headings":"","what":"Introduction and scope","title":"Function benchmarks","text":"main focus hydrographr process Hydrography90m (Amatulli et al. 2022) data include workflows R. Nevertheless, hydrographr provides geospatial functions can useful working geospatial data general, goal computationally RAM usage efficient. Benchmark tests selected hydrographr functions demonstrate capabilities R package. compared tested functions corresponding workflows employ terra package put performance perspective without claiming superiority either one R packages . focused five common geospatial tasks can easily implemented hydrographr terra: Merging multiple raster layers one file. Cropping raster layer bounding box extent. Cropping raster layer irregular boundary vector polygon. Reclassifying values raster layer. Zonal statistics raster layer. benchmark experiment tasks consisted reading input data, processing data, writing results back disk (output raster layer). experiment, recorded total run-time entire task using function microbenchmark::microbenchmark. peak RAM usage estimated observing task manager (Windows) top (Linux). ensure reliable estimates total run-time, repeated experiment three times. hydrographr requires different implementations Windows Linux systems. Therefore performed experiments two systems, Windows laptop computer (Windows 11 Pro 10.0.22621/Intel i5-8350U/16 GB RAM) Laptop running Ubuntu Linux (Ubuntu 20.04.6 LTS/Intel i5-10310U/16 GB RAM) identify major performance differences operating systems. code individual geospatial tasks executed platforms documented reproducability. implemented hydrographr terra functions default settings acknowledge specific input arguments may improve performance. Nevertheless benchmark experiments provide first general performance overview.","code":""},{"path":"/articles/benchmark.html","id":"r-packages-and-paths","dir":"Articles","previous_headings":"","what":"R packages and paths","title":"Function benchmarks","text":"Load required libraries: Function definitions: Define working directory:","code":"library(hydrographr) library(terra) library(microbenchmark) library(readr) # Small helper function to print benchmark experiments print_experiment <- function(micro) {   cat(\"Run times in seconds:\\n\")   t <- round(sort(micro$time*1e-9), digits = 1)   names(t) <- c(\"min\", \"median\", \"max\")   t } # Define the working directory for the benchmark experiments wdir <- \"my/working/directory/benchmark\"  data_dir <- paste0(wdir, \"/data\") out_dir  <- paste0(wdir, \"/output\")  # Create a new folder in the working directory to store all the data dir.create(data_dir) dir.create(out_dir)"},{"path":"/articles/benchmark.html","id":"loading-and-preparing-hydrography90m-test-data","dir":"Articles","previous_headings":"","what":"Loading and preparing Hydrography90m test data","title":"Function benchmarks","text":"selected Amazon river basin test case, covers six Hydrography90m tiles total. required tile ids download defined tile_id. calculation zonal statistics want perform flow accumulation. also download respective layer tiles Hydrography90m (defined vars_tif). also load Amazon \"basin\" boundary vector layer use cropping task. download Hydrography90m data can use hydrographr function download_tiles shown . also define extent bounding box Amazon basin. bounding box used benchmark test cropping, also preparation flow accumulation layer. use zonal statistics calculation merge downloaded flow accumulation tiles hydrographr function merge_tilesand crop merged layer defined bounding box crop_to_extent.","code":"# Define tile IDs tile_id <-  c(\"h10v06\",\"h10v08\", \"h10v10\", \"h12v06\", \"h12v08\", \"h12v10\") # Variables in raster format vars_tif <- c(\"sub_catchment\", \"accumulation\") # Variables in vector format vars_gpkg <- \"basin\"  # Download the .tif tiles of the desired variables download_tiles(variable = vars_tif, tile_id = tile_id, file_format = \"tif\",                download_dir = data_dir)  # Download the .gpkg tiles of the desired variables download_tiles(variable = vars_gpkg, tile_id = tile_id, file_format = \"gpkg\",                download_dir =   paste0(wdir, \"/data\")) bbox <- c(-79.6175000, -20.4991667, -50.3400000, 5.2808333) # Path where the flow accumulation tiles where loaded to. acc_dir <- paste0(data_dir, \"/r.watershed/accumulation_tiles20d/\") # List all flow accumulation tiles in the folder. acc_tifs <- list.files(acc_dir)  # Merge the tiles to one flow accumulation layer. merge_tiles(tile_dir = acc_dir,             tile_names = acc_tifs,             out_dir = acc_dir,             file_name = \"acc_merge.tif\")  crop_to_extent(raster_layer = paste0(acc_dir, \"acc_merge.tif\"),                bounding_box = bbox,                out_dir = acc_dir,                file_name =  \"acc_merge_crop.tif\")"},{"path":"/articles/benchmark.html","id":"experiment-1-merging-tiles","dir":"Articles","previous_headings":"","what":"Experiment 1: Merging tiles","title":"Function benchmarks","text":"first benchmark experiment merge 6 tiles cover Amazon basin “.tif” file. run time entire workflow evaluated, includes loading merging input tiles writing outputs back hard drive. tile files (tile_tifs) loaded data folder now define tile_dir. Merging tiles hydrographr performed function merge_tiles. output layer saved out_dir. default compression level (compression) \"low\", results already relatively small output files compromising run time much. tested terra workflow involves reading 6 sub-catchment tiles function rast, merging merge writing result layer writeRaster.","code":"# Folder where the sub-catchment tiles were downloaded to. tile_dir   <- paste0(data_dir, \"/r.watershed/sub_catchment_tiles20d\") # List all tile names in tile_dir tile_tifs  <- list.files(tile_dir, pattern = \".tif$\") # Run the benchmark test 3 times and save the run times in t_merge_hydr t_merge_hydr <- microbenchmark({   merge_tiles(tile_dir = tile_dir,               tile_names = tile_tifs,               out_dir = out_dir,               file_name = \"merge_hydr.tif\",               compression = \"low\") }, times = 3)  # Print the run times print_experiment(t_merge_hydr) ## Run times in seconds: ##    min median    max  ##  160.8  164.7  197.7 # Run the benchmark test 3 times and save the run times in t_merge_terra t_merge_terra <- microbenchmark({   r1 <- rast(paste0(tile_dir, \"/\", tile_tifs[1]))   r2 <- rast(paste0(tile_dir, \"/\", tile_tifs[2]))   r3 <- rast(paste0(tile_dir, \"/\", tile_tifs[3]))   r4 <- rast(paste0(tile_dir, \"/\", tile_tifs[4]))   r5 <- rast(paste0(tile_dir, \"/\", tile_tifs[5]))   r6 <- rast(paste0(tile_dir, \"/\", tile_tifs[6]))   r_merge <- merge(r1, r2, r3, r4, r5, r6)   writeRaster(r_merge, paste0(out_dir, \"/merge_terra.tif\"), overwrite = TRUE) }, times = 3)  # Print the run times print_experiment(t_merge_terra) ## Run times in seconds: ##    min median    max  ##  524.7  524.7  525.2"},{"path":"/articles/benchmark.html","id":"experiment-2-cropping-to-bounding-box","dir":"Articles","previous_headings":"","what":"Experiment 2: Cropping to bounding box","title":"Function benchmarks","text":"following experiment merged layer “merge_hydr.tif” cropped defined bounding box Amazon basin. Cropping hydrographr performed function crop_to_extent. corresponding terra workflow includes loading merged layer rast, cropping layer defined bounding box crop writing cropped layer hard drive writeRaster.","code":"# Run the benchmark test 3 times and save the run times in t_crop_bbox_hydr t_crop_bbox_hydr <- microbenchmark({   crop_to_extent(raster_layer = paste0(out_dir, '/merge_hydr.tif'),                  bounding_box = bbox,                  out_dir = out_dir,                  file_name =  'crop_bbox_hydr.tif') }, times = 3)  # Print the run times print_experiment(t_crop_bbox_hydr) ## Run times in seconds: ##    min median    max  ##  428.9  430.0  430.3 # Convert the numeric extent vector to an extent bboxe <- ext(bbox)  # Run the benchmark test 3 times and save the run times in t_crop_bbox_terra t_crop_bbox_terra <- microbenchmark({   r <- rast(paste0(out_dir, '/merge_hydr.tif'))   r_crop <- crop(r, bboxe)   writeRaster(r_crop, paste0(out_dir, '/crop_bbox_terra.tif'), overwrite = TRUE) }, times = 3)  # Print the run times print_experiment(t_crop_bbox_terra) ## Run times in seconds: ##    min median    max  ##  261.3  261.3  265.9"},{"path":"/articles/benchmark.html","id":"experiment-3-cropping-to-basin-boundary","dir":"Articles","previous_headings":"","what":"Experiment 3: Cropping to basin boundary","title":"Function benchmarks","text":"Experiment 3 generally experiment 2, instead bounding box merged layer cropped masked basin boundary Amazon basin. basin boundary layer shold located data folder. Cropping hydrographr performed function crop_to_extent. Instead bounding_box path ‘gpkg’ input file basin boundary passed function vector_layer. corresponding terra workflow includes loading merged layer rast, loading basin boundary vect, cropping layer basin boundary crop writing cropped layer hard drive writeRaster. crop input argument mask = TRUE set, crop mask layer basin boundary polygon.","code":"basin_gpkg <- paste0(data_dir, \"/basin_514761/basin_514761.gpkg\") # Run the benchmark test 3 times and save the run times in t_crop_vect_hydr t_crop_vect_hydr <- microbenchmark({   crop_to_extent(raster_layer = paste0(out_dir, '/merge_hydr.tif'),                  vector_layer = basin_gpkg,                  out_dir = out_dir,                  file_name =  'crop_vect_hydr.tif') }, times = 3)  # Print the run times print_experiment(t_crop_vect_hydr) ## Run times in seconds: ##    min median    max  ##  539.4  549.4  555.9 # Run the benchmark test 3 times and save the run times in t_crop_vect_terra t_crop_vect_terra <- microbenchmark({   r <- rast(paste0(out_dir, '/merge_terra_linux.tif'))   basin <- vect(basin_gpkg)   r_crop <- crop(r, basin, mask = TRUE)   writeRaster(r_crop, paste0(out_dir, '/crop_vect_terra.tif'), overwrite = TRUE) }, times = 3)  # Print the run times print_experiment(t_crop_vect_terra) ## Run times in seconds: ##    min median    max  ##  257.6  257.6  258.5"},{"path":"/articles/benchmark.html","id":"experiment-4-reclassifying-raster-values","dir":"Articles","previous_headings":"","what":"Experiment 4: Reclassifying raster values","title":"Function benchmarks","text":"reclassification task sub-catchment IDs Amazon basin reassigned new random value. set experiment, table generated includes unique sub-catchment IDs (recl_hydr$id) corresponding “new” random integer number 1 100 (recl_hydr$new). Amazon basin approximately 31 million sub-catchments reclassified. reclassification hydrographr performed function reclass_raster. reclassification sub-catchment IDs generated dummy input table recl_hydr passed function data cropped masked sub-catchment layer used reclass task. corresponding terra workflow includes loading cropped sub-catchment layer rast, reclassification classify writing cropped layer hard drive writeRaster. classify requires matrix defines /values. reclass workflow terra aborted 12 hours progress approximately 10% compared performance hydrographr.","code":"r <- rast(paste0(out_dir, '/crop_vect_hydr.tif')) ids <- terra::unique(r)  recl_hydr <- data.frame(id = ids$crop_vect_hydr_low_linux,                          new = round(runif(nrow(ids), 1, 100))) recl_hydr$id <- as.integer(recl_hydr$id) recl_hydr$new <- as.integer(recl_hydr$new) # Run the benchmark test 3 times and save the run times in t_recl_hydr t_recl_hydr <- microbenchmark({   reclass_raster(data = recl_hydr, rast_val = \"id\", new_val = \"new\",                  raster_layer = paste0(out_dir, '/crop_vect_hydr.tif'),                  recl_layer = paste0(out_dir, '/recl_hydr.tif'),                  read = FALSE, no_data = 0, type = \"Int32\") }, times = 3)  # Print the run times print_experiment(t_recl_hydr) ## Run times in seconds: ##    min median    max  ##  171.0  178.0  184.3 ids_mtx <- as.matrix(recl_hydr)  # Run the benchmark test 3 times and save the run times in t_recl_terra t_recl_terra <-microbenchmark({   r <- rast(paste0(out_dir, '/crop_vect_hydr.tif'))   recl_terra <- classify(r, ids_mtx)   writeRaster(recl_terra, paste0(out_dir, '/recl_terra.tif')) }, times = 5)"},{"path":"/articles/benchmark.html","id":"experiment-5-zonal-statistics","dir":"Articles","previous_headings":"","what":"Experiment 5: Zonal statistics","title":"Function benchmarks","text":"Zonal statistics calculated based cropped masked sub-catchment layer define zones flow accumulation Amazon basin zonal statistics calculated. hydrographr calculates zonal statistics function extract_zonal_stat. current version select specific statistical metrics calculated, returns table statistics including minimum maximum values, range, arithmetic mean, variance standard deviation, coefficient variation, number cells per zone. Therefore, run time calculate metrics compared calculation arithmetic mean standard deviation terra. first test workflow terra mean value flow accumulation sub-catchments calculated. corresponding terra workflow includes loading cropped masked sub-catchment layer rast, loading cropped flow accumulation layer, calculating mean value sub-catchments zonal input argument fun = \"mean\" writing cropped layer hard drive writeRaster. second experiment terra standard deviation calculated. workflow remains . difference input argument fun zonal now set fun = \"sd\". execution experiment resulted memory overflow completed.","code":"# Run the benchmark test 3 times and save the run times in t_zonal_hydr t_zonal_hydr <- microbenchmark({   extract_zonal_stat(data_dir= acc_dir,                      subc_id = \"all\",                      subc_layer = paste0(out_dir, '/crop_vect_hydr.tif'),                      var_layer = \"acc_merge_crop.tif\",                      out_dir = out_dir,                      file_name = \"zonal_hydr.csv\",                      n_cores =1) }, times = 3)  # Print the run times print_experiment(t_zonal_hydr) ## Run times in seconds: ##    min median    max  ##  584.7  585.7  592.4 # Run the benchmark test 3 times and save the run times in t_zonal_terra_mean t_zonal_terra_mean <- microbenchmark({   z <- rast(paste0(out_dir, '/crop_vect_hydr.tif'))   acc <- rast(paste0(acc_dir, \"/acc_merge_crop.tif\"))   zonal_terra_mean <- zonal(acc, z, fun = \"mean\") }, times = 3)  # Print the run times print_experiment(t_zonal_terra_mean) ## Run times in seconds: ##    min median    max  ##  246.6  247.0  251.7 # Run the benchmark test 3 times and save the run times in t_zonal_terra_sd t_zonal_terra_sd <- microbenchmark({   z <- rast(paste0(out_dir, '/crop_vect_hydr.tif'))   acc <- rast(paste0(acc_dir, \"/acc_merge_crop.tif\"))   zonal_terra_sd <- zonal(acc, z, fun = \"sd\") }, times = 3)"},{"path":[]},{"path":"/articles/case_study_brazil.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Case study - São Francisco drainage basin","text":"following example showcases functionality hydrographr package along workflow analyse fragmentation stream network. evaluate number dams - downstream fish occurrence site calculate distance along stream network fish occurrence site closest existing future dam. example focus catfish species Conorhynchos conirostris, catfish species endemic São Francisco drainage basin Brazil. catfish listed ‘endangered’ IUCN red list (IUCN 2022). Wetlands preferred habitats species. Conorhynchos conirostris migratory species, migrates spawning. migratory species, mostly threatened dams along stream network. , threatened fishing spawning season upstream part river. start defining regular tiles Hydrography90m (Amatulli et al. 2022) occurrence points species located. Afterwards download Hydrography90m layers, filter/crop merge . select fish occurrence dam point locations using polygon drainage basin snap point locations stream network. Afterwards, reduce drainage basin using dam downstream fish occurrence sites, Sobradinho dam, outlet location calculate upstream catchment point location. next step, calculate distances along stream network point location (fish dams) get - downstream stream segments point locations graph. Afterwards, query tables evaluate shortest distance next dam well number dams.  Let’s get started!","code":"# Load libraries library(hydrographr) library(terra) library(sf) library(rgbif) library(data.table) library(dplyr) library(stringr) library(purrr) library(forcats) library(leaflet) library(ggplot2) # Define working directory wdir <- \"my/working/directory/data_brazil\" # Set defined working directory setwd(wdir) # Create a directory to store all the data of the case study dir.create(\"data\")"},{"path":"/articles/case_study_brazil.html","id":"species-data","dir":"Articles","previous_headings":"","what":"Species data","title":"Case study - São Francisco drainage basin","text":"start downloading occurrence records Conorhynchos conirostris GBIF.org using R package rgbif. Let’s visualise species occurrences map.","code":"# Download occurrence data with coordinates from GBIF gbif_data <- occ_data(scientificName = \"Conorhynchos conirostris\",                        hasCoordinate = TRUE)  # To cite the data use: # gbif_citation(gbif_data)          # Clean the data spdata <- gbif_data$data %>%    select(decimalLongitude, decimalLatitude, species, occurrenceStatus,           country, year) %>%    filter(!is.na(year)) %>%    distinct() %>%    mutate(occurence_id = 1:nrow(.)) %>%    rename(\"longitude\" = \"decimalLongitude\",          \"latitude\" = \"decimalLatitude\") %>%    select(7, 1:6)  spdata # Define the extent bbox <- c(min(spdata$longitude), min(spdata$latitude),           max(spdata$longitude), max(spdata$latitude))  # Define color palette for the different years of record factpal <- colorFactor(hcl.colors(unique(spdata$year)), spdata$year)  # Create leaflet plot spdata_plot <- leaflet(spdata) %>%   addProviderTiles('Esri.WorldShadedRelief') %>%   setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%   addCircles(lng = ~longitude, lat = ~ latitude, color =  ~factpal(as.factor(year)),               opacity = 1) %>%   addLegend(pal = factpal, values = ~as.factor(year),             title = \"Year of record\") spdata_plot"},{"path":"/articles/case_study_brazil.html","id":"download-files","dir":"Articles","previous_headings":"","what":"Download files","title":"Case study - São Francisco drainage basin","text":"order download layers Hydrography90m, need know IDs 20°x20° tiles occurrence sites located. can obtain IDs using function get_tile_id. function downloads uses auxiliary raster file contains regional units globally, thus requires active internet connection. Currently function returns tiles regional unit input points located . However, tiles may far study area hence always needed steps. Please double check tile IDs relevant purpose using Tile map found . case, São Francisco river basin spreads across three tiles, keep tiles. Let’s define Hydrography90m raster layers GeoPackages like download. use basin raster vector layer, contains drainage basin São Francisco river basin. sub_catchment layer, contains sub-catchment single stream segment unique ID. need layers cropping filtering. need raster layer stream segment flow accumulation snap fish occurrence dam sites stream network. , download raster layer direction calculate upstream catchment defined point location. need Geopackage order_vect_segment, containing stream network, stream (sub-catchment) IDs, ID next stream additional attributes. need GeoPackage calculate distance point locations create graph find stream segments - downstream fish occurrence sites. list available Hydrography90m variables, well details visualisations available .","code":"# Get the tile IDs where the points are located tile_id <- get_tile_id(data = spdata, lon = \"longitude\", lat = \"latitude\") tile_id ## [1] \"h12v08\" \"h12v10\" \"h14v08\" # Define the raster layers vars_tif <- c(\"basin\", \"sub_catchment\", \"segment\", \"accumulation\", \"direction\") # Define the vector layers vars_gpkg <- c(\"basin\", \"order_vect_segment\") # Download the .tif tiles of the defined raster layer download_tiles(variable = vars_tif, tile_id = tile_id, file_format = \"tif\",                download_dir = paste0(wdir, \"/data\"))  # Download the .gpkg tiles of the defined vector layers download_tiles(variable = vars_gpkg, tile_id = tile_id, file_format = \"gpkg\",                download_dir =   paste0(wdir, \"/data\"))"},{"path":"/articles/case_study_brazil.html","id":"filtering-cropping-and-merging","dir":"Articles","previous_headings":"","what":"Filtering, cropping and merging","title":"Case study - São Francisco drainage basin","text":"downloaded layers, need filter crop extent study area. faster processing, recommend filter (vector) crop (raster) files first merge . First, filter GeoPackage tiles containing drainage basins basin ID 481051 (São Francisco drainage basin) merge get extent drainage basin São Francisco river. use polygon drainage basin crop downloaded raster tiles merge . Afterwards, extract unique sub-catchment IDs raster layer filter stream network GeoPackage tiles (order_vect_segment) stream segments sub-catchment ID merge well.","code":"# Define a directory for the São Francisco drainage basin saofra_dir <-  paste0(wdir, \"/data/basin_481051\") if(!dir.exists(saofra_dir)) dir.create(saofra_dir)   # Get the full paths of the basin  GeoPackage tiles basin_dir <- list.files(wdir, pattern = \"basin_h[v0-8]+.gpkg$\",                          full.names = TRUE, recursive = TRUE)  # Filter basin ID from the GeoPackages of the basin tiles # Save the filtered tiles for(itile in basin_dir) {      filtered_tile <- read_geopackage(itile,                                    import_as = \"sf\",                                    subc_id = 481051,                                    name = \"ID\")           write_sf(filtered_tile, paste(saofra_dir,                                 paste0(str_remove(basename(itile), \".gpkg\"),                                        \"_tmp.gpkg\"), sep=\"/\")) }  # Merge filtered GeoPackage tiles merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"basin_.+_tmp.gpkg$\"),             out_dir = saofra_dir,             file_name = \"basin_481051.gpkg\",             name = \"ID\",             read = FALSE)   # Get the full paths of the raster tiles raster_dir <- list.files(paste0(wdir, \"/data/r.watershed\"), pattern = \".tif\",                        full.names = TRUE, recursive = TRUE)  # Crop raster tiles to the extent of the drainage basin for(itile in raster_dir) {      crop_to_extent(raster_layer = itile,                  vector_layer = paste0(saofra_dir, \"/basin_481051.gpkg\"),                  out_dir = saofra_dir,                  file_name =  paste0(str_remove(basename(itile), \".tif\"),                                       \"_tmp.tif\"))    }    # Merge the cropped raster layers of the different variables merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"basin_.+_tmp.tif$\"),             out_dir = saofra_dir,             file_name = \"basin_481051.tif\")  merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"segment_.+_tmp.tif$\"),             out_dir = saofra_dir,             file_name = \"segment_481051.tif\")  merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"accumulation_.+_tmp.tif$\"),             out_dir = saofra_dir,             file_name = \"accumulation_481051.tif\")   merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"direction_.+_tmp.tif$\"),             out_dir = saofra_dir,             file_name = \"direction_481051.tif\")   # Merge the cropped sub-catchment raster layers subc_layer <- merge_tiles(tile_dir = saofra_dir,                           tile_names = list.files(saofra_dir,                                                    full.names = FALSE,                                                   pattern = \"sub_.+_tmp.tif$\"),                           out_dir = saofra_dir,                           file_name = \"sub_catchment_481051.tif\",                           read = TRUE) # Load the merged sub-catchment raster layer of the São Francisco drainage basin subc_layer <- rast(paste0(saofra_dir, \"/sub_catchment_481051.tif\")) # Get all sub-catchment IDs of the São Francisco drainage basin subc_ids <- terra::unique(subc_layer)   # Get the full paths of the stream order segment GeoPackage tiles order_dir <- list.files(wdir, pattern = \"order_.+_h[v0-8]+.gpkg$\",                         full.names = TRUE, recursive = TRUE)  # Filter the sub-catchment IDs from the GeoPackage of the order_vector_segment # tiles (sub-catchment ID = stream ID) # Save the stream segments of the São Francisco drainage basin for(itile in order_dir) {      filtered_tile <- read_geopackage(itile,                                    import_as = \"sf\",                                    subc_id = subc_ids$sub_catchment_481051,                                    name = \"stream\")          write_sf(filtered_tile, paste(saofra_dir,                                 paste0(str_remove(basename(itile), \".gpkg\"),                                        \"_tmp.gpkg\"), sep=\"/\"))   }     # Merge filtered GeoPackage tiles # This process takes a few minutes merge_tiles(tile_dir = saofra_dir,             tile_names = list.files(saofra_dir, full.names = FALSE,                                     pattern = \"order_.+_tmp.gpkg$\"),             out_dir = saofra_dir,             file_name = \"order_vect_segment_481051.gpkg\",             name = \"stream\",             read = FALSE)   # Delete temporary files of the filtered and cropped tiles  tmp_files <-  list.files(saofra_dir, pattern = \"_tmp.\",                           full.names = TRUE, recursive = TRUE) file.remove(tmp_files)"},{"path":"/articles/case_study_brazil.html","id":"filtering-of-the-species-occurrences-and-dams","dir":"Articles","previous_headings":"","what":"Filtering of the species occurrences and dams","title":"Case study - São Francisco drainage basin","text":"information existing future dams provided GRanD (Lehner et al. 2011) FHReD (Zarfl et al. 2015) database, respectively. datasets can downloaded globaldamwatch.org. select point locations fish occurrences dams, located within São Francisco drainage use polygon drainage basin. step use functions R package sf.","code":"# Load the GRanD data shape file grand_pts <- st_read(paste0(wdir,                              \"/data/GRanD_Version_1_3/GRanD_dams_v1_3.shp\")) # Load the FHReD dataset fhred_corr <- fread(paste0(wdir,    \"/data/FHReD_2015/FHReD_2015_future_dams_Zarfl_et_al_beta_version.csv\")) # Load the polygon of the drainage basin basin_poly <- read_sf(paste0(saofra_dir,\"/basin_481051.gpkg\"))  # Create simply features from the data tables  spdata_pts <- st_as_sf(spdata,                          coords = c(\"longitude\",\"latitude\"),                         remove = FALSE, crs=\"WGS84\") fhred_pts <- st_as_sf(fhred_corr,                        coords = c(\"Lon_Cleaned\",\"LAT_cleaned\"),                        crs=\"WGS84\")  # Only keep species occurrences and dams within the drainage basin spdata_pts_481051 <- st_filter(spdata_pts, basin_poly) grand_pts_481051 <- st_filter(grand_pts, basin_poly) fhred_pts_481051 <- st_filter(fhred_pts, basin_poly)   # Transfer the simply featurs back to a data table  and harmonise the column names # Existing dams (ED) existing_dams <- grand_pts_481051 %>%    mutate(longitude = st_coordinates(.)[,1],          latitude = st_coordinates(.)[,2]) %>%    st_drop_geometry() %>%    mutate(type = \"ED\") %>%   rename(site_id = GRAND_ID) %>%    select(site_id, type, longitude, latitude)  # Future dams (Under construction UD, planned PD) future_dams <- fhred_pts_481051 %>%    mutate(longitude = sf::st_coordinates(.)[,1],          latitude = sf::st_coordinates(.)[,2]) %>%    sf::st_drop_geometry() %>%    mutate(type = paste0(.$Stage, \"D\")) %>%    rename(site_id = DAM_ID) %>%    select(site_id, type, longitude, latitude)  # Fish occurrence (FS) species_occurrence <- spdata_pts_481051 %>%   sf::st_drop_geometry() %>%    mutate(type = \"FS\") %>%    rename(site_id = occurence_id) %>%   select(site_id, type, longitude, latitude)  # Bind the data.frames of the dam  and the fish occurrence locations # Remove locations of the same type with duplicated coordinates point_locations <- existing_dams %>%    bind_rows(future_dams) %>%    bind_rows(species_occurrence) %>%    distinct(type, longitude, latitude, .keep_all = TRUE)"},{"path":"/articles/case_study_brazil.html","id":"snap-species-occurrences-and-dams-to-a-stream-segment-within-a-certain-distance-and-with-a-certain-flow-accumulation","dir":"Articles","previous_headings":"","what":"Snap species occurrences and dams to a stream segment within a certain distance and with a certain flow accumulation","title":"Case study - São Francisco drainage basin","text":"can calculate upstream catchment defined point location distance along stream network species occurrences dams, need snap coordinates sites stream network. (Recorded coordinates point locations usually exactly overlap digital stream network therefore, need corrected slightly.) hydrographr package offers two different snapping functions snap_to_network snap_to_subc_segment. first function uses defined distance radius flow accumulation threshold, second function snaps point stream segment sub-catchment point originally located . case study use function snap_to_network able define certain flow accumulation threshold ensure fish occurrences dams snapped headwater stream (first order stream) also higher order stream next .  function implemented -loop start searching streams high flow accumulation 400,000 km² short distance slowly decreases flow accumulation 100 km². still sites left snapped stream segment yet. distance increase 10 30 cells. addition new coordinates function also report unique sub-catchment IDs. cases GRASS GIS function r.stream.snap snap point locations stream network, matter much increase distance radius. happens coordinates need changed slightly within cell point gets snapped. might happen needs done multiple times.","code":"# Define full path to the stream network raster layer stream_rast <- paste0(saofra_dir, \"/segment_481051.tif\") # Define full path to the flow accumulation raster layer flow_rast <- paste0(saofra_dir, \"/accumulation_481051.tif\")   # Define thresholds for the flow accumulation of the stream segment, where # the point location should be snapped to accu_threshold <- c(400000, 300000, 100000, 50000, 10000, 5000, 1000, 500, 100)  # Define the distance radius dist_radius <- c(10, 20, 30)  # Create a temporary data.table point_locations_tmp <- point_locations  # Note: The for loop takes about 9 minutes first <- TRUE for (idist in dist_radius) {         # If the distance increases to 20 cells only a flow accumulation of 100 km²    # will be used    if (idist == 20) {     # Set accu_threshold to 100     accu_threshold <- c(100)    }       for (iaccu in accu_threshold) {     # Snap point locations to the stream network     point_locations_snapped_tmp <- snap_to_network(data = point_locations_tmp,                                                    lon = \"longitude\",                                                    lat = \"latitude\",                                                    id = \"site_id\",                                                    stream_layer = stream_rast,                                                    accu_layer = flow_rast,                                                    method = \"accumulation\",                                                    distance = idist,                                                    accumulation = iaccu,                                                    quiet = FALSE)          # Keep point location with NAs for the next loop     point_locations_tmp <- point_locations_snapped_tmp %>%        filter(is.na(subc_id_snap_accu))      if (first == TRUE) {     # Keep the point locations with the new coordinates and remove rows with NA     point_locations_snapped <- point_locations_snapped_tmp %>%      filter(!is.na(subc_id_snap_accu))     first <- FALSE   } else {     # Bind the new data.frame to the data.frame of the loop before     # and remove the NA     point_locations_snapped <- point_locations_snapped %>%        bind_rows(point_locations_snapped_tmp) %>%        filter(!is.na(subc_id_snap_accu))        }      }      } # Run the snapping function until all points are snapped # Note: The while condition runs a few times and # takes about 5 minutes while(nrow(point_locations_tmp) > 0) {   # Create random number smaller than the size if a cell   rn <- runif(n=1, min=-0.000833333, max=+0.000833333)   # Add random number to the longitude and latitude    point_locations_tmp <- point_locations_tmp %>%      mutate(longitude = longitude + rn,            latitude = latitude + rn)          point_locations_snapped_tmp <- snap_to_network(data = point_locations_tmp,                                                    lon = \"longitude\",                                                    lat = \"latitude\",                                                    id = \"site_id\",                                                    stream_layer = stream_rast,                                                    accu_layer = flow_rast,                                                    method = \"accumulation\",                                                    distance = idist,                                                    accumulation = iaccu,                                                    quiet = FALSE)        # Keep point location with NAs for the next loop   point_locations_tmp <- point_locations_snapped_tmp %>%      filter(is.na(subc_id_snap_accu))        # Bind the new data.frame to the data.frame of the loop before   # and remove the NA   point_locations_snapped <- point_locations_snapped %>%      bind_rows(point_locations_snapped_tmp) %>%      filter(!is.na(subc_id_snap_accu)) }  point_locations_snapped # Load the polygon of the drainage basin basin_poly <- read_sf(paste0(saofra_dir,\"/basin_481051.gpkg\"))  # Define the extent bbox <- c(min(spdata$longitude), min(spdata$latitude),           max(spdata$longitude), max(spdata$latitude))  point_locations_snapped <- point_locations[ ,1:2] %>%    left_join(., point_locations_snapped,              by=c(\"site_id\")) %>%    mutate(type = factor(type, levels = c(\"FS\", \"ED\", \"UD\", \"PD\")))   # Define color palette for the different site types pal <- colorFactor(   palette = c('blue', 'red', 'orange', 'yellow'),   domain = point_locations_snapped$type )  labels <-  c(\"Fish occurrence\", \"Existing dam\",               \"Dam under construction \", \"Planned dam\")  # Create leaflet plot locations_plot <- leaflet() %>%   addProviderTiles('Esri.WorldShadedRelief') %>%   setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%   addPolygons(data = basin_poly ,color = \"#444444\",                weight = 1, smoothFactor = 0.5,               opacity = 1.0, fillOpacity = 0.5 ) %>%    #addPolylines(data = stream_vect) %>%    addCircles(data = point_locations_snapped, lng = ~lon_snap_accu,               lat = ~lat_snap_accu, radius = 0.2, color = ~pal(type),               opacity = 1) %>%   addLegend(data = point_locations_snapped, pal = pal, values = ~type,             labFormat = function(type, cuts, p) {paste0(labels)},             title = \"Site type\")  locations_plot"},{"path":"/articles/case_study_brazil.html","id":"snap-point-locations-to-the-stream-segment-within-the-sub-catchment-with-the-same-unique-id","dir":"Articles","previous_headings":"","what":"Snap point locations to the stream segment within the sub-catchment with the same unique ID","title":"Case study - São Francisco drainage basin","text":"use snap_to_subc_segment example code just show second option snap point locations stream network. Points snapped function always located middle stream segment. calculation function needs unique basin sub- catchment IDs. can done using function extract_ids arguments subc_id basin_id set NULL snap_to_subc_segment function extract IDs well.","code":"# # Define full path to the GeoPackage of the stream segments # stream_vect <- paste0(saofra_dir, \"/order_vect_segment_481051.gpkg\") #  # # # Note: The snapping of 138 points will takes about an hour # # Snap point locations to the stream segment within the sub-catchment the fish # # occurrence or dam site is located # point_data_snapped <- snap_to_subc_segment(data = point_locations, #                                            lon = \"longitude\", #                                            lat = \"latitude\"\", #                                            id = \"site_id\", #                                            basin_id = NULL, #                                            subc_id = NULL, #                                            basin_layer = basin_rast, #                                            subc_layer = subc_rast, #                                            stream_layer = stream_vect, #                                            n_cores = 3)"},{"path":"/articles/case_study_brazil.html","id":"get-upstream-catchment","dir":"Articles","previous_headings":"","what":"Get upstream catchment","title":"Case study - São Francisco drainage basin","text":"website IUCN red list can see habitat range Conorhynchos conirostris restricted area upstream Sobradinho dam (site ID 2516) fish occurrences located upstream dam well. Therefore, interested dams upstream Sobradinho dam. get upstream catchment point location Sobradinho dam use function get_upstream_catchment. Afterwards use terra package polygonise raster file upstream catchment use polygon upstream catchment select point locations using functions provided sf package.","code":"# Define full path for the direction raster layer direction_rast <- paste0(saofra_dir, \"/direction_481051.tif\") # Define full path for the output upcatch_dir <- paste0(saofra_dir, \"/upstream_catchment\") # Create output folder if it doesn't exist if(!dir.exists(upcatch_dir)) dir.create(upcatch_dir)  # Get the upstream catchment of ED 2516 # Increasing the number of cores only makes sense if the calculation # is done for multiple points get_upstream_catchment(as.data.table(point_locations_snapped)[site_id == 2516,],                        lon = \"lon_snap_accu\",                        lat = \"lat_snap_accu\",                        id = \"site_id\",                        direction_layer = direction_rast,                        out_dir = upcatch_dir,                        n_cores = 1)  # Load raster file of the upstream catchment  upstream_basin_rast <- rast(paste0(upcatch_dir , \"/upstream_basin_2516.tif\")) # Polygonise the raster upstream_basin_vect <- as.polygons(upstream_basin_rast) # Save the vector file of the upstream catchment writeVector(upstream_basin_vect, paste0(upcatch_dir ,                                          \"/upstream_basin_2516.gpkg\"),              filetype = \"gpkg\", overwrite=TRUE )  upstream_subc_rast <- crop_to_extent(raster_layer =                                         paste0(saofra_dir,                                               \"/sub_catchment_481051.tif\"),                                      vector_layer =                                          paste0(upcatch_dir ,                                               \"/upstream_basin_2516.gpkg\"),                                      out_dir = upcatch_dir,                                      file_name =  \"sub_catchment_2516.tif\",                                      read = TRUE)   # Get all sub-catchment IDs of the upstream basin subc_ids <- terra::unique(upstream_subc_rast)   # Filter the sub-catchment IDs from the GeoPackages of the order_vector_segment # basin file (sub-catchment ID = stream ID) upstream_segment_vect <- read_geopackage(gpkg =                                              paste0(saofra_dir,                                               \"/order_vect_segment_481051.gpkg\"),                                         import_as = \"sf\",                                         subc_id = subc_ids$sub_catchment_2516,                                         name = \"stream\") # Save the stream segments of the upstream drainage basin   write_sf(upstream_segment_vect, paste0(upcatch_dir,                                         \"/order_vect_segment_2516.gpkg\"))                                 # Convert the data.frame into a simply feature point_locations_snapped_pts <- st_as_sf(point_locations_snapped,                                           coords = c(\"lon_snap_accu\",                                                    \"lat_snap_accu\"),                                          remove = FALSE, crs=\"WGS84\")  # Save point locations as a GeoPackage # write_sf(point_locations_snapped_pts,  #   paste0(upcatch_dir, \"/point_locations_snapped_2516.gpkg\"))  # Use the polygon to filter the upstream point locations upstream_point_locations <- st_filter(point_locations_snapped_pts,                                        st_as_sf(upstream_basin_vect)) %>%    sf::st_drop_geometry()  # Save upstream point locations fwrite(upstream_point_locations, paste0(upcatch_dir,                                          \"/upstream_point_locations_2516.csv\")) # Load the GeoPackage of the upstream basin (if not already loaded) # upstream_basin_vect <- vect(paste0(upcatch_dir , \"/upstream_basin_2516.gpkg\"))  # Create leaflet plot upcatch_plot <- leaflet() %>%   addProviderTiles('Esri.WorldShadedRelief') %>%   setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%   addPolygons(data = st_as_sf(upstream_basin_vect),               color = \"#444444\", weight = 1,                smoothFactor = 0.5, opacity = 1.0,                fillOpacity = 0.5 ) %>%    #addPolylines(data = stream_vect) %>%    addCircles(data = upstream_point_locations,               lng = ~lon_snap_accu, lat = ~lat_snap_accu,               radius = 0.2, color = ~pal(type),              opacity = 1) %>%   addLegend(data = upstream_point_locations,              pal = pal, values = ~type,             labFormat = function(type, cuts, p) {paste0(labels)},             title = \"Site type\")  upcatch_plot"},{"path":"/articles/case_study_brazil.html","id":"get-distance-along-the-stream-network","dir":"Articles","previous_headings":"","what":"Get distance along the stream network","title":"Case study - São Francisco drainage basin","text":"determine distance closest dam - downstream fish occurrence first calculate distance point locations (fish occurrences dams) using function get_distance.","code":"# Define the full path to the upstream basin raster layer and the GeoPackage # of the upstream stream segments  upstream_basin_rast <- paste0(upcatch_dir, \"/upstream_basin_2516.tif\") upstream_segment_vect <- paste0(upcatch_dir, \"/order_vect_segment_2516.gpkg\")  # Get the distances along the stream network between each pair of  # point locations # The process takes about 1 hour  # Increasing the number of cores only makes sense if the calculation # is done for point locations in multiple drainage basins upstream_point_locations <- upstream_point_locations %>%    mutate(basin_id = 2516)  network_distance <- get_distance(data = upstream_point_locations,                                  lon = \"lon_snap_accu\",                                  lat = \"lat_snap_accu\",                                  id = \"site_id\",                                  basin_id = \"basin_id\",                                  basin_layer = upstream_basin_rast,                                  stream_layer = upstream_segment_vect,                                  distance = \"network\",                                  n_cores = 1)"},{"path":"/articles/case_study_brazil.html","id":"get-catchment-graph","dir":"Articles","previous_headings":"","what":"Get catchment graph","title":"Case study - São Francisco drainage basin","text":"evaluate many dames located - downstream fish occurrences, need load GeoPackage stream segments ( order_vect_segment) graph. , need sub-catchment IDs fish occurrences use function get_catchment_graph. function return fish occurrence sub-catchment IDs located - downstream. head example data.frame one fish occurrence site (sub-catchment ID 168036446).","code":"# Define full path to the GeoPackage of the stream segments # upstream_segment_vect <- paste0(upcatch_dir, \"/order_vect_segment_2516.gpkg\")  # Load the GeoPackage as graph stream_network_graph <- read_geopackage(gpkg = upstream_segment_vect,                                          import_as = \"graph\")  # Get sub-catchment IDs of the fish occurrences fs_subc_ids <- upstream_point_locations %>%    rename(subcatchment_id = subc_id_snap_accu) %>%    filter(site_id %in% c(1:2, 4:14)) %>%    .$subcatchment_id %>%   unique() %>%    as.character()  # Get all stream segment IDs (=sub-catchments IDs) upstream of each # fish occurrence # Note: There will be no output for fish occurrences which are located # in headwaters (Strahler order 1) upstream_segments <- get_catchment_graph(g = stream_network_graph,                                          subc_id = fs_subc_ids, mode = \"in\")  # Get all stream segment IDs downstream for each # fish occurrence # Note: There will be no output for fish occurrences which are located # in the last segments before the outlet downstream_segments <- get_catchment_graph(g = stream_network_graph,                                            subc_id = fs_subc_ids, mode = \"out\")"},{"path":"/articles/case_study_brazil.html","id":"determine-the-number-of-dams-and-the-distance-along-the-network-to-the-closest-dam","dir":"Articles","previous_headings":"","what":"Determine the number of dams and the distance along the network to the closest dam","title":"Case study - São Francisco drainage basin","text":"determined distance point locations sub-catchment IDs - downstream fish occurrence, can estimate number dams evaluate distance closest dam.  exception fish occurrence sites 4 6 number upstream dams dramatically increase. Especially site 9, already 14 dams located upstream, number might increase 94 dams future. Downstream fish occurrence sites currently one existing dam. dam ID 2516 outlet delineated upstream catchment. leads assumption fish occurrence sites currently connected. future number dams increase fish occurrence sites 7 8 one two dames. future dam might lead disconnection two sites others.  distance next existing dam upstream ranges 12.6km fish occurrence site 8 528.9km fish occurrence site 9. sites distance next dam upstream decrease future. fish occurrence sites 7 8 next future dam upstream farther away existing dam. distance existing dam downstream ranges 415.2km 1672.0km. future distance next dam downstream decrease 1475km 114km fish occurrence sites 7 8.","code":"# Get all sub-catchment IDs of fish occurrences with stream segments upstream fs_subc_ids_up <- names(upstream_segments)  # Get unique site IDs of the fish occurrences fs_site_id <- upstream_point_locations %>%    filter(subc_id_snap_accu %in% fs_subc_ids_up) %>%    arrange(match(subc_id_snap_accu, fs_subc_ids_up)) %>%    select(site_id) %>%    rename(from_site_id = site_id)  # Get the sub-catchment IDs where the existing dams are located ed_site_id <- upstream_point_locations %>%    filter(type == \"ED\") %>%    rename(stream =  subc_id_snap_accu) %>%    mutate(stream = as.character(stream))  # Get the number of existing dams upstream and the distance to the closest # dam upstream ed_upstream <- upstream_segments %>%    map(., ~ inner_join(ed_site_id, .x, by= \"stream\")) %>%    map2(., fs_site_id$from_site_id, ~ mutate(.x, from_site_id = .y)) %>%    list_rbind(.) %>%    rename(., to_site_id = site_id) %>%    select(., from_site_id, to_site_id) %>%    inner_join(network_distance, ., by=c(\"from_site_id\", \"to_site_id\")) %>%    group_by(from_site_id) %>%    mutate(., n_dams = n()) %>%    filter(., dist == min(dist)) %>%    left_join(fs_site_id, ., by = \"from_site_id\") %>%    mutate(type = \"ED\",          direction = \"upstream\")  # Get the sub-catchment IDs where the future dams will be located fd_site_id <- upstream_point_locations %>%    filter(site_id > 100 & site_id < 2000) %>%    rename(stream =  subc_id_snap_accu) %>%    mutate(stream = as.character(stream)) %>%    inner_join(.,point_locations, by = c(\"site_id\", \"longitude\", \"latitude\"))  # Get the number of future dams upstream and the distance to the closest # dam upstream fd_upstream <- upstream_segments %>%    map(., ~ inner_join(fd_site_id, .x, by= \"stream\")) %>%    map2(., fs_site_id$from_site_id, ~ mutate(.x, from_site_id = .y)) %>%    list_rbind(.) %>%    rename(., to_site_id = site_id) %>%    select(., from_site_id, to_site_id) %>%    inner_join(network_distance, ., by=c(\"from_site_id\", \"to_site_id\")) %>%    group_by(from_site_id) %>%    mutate(., n_dams = n()) %>%    filter(., dist == min(dist)) %>%    left_join(fs_site_id, ., by = \"from_site_id\")  %>%    mutate(type = \"FD\",          direction = \"upstream\")  # Get all sub-catchment IDs of fish occurrence sites with stream  # segments downstream fs_subc_ids_down <- names(downstream_segments)  # Get unique site IDs of the fish occurrences  fs_site_id <- upstream_point_locations %>%    filter(subc_id_snap_accu %in% fs_subc_ids_down) %>%    arrange(match(subc_id_snap_accu, fs_subc_ids_down)) %>%    select(site_id) %>%    rename(from_site_id = site_id) %>%    filter(from_site_id != 14)   # Get the number of existing dams downstream and the distance to the closest # dam downstream ed_downstream <- downstream_segments %>%    map(., ~ inner_join(ed_site_id, .x, by= \"stream\")) %>%    map2(., fs_site_id$from_site_id, ~ mutate(.x, from_site_id = .y)) %>%    list_rbind(.) %>%    rename(., to_site_id = site_id) %>%    select(., from_site_id, to_site_id) %>%    inner_join(network_distance, ., by=c(\"from_site_id\", \"to_site_id\")) %>%    group_by(from_site_id) %>%    mutate(., n_dams = n()) %>%    filter(., dist == min(dist)) %>%    left_join(fs_site_id, ., by = \"from_site_id\") %>%    mutate(type = \"ED\",          direction = \"downstream\")  # Get the number of future dams downstream and the distance to the closest # dam downstream fd_downstream <- downstream_segments %>%    map(., ~ inner_join(fd_site_id, .x, by= \"stream\")) %>%    map2(., fs_site_id$from_site_id, ~ mutate(.x, from_site_id = .y)) %>%    list_rbind(.) %>%    rename(., to_site_id = site_id) %>%    select(., from_site_id, to_site_id) %>%    inner_join(network_distance, ., by=c(\"from_site_id\", \"to_site_id\")) %>%    group_by(from_site_id) %>%    mutate(., n_dams = n()) %>%    filter(., dist == min(dist)) %>%    left_join(fs_site_id, ., by = \"from_site_id\") %>%    mutate(type = \"FD\",          direction = \"downstream\")  # Bind tables dam_num_dist <- ed_upstream %>%    bind_rows(fd_upstream) %>%    bind_rows(ed_downstream) %>%    bind_rows(fd_downstream) # Prepare data.frame for the plot plot_data <- dam_num_dist %>%    mutate(dist = dist/1000) %>%    mutate(direction = factor(direction, levels = c(\"upstream\", \"downstream\")))  # Define colors cols <- c(ED = \"red3\", FD = \"orange\")  # Create plot to show the number of dams ggplot(plot_data, aes(x = as.factor(from_site_id), y = n_dams,                        fill = fct_rev(type))) +    geom_bar(stat=\"identity\", position= \"stack\", width = 0.5) +    facet_grid(~ direction, scales = \"free\") +   theme_bw() +   scale_fill_manual(name = \"Dams\",                       breaks = c(\"ED\", \"FD\"),                       labels = c(\"Existing\", \"Future\"),                       values = cols) +   labs(x = \"Fish occurrence site ID\", y = \"Number of dams\") # Create plot to show the distance to the closest dam ggplot(plot_data, aes(x = as.factor(from_site_id), y = dist, fill = type)) +    geom_bar(stat=\"identity\", position= \"dodge\", width = 0.5) +    facet_grid(~ direction, scales = \"free\") +   theme_bw() +   scale_fill_manual(name = \"Dams\",                       breaks = c(\"ED\", \"FD\"),                       labels = c(\"Existing\", \"Future\"),                       values = cols) +   labs(x = \"Fish occurrence site ID\", y = \"Distance along the stream network (km)\")"},{"path":[]},{"path":"/articles/case_study_cuba.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Case study - Cuba","text":"following example showcases functionality hydrographr package along species distribution modelling (SDM) workflow. use SDM predict suitable habitats dragonfly species Cuba. start defining regular tiles Hydrography90m (Amatulli et al., 2022) occurrence points species located. Afterwards download Hydrography90m CHELSA Bioclim (Karger & Zimmermann, 2019) layers predictor variables, crop merge , necessary. , aggregate variable values within sub-catchment study area, keeping predictors mean SD per sub-catchment. Using random forest (RF) algorithm, build simple SDM predict species potentially occur. Finally, reclassify layer sub-catchments study area, creating habitat suitability map.   Hypolestes trinitatis damselfly species endemic Cuba. species inhabits rivers streams located forest areas main mountain ranges eastern central Cuba. presence riparian forests well clean well oxygenated water seems important ecological conditions required H. trinitatis. Larvae can found clinging boulders cobbles river bed fast-flowing water stream segments pre-reproductive adults sexes sexually mature females remain time vegetation riparian forest find shelter prey. Let’s get started! Load required libraries Define working directory","code":"library(hydrographr) library(data.table) library(dplyr) library(terra) library(tools) library(stringr) library(ranger) library(leaflet) library(leafem) # Define the \"data_cuba\" directory, where you have downloaded all the data, # as the working directory wdir <- \"my/working/directory/data_cuba\" setwd(wdir)  # Create a new folder in the working directory to store all the data dir.create(\"data\")"},{"path":"/articles/case_study_cuba.html","id":"species-data","dir":"Articles","previous_headings":"","what":"Species data","title":"Case study - Cuba","text":"start downloading dataset including aquatic insect occurrence records Cuba https://doi.org/10.18728/igb-fred-778.3 (Cambas & Salina, 2022). Import dataset select occurrences species interest, sampled year 1980 (match climate data) Subset columns Let’s visualise species occurrences map Let’s define extent (bounding box) study area (xmin, ymin, xmax, ymax)","code":"download.file(\"https://fred.igb-berlin.de/data/file_download/1395\",               destfile = paste0(wdir, \"/data/spdata_cuba.csv\"), mode = \"wb\") spdata <- fread(paste0(wdir, \"/data/spdata_cuba.csv\"), sep = \"\\t\", fill = TRUE) %>%           filter(species == \"Hypolestes_trinitatis\") %>%           filter(year > 1980) spdata <- spdata %>%   dplyr::select(c(\"occurrence_ID\", \"species\", \"longitude\", \"latitude\", \"year\")) spdata # Convert species data to a spatial vector object to plot the points spdata_vect <- vect(spdata, geom=c(\"longitude\", \"latitude\")) bbox <- c(-84.9749110583, 19.8554808619, -74.1780248685, 23.1886107447) m <- leaflet() %>%   addProviderTiles('Esri.WorldShadedRelief') %>%   setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%   addCircles(data = spdata_vect, color = \"purple\") # data = spdata  m"},{"path":"/articles/case_study_cuba.html","id":"download-files","dir":"Articles","previous_headings":"","what":"Download files","title":"Case study - Cuba","text":"order download layers Hydrography90m, need know IDs 20°x20° tiles located. can obtain IDs using function get_tile_id(). function downloads uses auxiliary raster file contains regional units globally, thus requires active internet connection.  Currently function returns tiles regional unit input points located. However, may far study area hence always needed steps. Please double check tile IDs relevant purpose using Tile map found . case, Cuba spreads across two tiles, hold IDs “h08v06” “h10v06”, keep two.  Let’s define Hydrography90m variables like download. use variables slope_curv_max_dw_cel (maximum curvature highest upstream, focal downstream cell) spi (Stream Power Index) stream length predictors model. Additionally, download sub_catchment layer, necessary base-layer workflow. list available Hydrography90m variables, well details visualisations available . download CHELSA present future Bioclim variables quick outlook bioclimatic variables can look .","code":"tile_id <- get_tile_id(data = spdata, lon = \"longitude\", lat = \"latitude\") tile_id ## [1] \"h08v06\" \"h10v04\" \"h10v06\" tile_id <- tile_id[c(1,3)] # Variables in raster format vars_tif <- c(\"sub_catchment\", \"slope_curv_max_dw_cel\", \"spi\") # Variables in vector format. This layerholds the information on stream length vars_gpkg <- c(\"order_vect_point\") # Download the .tif tiles of the desired variables download_tiles(variable = vars_tif, tile_id = tile_id, file_format = \"tif\",               download_dir = \"data\")  # Download the .gpkg tiles of the desired variables download_tiles(variable = vars_gpkg, tile_id = tile_id, file_format = \"gpkg\",               download_dir = \"data\") # Create download directory dir.create(paste0(wdir, \"/data/chelsa_bioclim\")) # Extend timeout to 1000s to allow uninterrupted downloading options(timeout = 1000)  # Download # Present, 1981-2010 download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio12_1981-2010_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio12_1981-2010.tif\", mode = \"wb\") download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio15_1981-2010_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio15_1981-2010.tif\", mode = \"wb\") download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/1981-2010/bio/CHELSA_bio1_1981-2010_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio1_1981-2010.tif\", mode = \"wb\") # Future, 2041-2070 download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio12_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio12_IPSL-CM6A-LR_ssp370_2041-2070.tif\", mode = \"wb\") download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio15_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio15_IPSL-CM6A-LR_ssp370_2041-2070.tif\", mode = \"wb\") download.file(\"https://os.zhdk.cloud.switch.ch/envicloud/chelsa/chelsa_V2/GLOBAL/climatologies/2041-2070/IPSL-CM6A-LR/ssp370/bio/CHELSA_bio1_2041-2070_ipsl-cm6a-lr_ssp370_V.2.1.tif\", destfile = \"data/chelsa_bioclim/bio1_IPSL-CM6A-LR_ssp370_2041-2070.tif\", mode = \"wb\")"},{"path":"/articles/case_study_cuba.html","id":"cropping","dir":"Articles","previous_headings":"","what":"Cropping","title":"Case study - Cuba","text":"downloaded layers, need crop extent study area. straightforward case global CHELSA layers: define directory containing layers cropped … final output directory crop files using function crop_to_extent() loop However, hydrographic variable slope_curv_max_dw_cel (maximum curvature highest upstream, focal, downstream cell) split across two tiles. highly recommend first crop tiles extent study area limit size, afterwards merge one file. , first define input directories, time store cropped files directory input files, extra step reaching final output.","code":"dirs_chelsa <- paste0(wdir, \"/data/chelsa_bioclim\") # Define output directory for merged files layer_dir <- paste0(wdir, \"/data/final_layers\") # Create the directory dir.create(layer_dir) files_chelsa <- list.files(dirs_chelsa, pattern = \".tif\", full.names = TRUE)  for(ifile in files_chelsa) {     crop_to_extent(       raster_layer = ifile,       bounding_box = bbox,       out_dir = layer_dir,       file_name = basename(ifile),       read = FALSE,       quiet = TRUE) } dirs_h90m <- list.dirs(paste0(wdir, \"/data\"),                        recursive = TRUE, full.names = TRUE)  dirs_h90m <- dirs_h90m[grep(\"tiles20d\", dirs_h90m)] for(idir in dirs_h90m) {   # only choose rasters   tiles <- list.files(idir, pattern = \".tif$\", full.names = TRUE)   for(itile in tiles) {       crop_to_extent(         raster_layer = itile,         bounding_box = bbox,         out_dir = idir,         file_name = paste0(str_remove(basename(itile), \".tif\"), \"_crop.tif\"),         read = FALSE,         quiet = TRUE)   } }"},{"path":"/articles/case_study_cuba.html","id":"merging","dir":"Articles","previous_headings":"","what":"Merging","title":"Case study - Cuba","text":"cropped tiles variable now need merged together.","code":"# This can be done sequentially: idir <- dirs_h90m[1] merge_tiles(tile_dir = idir,             tile_names = list.files(idir, full.names = FALSE,                                     pattern = \"_crop.tif\"),             out_dir = layer_dir,             file_name = \"spi.tif\",             read = FALSE)  idir <- dirs_h90m[3] merge_tiles(tile_dir = idir,             tile_names = list.files(idir, full.names = FALSE,                                     pattern = \"_crop.tif\"),             out_dir = layer_dir,             file_name = \"slope_curv_max_dw_cel.tif\",             read = FALSE)  idir <- dirs_h90m[4] merge_tiles(tile_dir = idir,             tile_names = list.files(idir, full.names = FALSE,                                     pattern = \"_crop.tif\"),             out_dir = layer_dir,             file_name = \"sub_catchment.tif\",             read = FALSE)    # ...or in a loop for(idir in dirs_h90m) {   # Get input file extension   file_extension <- file_ext(list.files(idir, full.names = FALSE)[1])    # Assign file extension to output files   ivar_name <- paste0(     str_remove(basename(idir), \"_tiles20d\"), \".\", file_extension   )    # Run the function   merge_tiles(tile_dir = idir,               tile_names = list.files(idir, full.names = FALSE,                                       pattern = \"_crop.tif\"),               out_dir = layer_dir,               file_name = ivar_name,               read = FALSE) }"},{"path":"/articles/case_study_cuba.html","id":"extraction-of-sub-catchment-ids","dir":"Articles","previous_headings":"","what":"Extraction of sub-catchment IDs","title":"Case study - Cuba","text":"Extract IDs sub-catchments points located. step crucial, many functions later use require vector sub-catchment IDs input. Note function extract_ids() can used extract values specific points raster file provided argument subc_layer. can safely used query large raster files, loaded R.  species data now corresponding sub-catchment ids","code":"spdata_ids <- extract_ids(data = spdata, lon = \"longitude\", lat = \"latitude\",                           id = \"occurrence_ID\", quiet = FALSE,                           subc_layer = paste0(layer_dir, \"/sub_catchment.tif\"))"},{"path":"/articles/case_study_cuba.html","id":"aggregation-of-environmental-layers","dir":"Articles","previous_headings":"","what":"Aggregation of environmental layers","title":"Case study - Cuba","text":"calculate zonal statistics Hydrography90m CHELSA Bioclim variables sub-catchments study area. Caution, don’t increase number cores 3 can cause memory problems. However, highly depends number sub-catchments well. recommend test function different parameters find works best case.  good practice aggregating variables check NoData values: function also reports NoData values used calculation zonal statistics variable. Let’s inspect resulting table keep mean sd variable stats_table, use predictors species distribution model","code":"# Define input var_layers for the extract_zonal_stat() function var_layers <- list.files(layer_dir)[-9] # var_layers <- list.files(layer_dir)[c(3,5,7)] var_layers ## [1] \"bio1_1981-2010.tif\"                      ## [2] \"bio1_IPSL-CM6A-LR_ssp370_2041-2070.tif\" ## [3] \"bio12_1981-2010.tif\"                     ## [4] \"bio12_IPSL-CM6A-LR_ssp370_2041-2070.tif\" ## [5] \"bio15_1981-2010.tif\"                     ## [6] \"bio15_IPSL-CM6A-LR_ssp370_2041-2070.tif\" ## [7] \"slope_curv_max_dw_cel.tif\"               ## [8] \"spi.tif\" report_no_data(data_dir = layer_dir, var_layer = var_layers) ##                                    Raster   NoData ## 1                     bio15_1981-2010.tif        0 ## 2                      bio1_1981-2010.tif        0 ## 3  bio1_IPSL-CM6A-LR_ssp370_2041-2070.tif        0 ## 4                     bio12_1981-2010.tif        0 ## 5 bio12_IPSL-CM6A-LR_ssp370_2041-2070.tif        0 ## 6 bio15_IPSL-CM6A-LR_ssp370_2041-2070.tif        0 ## 7               slope_curv_max_dw_cel.tif -9999999 ## 8                                 spi.tif -9999999 # Run the function that returns the zonal statistics stats_table_zon <- extract_zonal_stat(                     data_dir = layer_dir,                     subc_layer = paste0(layer_dir, \"/sub_catchment.tif\"),                     subc_id = \"all\",                     var_layer = var_layers,                     out_dir = paste0(wdir, \"/data\"),                     file_name = \"zonal_stats.csv\",                     n_cores = 2) # Read the previously created file stats_table_zon <- fread(paste0(wdir, \"/data/zonal_stats.csv\")) colnames(stats_table_zon) ##  [1] \"subc_id\"                                   ##  [2] \"bio1_1981.2010_min\"                        ##  [3] \"bio1_1981.2010_max\"                        ##  [4] \"bio1_1981.2010_range\"                      ##  [5] \"bio1_1981.2010_mean\"                       ##  [6] \"bio1_1981.2010_sd\"                         ##  [7] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_min\"    ##  [8] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_max\"    ##  [9] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_range\"  ## [10] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_mean\"   ## [11] \"bio1_IPSL.CM6A.LR_ssp370_2041.2070_sd\"     ## [12] \"bio12_1981.2010_min\"                       ## [13] \"bio12_1981.2010_max\"                       ## [14] \"bio12_1981.2010_range\"                     ## [15] \"bio12_1981.2010_mean\"                      ## [16] \"bio12_1981.2010_sd\"                        ## [17] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_min\"   ## [18] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_max\"   ## [19] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_range\" ## [20] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_mean\"  ## [21] \"bio12_IPSL.CM6A.LR_ssp370_2041.2070_sd\"    ## [22] \"bio15_1981.2010_min\"                       ## [23] \"bio15_1981.2010_max\"                       ## [24] \"bio15_1981.2010_range\"                     ## [25] \"bio15_1981.2010_mean\"                      ## [26] \"bio15_1981.2010_sd\"                        ## [27] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_min\"   ## [28] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_max\"   ## [29] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_range\" ## [30] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_mean\"  ## [31] \"bio15_IPSL.CM6A.LR_ssp370_2041.2070_sd\"    ## [32] \"slope_curv_max_dw_cel_min\"                 ## [33] \"slope_curv_max_dw_cel_max\"                 ## [34] \"slope_curv_max_dw_cel_range\"               ## [35] \"slope_curv_max_dw_cel_mean\"                ## [36] \"slope_curv_max_dw_cel_sd\"                  ## [37] \"spi_min\"                                   ## [38] \"spi_max\"                                   ## [39] \"spi_range\"                                 ## [40] \"spi_mean\"                                  ## [41] \"spi_sd\" stats_table_zon <- stats_table_zon %>%   dplyr::select(contains(\"subc\") |  ends_with(\"_mean\") | ends_with(\"_sd\")) %>%   rename('subcatchment_id' = 'subc_id')"},{"path":"/articles/case_study_cuba.html","id":"reading-vector-files","dir":"Articles","previous_headings":"","what":"Reading vector files","title":"Case study - Cuba","text":"Read .gpkg databases two tiles, filtering sub-catchments study area. IDs can retrieved stats_table join two .gpkg databases select columns “subcatchment_id” “length”. use length stream indicator sub-catchment size","code":"stats_gpkg_h08v06 <- read_geopackage(   \"data/r.stream.order/order_vect_tiles20d/order_vect_point_h08v06.gpkg\",   subc_id = stats_table_zon$subcatchment_id) %>%   rename('subcatchment_id' = 'stream')  stats_gpkg_h10v06 <- read_geopackage(   \"data/r.stream.order/order_vect_tiles20d/order_vect_point_h10v06.gpkg\",   subc_id = stats_table_zon$subcatchment_id) %>%   rename('subcatchment_id' = 'stream') stats_gpkg <- rbind(stats_gpkg_h08v06, stats_gpkg_h10v06) %>%   dplyr::select(c(\"subcatchment_id\", \"length\"))  # Clear up memory rm(stats_gpkg_h08v06, stats_gpkg_h10v06); gc() ##            used  (Mb) gc trigger  (Mb) max used  (Mb) ## Ncells  4581027 244.7    8583821 458.5  6154826 328.8 ## Vcells 25723286 196.3   52452608 400.2 48537276 370.4"},{"path":"/articles/case_study_cuba.html","id":"prepare-data-for-modelling","dir":"Articles","previous_headings":"","what":"Prepare data for modelling","title":"Case study - Cuba","text":"Join stats_table .gpkg database values original raster files scaled, need re-scale modelling define following functions: … apply rescale values check IDs sub-catchments null values variables, observe identical: happens variables defined single-pixel sub-catchments, outlets. remove sub-catchments, model able handle data values. split dataset two datasets, according present future Bioclim variables. first used training model second prediction future suitable habitats classification model needs least two classes, case presences absences. sample 10,000 random sub-catchments used pseudoabsences model. filtering takes place sampling assures avoid sampling sub-catchments known presences species join species occurrences present environmental variables table join predictors presences pseudoabsences, obtain modelling data table Define binary column presence-absence (0-1) set factor step, two alternatives: Either split data train test sets: …use whole dataset training set, since RF performs intrinsic evaluation every tree","code":"stats_table <- left_join(stats_table_zon, stats_gpkg, by = \"subcatchment_id\") slope_scale <- function(x, na.rm = F) (x * 0.000001) clim_scale <- function(x, na.rm = F) (x * 0.1) offset <- function(x, na.rm = F) (x - 273.15) stats_table <- stats_table  %>%   mutate(across(contains(\"slope_curv_max_dw_cel\"), slope_scale)) %>%   mutate(across(starts_with(\"bio\"), clim_scale))  %>%   mutate(across(matches(\"bio1_.*_mean\"), offset)) head(stats_table) head(stats_table[is.na(stats_table$slope_curv_max_dw_cel_mean),]) head(stats_table[is.na(stats_table$spi_mean),]) stats_table[is.na(stats_table$spi_mean),]$subcatchment_id == stats_table[is.na(stats_table$slope_curv_max_dw_cel_mean),]$subcatchment_id ##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE stats_table <- stats_table[!is.na(stats_table$slope_curv_max_dw_cel_mean),] stats_table_present <- stats_table %>%   dplyr::select(!contains(\"IPSL\")) %>%   rename(bio1_mean = bio1_1981.2010_mean,          bio1_sd = bio1_1981.2010_sd,          bio12_mean = bio12_1981.2010_mean,          bio12_sd = bio12_1981.2010_sd,          bio15_mean = bio15_1981.2010_mean,          bio15_sd = bio15_1981.2010_sd)  stats_table_future <- stats_table %>%   dplyr::select(!contains(\"1981\")) %>%   rename(bio1_mean = bio1_IPSL.CM6A.LR_ssp370_2041.2070_mean,          bio1_sd = bio1_IPSL.CM6A.LR_ssp370_2041.2070_sd,          bio12_mean = bio12_IPSL.CM6A.LR_ssp370_2041.2070_mean,          bio12_sd = bio12_IPSL.CM6A.LR_ssp370_2041.2070_sd,          bio15_mean = bio15_IPSL.CM6A.LR_ssp370_2041.2070_mean,          bio15_sd = bio15_IPSL.CM6A.LR_ssp370_2041.2070_sd)  # Clear up memory # rm(stats_table) ; gc() pseudoabs <- stats_table_present %>%   filter(!subcatchment_id %in% spdata_ids$subcatchment_id) %>%   sample_n(10000) head(pseudoabs) presence <- left_join(spdata_ids, stats_table_present, by = \"subcatchment_id\") data_model <- data.table::rbindlist(list(presence, pseudoabs), fill = TRUE) data_model$occurrence <- ifelse(!is.na(data_model$occurrence_ID), 1, 0) data_model$occurrence <- as.factor(data_model$occurrence) head(data_model) set.seed(17) #obtain stratified training sample train_idx <- sample(nrow(data_model), 2/3 * nrow(data_model)) data_train <- data_model[train_idx, ] data_test <- data_model[-train_idx, ] data_train <- data_model"},{"path":"/articles/case_study_cuba.html","id":"a-simple-sdm-using-random-forest","dir":"Articles","previous_headings":"","what":"A simple SDM using Random Forest","title":"Case study - Cuba","text":"use ranger random forest (RF) algorithm (Wright & Ziegler, 2017) build species distribution model. Random forest can handle huge quantities data therefore scalable larger datasets. dataset highly imbalanced towards pseudo-absence data, apply -sampling method presence-data modelling process (Valavi et al., 2021). -sampling works balancing training data, contain significantly absence presence points. Following approach, classification-RF model use number pseudo-absences presence points classification tree, sampling replacement (bootstrapping) full pseudo-absence set (Valavi et al., 2021). Let’s define sample size classification tree proportion whole training set, based number presences. Run inspect model. model can evaluated based OOB prediction error. metric lies fact data points used training tree can used test tree. errors OOB samples called --bag errors.","code":"# number of presence records: pres_num <- as.numeric(table(data_train$occurrence)[\"1\"]) sample_size <- c(\"0\" = pres_num / nrow(data_train),                  \"1\" = pres_num / nrow(data_train)) sample_size ##           0           1  ## 0.004281589 0.004281589 model <- ranger(data_train$occurrence ~ .,                  data = data_train[, 6:14],                  num.trees = 1000,                  mtry= 6,                  replace = T,                  sample.fraction = sample_size,                  oob.error = T,                  keep.inbag = T,                  num.threads = 4,                  importance ='impurity',                  probability = T)  model ## Ranger result ##  ## Call: ##  ranger(data_train$occurrence ~ ., data = data_train[, 6:14],      num.trees = 1000, mtry = 6, replace = T, sample.fraction = sample_size,      oob.error = T, keep.inbag = T, num.threads = 4, importance = \"impurity\",      probability = T)  ##  ## Type:                             Probability estimation  ## Number of trees:                  1000  ## Sample size:                      10043  ## Number of independent variables:  9  ## Mtry:                             6  ## Target node size:                 10  ## Variable importance mode:         impurity  ## Splitrule:                        gini  ## OOB prediction error (Brier s.):  0.09067802"},{"path":"/articles/case_study_cuba.html","id":"model-prediction","dir":"Articles","previous_headings":"","what":"Model prediction","title":"Case study - Cuba","text":"predict table sub-catchments across Cuba, including future Bioclim variables. can now reclassify sub-catchment raster based predicted probabilities occurrence, visualise potential suitable habitats species. purpose use ‘reclass_raster’ function. function requires table reclassification rules (.e., subc_id = predicted_occurrence), values need integers. First, let’s join probabilities presence sub-catchment corresponding sub-catchment IDs, create table reclassification rules multiply probability values 100, convert integers can now reclassify sub-catchment raster Let’s plot future habitat suitability map, add presence points  zoom habitat suitability map, can observe sub-catchments offer suitable habitat species detailed view.","code":"pred <- predict(model, data = stats_table_future[,!1]) prediction <- data.table(subcatchment_id = stats_table_future$subcatchment_id,                          pred_occur = as.numeric(pred$predictions[,2])) head(prediction) ##    subcatchment_id pred_occur ## 1:       399592461 0.02508095 ## 2:       399592865 0.02323095 ## 3:       399593700 0.03909881 ## 4:       399593701 0.03078929 ## 5:       399593817 0.02586667 ## 6:       399593818 0.08412738 prediction$pred_occur <- as.integer(round(prediction$pred_occur, 2) * 100) reclass_raster(   data = prediction,   rast_val = \"subcatchment_id\",   new_val = \"pred_occur\",   raster_layer = paste0(layer_dir, \"/sub_catchment.tif\"),   recl_layer = paste0(wdir, \"/prediction.tif\"),   read = FALSE) # Define colour palette num_pal <- colorNumeric(   viridisLite::inferno(256)   , domain = prediction$pred_occur   , na.color = \"transparent\" )  p <- leaflet() %>% addTiles() %>%   setMaxBounds(bbox[1], bbox[2], bbox[3], bbox[4]) %>%     leafem::addGeotiff(   file = paste0(wdir, \"/prediction.tif\"), opacity = 1,    colorOptions = colorOptions(                   palette = hcl.colors(256, palette = \"inferno\"),                   na.color = \"transparent\"                   ) # read external raster file without loading it to R   ) %>%   leaflet::addCircles(data = spdata_vect, color = \"turquoise\", stroke = TRUE, # data = spdata                       weight = 5, opacity = 1) %>% # add points   addLegend(pal = num_pal,            values = prediction$pred_occur,            labels = palette(),            title = \"Future habitat suitability map<\/br>for Hypolestes trinitatis\",            position = \"bottomleft\", opacity = 1, labFormat = labelFormat(suffix = \"%\"))  # add a legend  p"},{"path":[]},{"path":"/articles/hydrographr.html","id":"system-requirements","dir":"Articles","previous_headings":"","what":"System requirements","title":"Getting started with hydrographr","text":"work smoothly hydrographr package, GRASS GIS, GDAL, pktools need installed. can find installation guideline operating systems: Linux Windows macOS","code":""},{"path":"/articles/hydrographr.html","id":"loading-hydrographr","dir":"Articles","previous_headings":"","what":"Loading hydrographr","title":"Getting started with hydrographr","text":"can install hydrographr GitHub repository. install R package yet, install remotes::install_github(). start exploring package load hydrographr.","code":"# If the package remotes is not installed run first: install.packages(\"remotes\")  remotes::install_github(\"glowabio/hydrographr\") library(hydrographr) #> Warning: replacing previous import 'dplyr::as_data_frame' by #> 'igraph::as_data_frame' when loading 'hydrographr'"},{"path":"/articles/linux_system_setup.html","id":"installation-of-the-required-gis-tools","dir":"Articles","previous_headings":"","what":"Installation of the required GIS tools","title":"Setting up the package requirements on Linux","text":"installed GIS tools, please, install using hydrographr. use Ubuntu can copy-paste commands . use another Linux distribution, please, see linked software webpage installation instruction. Warning! Sometimes installation work within VPN connection. fail fetch data online repositories installation, please try deactivating VPN connection. Add Ubuntugis repository First, add “Ubuntugis” Personal Package Archive (PPA) system’s software sources able install GIS tools available . Next, need tell system pull latest list software archive knows , including PPA just added: Now ready installGDAL, GRASS GIS GNU parallel. Copy paste commands console follow instructions install tools. GDAL GDAL translator library raster vector geospatial data formats comes variety useful command line utilities data translation processing. information check GDAL website. GRASS GIS GRASS GIS powerful raster, vector, geospatial processing engine including tools terrain ecosystem modeling, hydrology processing satellite aerial imagery. detailed installation instructions check GRASS GIS users wiki GRASS GIS website. Please, make sure grass8.2 installed. GRASS GIS addons Copy-paste commands install required addons GRASS GIS. GNU parallel GNU parallel shell tool executing jobs parallel multiple cores. faster processing, hydrographr functions snap_to_subc_segment() extract_zonal_stat() GNU parallel implemented. information check GNU parallel website. Now type ctrl+z exit GNU parallel environment. GNU bc GNU bc (Basic Calculator) arbitrary precision numeric processing language, used function snap_to_subc_segment(). information check GNU bc website. recommend reboot computer installation.","code":"# Add the Ubuntugis PPA sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable # Update the packages list sudo apt update # Install GDAL sudo apt install gdal-bin python3-gdal # Install GRASS GIS sudo apt-get install grass grass-core grass-dev grass-gui grass-doc sudo apt install make  # Install GRASS GIS addons export GRASSEXEC=\"grass --exec\" $GRASSEXEC  g.extension  extension=r.stream.distance $GRASSEXEC  g.extension  extension=r.stream.order $GRASSEXEC  g.extension  extension=r.stream.snap $GRASSEXEC  g.extension  extension=r.stream.basins # Download the latest version of GNU parallel  wget http://ftp.gnu.org/gnu/parallel/parallel-latest.tar.bz2  # Unzip the the .tar file and create a new folder parallel-yyyymmdd sudo tar xjf parallel-latest.tar.bz2 # Check folders and files in the directory and see the date of  # latest version e.g. parallel-20221122 ls # Move to the new created folder parallel-yyyymmdd cd parallel-20221122  sudo ./configure && make sudo make install parallel --citation  #> Academic tradition requires you to cite works you base your article on. #> If you use programs that use GNU Parallel to process data for an article in a #> scientific publication, please cite: #>  #> @software{tange_2022_7347980, #>       author       = {Tange, Ole}, #>       title        = {GNU Parallel 20221122 ('Херсо́н')}, #>       month        = Nov, #>       year         = 2022, #>       note         = {{GNU Parallel is a general parallelizer to run #>                        multiple serial command line programs in parallel #>                        without changing them.}}, #>       publisher    = {Zenodo}, #>       doi          = {10.5281/zenodo.7347980}, #>       url          = {https://doi.org/10.5281/zenodo.7347980} #> }  #> (Feel free to use \\nocite{tange_2022_7347980})  #> This helps funding further development; AND IT WON'T COST YOU A CENT. #> If you pay 10000 EUR you should feel free to use GNU Parallel without citing.  #> More about funding GNU Parallel and the citation notice: #> https://lists.gnu.org/archive/html/parallel/2013-11/msg00006.html #> https://www.gnu.org/software/parallel/parallel_design.html#citation-notice #> https://git.savannah.gnu.org/cgit/parallel.git/tree/doc/citation-notice-faq.txt  #> If you send a copy of your published article to tange@gnu.org, it will be #> mentioned in the release notes of next version of GNU Parallel. #>  #> Type: 'will cite' and press enter.  will cite  #> Thank you for your support: You are the reason why there is funding to #> continue maintaining GNU Parallel. On behalf of future versions of #> GNU Parallel, which would not exist without your support:  #>  THANK YOU SO MUCH  #> It is really appreciated. The citation notice is now silenced. # Install bc sudo apt install bc"},{"path":"/articles/windows_system_setup.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"Setting up the package requirements on Windows","text":"Make sure least 4 GB free disk space. need administrator privileges Windows. must running Windows 10 version 2004 higher Windows 11, otherwise WSL available.","code":""},{"path":"/articles/windows_system_setup.html","id":"installation-of-the-windows-subsystem-for-linux-with-ubuntu","dir":"Articles","previous_headings":"","what":"Installation of the Windows Subsystem for Linux with Ubuntu","title":"Setting up the package requirements on Windows","text":"Open Windows Command Prompt (cmd.exe) administrator mode clicking Start button typing “cmd” search bar.right-click “Command Prompt” select “Run Administrator” highlight result arrow keys press Ctrl+Shift+Enter open command prompt administrative privileges. Enter wsl --install enable features necessary run WSL also directly install Ubuntu distribution Linux, default option WSL. installation complete, restart machine. open “Start” menu click “Ubuntu” open Ubuntu console. Please note Ubuntu get installed automatically, can alternatively visit Windows store (https://aka.ms/wslstore), search Ubuntu, install restart. first time open Ubuntu via “Start” menu prompted create username password Linux distribution. default user account administrator Ubuntu. purpose guideline use hydrographr, can pick name want. Please note whilst entering password, nothing appear screen won’t see typing, completely normal. Window automatically update Linux distribution. update upgrade packages Ubuntu, please copy paste command Ubuntu terminal enter password. done regularly basis. run command sudo system ask password UNIX user account. details installation WSL see . details setting username password see .","code":"# Enable the features necessary to run WSL and  # install the Ubuntu distribution of Linux.  wsl --install  #> Installing: Virtual Machine Platform #> Virtual Machine Platform has been installed. #> Installing: Windows Subsystem for Linux #> Windows Subsystem for Linux has been installed. #> Downloading: WSL Kernel #> Installing: WSL Kernel #> WSL Kernel has been installed. #> Downloading: Ubuntu #> The requested operation is successful. Changes will not be effective until the system is rebooted. #> Installing, this may take a few minutes... #> Please create a default UNIX user account. The username does not need to match your Windows username. #> For more information visit: https://aka.ms/wslusers #> Enter new UNIX username: hydrographr #> New password: #> Retype new password: #> passwd: password updated successfully #> Installation successful! #> To run a command as administrator (user \"root\"), use \"sudo <command>\". #> See \"man sudo_root\" for details. # Update and upgrade packages on Ubuntu sudo apt update && sudo apt upgrade"},{"path":"/articles/windows_system_setup.html","id":"installation-of-the-required-gis-tools-on-the-wsl","dir":"Articles","previous_headings":"","what":"Installation of the required GIS tools on the WSL","title":"Setting up the package requirements on Windows","text":"Next, need install required GIS tools WSL Ubuntu system. can use Ubuntu terminal, PowerShell Windows Command Prompt start installation. use PowerShell Windows Command Prompt use command wsl enter WSL first, exit go back Windows OS installation GIS tools. Warning! Sometimes installation work within VPN connection. fail fetch data online repositories installation, please try deactivating VPN connection. Add Ubuntugis repository First, add “Ubuntugis” Personal Package Archive (PPA) system’s software sources able install GIS tools available . Next, need tell system pull latest list software archive knows , including PPA just added: Now ready installGDAL, GRASS GIS GNU parallel. Copy paste commands console follow instructions install tools. GDAL GDAL translator library raster vector geospatial data formats comes variety useful command line utilities data translation processing. information check GDAL website. GRASS GIS GRASS GIS powerful raster, vector, geospatial processing engine including tools terrain ecosystem modeling, hydrology processing satellite aerial imagery. detailed installation instructions check GRASS GIS users wiki GRASS GIS website. GRASS GIS addons Copy-paste commands install required addons GRASS GIS. GNU parallel GNU parallel shell tool executing jobs parallel multiple cores. faster processing, hydrographr functions snap_to_subc_segment() extract_zonal_stat() GNU parallel implemented. information check GNU parallel website. Now type ctrl+z exit GNU parallel environment. GNU bc GNU bc (Basic Calculator) arbitrary precision numeric processing language, used function snap_to_subc_segment(). information check GNU bc website. dos2unix Windows systems uses different text file line endings Linux systems. Dos2Unix package contains commands converting line endings text file Windows Linux vice versa. recommend reboot computer installation.","code":"# Add the Ubuntugis PPA sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable # Update the packages list sudo apt update && sudo apt upgrade # Install GDAL sudo apt install gdal-bin python3-gdal # Install GRASS GIS sudo apt-get install grass grass-core grass-dev grass-gui grass-doc sudo apt install make  # Install GRASS GIS addons export GRASSEXEC=\"grass --exec\" $GRASSEXEC  g.extension  extension=r.stream.distance $GRASSEXEC  g.extension  extension=r.stream.order $GRASSEXEC  g.extension  extension=r.stream.snap $GRASSEXEC  g.extension  extension=r.stream.basins # Download the latest version of GNU parallel  wget http://ftp.gnu.org/gnu/parallel/parallel-latest.tar.bz2  # Unzip the the .tar file and create a new folder parallel-yyyymmdd sudo tar xjf parallel-latest.tar.bz2 # Check folders and files in the directory and see the date of  # latest version e.g. parallel-20221122 ls # Move to the new created folder parallel-yyyymmdd cd parallel-20221122  sudo ./configure && make sudo make install parallel --citation  #> Academic tradition requires you to cite works you base your article on. #> If you use programs that use GNU Parallel to process data for an article in a #> scientific publication, please cite: #>  #> @software{tange_2022_7347980, #>       author       = {Tange, Ole}, #>       title        = {GNU Parallel 20221122 ('Херсо́н')}, #>       month        = Nov, #>       year         = 2022, #>       note         = {{GNU Parallel is a general parallelizer to run #>                        multiple serial command line programs in parallel #>                        without changing them.}}, #>       publisher    = {Zenodo}, #>       doi          = {10.5281/zenodo.7347980}, #>       url          = {https://doi.org/10.5281/zenodo.7347980} #> }  #> (Feel free to use \\nocite{tange_2022_7347980})  #> This helps funding further development; AND IT WON'T COST YOU A CENT. #> If you pay 10000 EUR you should feel free to use GNU Parallel without citing.  #> More about funding GNU Parallel and the citation notice: #> https://lists.gnu.org/archive/html/parallel/2013-11/msg00006.html #> https://www.gnu.org/software/parallel/parallel_design.html#citation-notice #> https://git.savannah.gnu.org/cgit/parallel.git/tree/doc/citation-notice-faq.txt  #> If you send a copy of your published article to tange@gnu.org, it will be #> mentioned in the release notes of next version of GNU Parallel. #>  #> Type: 'will cite' and press enter.  will cite  #> Thank you for your support: You are the reason why there is funding to #> continue maintaining GNU Parallel. On behalf of future versions of #> GNU Parallel, which would not exist without your support:  #>  THANK YOU SO MUCH  #> It is really appreciated. The citation notice is now silenced. # Install bc sudo apt install bc # Install dos2unix sudo apt install -y dos2unix"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Maria Üblacker. Author, maintainer. Afroditi Grigoropoulou. Author. Jaime Garcia Marquez. Author. Yusdiel Torres Cambas. Author. Christoph Schürz. Author. Mathieu Floury. Author. Thomas Tomiczek. Author. Vanessa Bremerich. Author. Giuseppe Amatulli. Author. Sami Domisch. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Üblacker M, Grigoropoulou , Garcia Marquez J, Torres Cambas Y, Schürz C, Floury M, Tomiczek T, Bremerich V, Amatulli G, Domisch S (2023). hydrographr: Scalable Hydrographic Data Processing R. R package version 1.0.14, https://glowabio.github.io/hydrographr/.","code":"@Manual{,   title = {hydrographr: Scalable Hydrographic Data Processing in R},   author = {Maria Üblacker and Afroditi Grigoropoulou and Jaime {Garcia Marquez} and Yusdiel {Torres Cambas} and Christoph Schürz and Mathieu Floury and Thomas Tomiczek and Vanessa Bremerich and Giuseppe Amatulli and Sami Domisch},   year = {2023},   note = {R package version 1.0.14},   url = {https://glowabio.github.io/hydrographr/}, }"},{"path":"/index.html","id":"hydrographr-","dir":"","previous_headings":"","what":"Scalable Hydrographic Data Processing in R","title":"Scalable Hydrographic Data Processing in R","text":"hydrographr provides collection R function wrappers GDAL GRASS-GIS functions efficiently work Hydrography90m spatial biodiversity data. easy--use functions process large raster vector data directly disk parallel, memory R get overloaded. allows creating scalable data processing analysis workflows R, even though data processed directly R. add functions vignette time, invite users test package. Please notify us possible issues, bugs feature requests issues tab top page.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Scalable Hydrographic Data Processing in R","text":"Please see installation guide required tools https://glowabio.github.io/hydrographr/articles/hydrographr.html. Afterwards, use following lines install package R: pdf manual hydrographr package can downloaded . thank NFDI4Biodiversity NFDI4Earth providing funding helped us getting hydrographr package together!","code":"install.packages(\"remotes\") remotes::install_github(\"glowabio/hydrographr\") library(hydrographr)"},{"path":"/reference/check_tiles_filesize.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function that checks the size of single files before downloading.\nIt is called and inherits arguments by the function 'download_tiles()' — check_tiles_filesize","title":"Internal function that checks the size of single files before downloading.\nIt is called and inherits arguments by the function 'download_tiles()' — check_tiles_filesize","text":"Internal function checks size single files downloading. called inherits arguments function 'download_tiles()'","code":""},{"path":"/reference/check_tiles_filesize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function that checks the size of single files before downloading.\nIt is called and inherits arguments by the function 'download_tiles()' — check_tiles_filesize","text":"","code":"check_tiles_filesize(   variable,   file_format = \"tif\",   tile_id = NULL,   reg_unit_id = NULL,   global = FALSE,   h90m_varnames,   h90m_tile_id,   h90m_file_formats,   file_size_table_sep )"},{"path":"/reference/check_tiles_filesize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function that checks the size of single files before downloading.\nIt is called and inherits arguments by the function 'download_tiles()' — check_tiles_filesize","text":"variable character vector variable names. file_format character. Format requested file (\"tif\" \"gpkg\"). tile_id character. ID requested tile regional unit. global logical. TRUE, global extent file downloaded. Default FALSE. h90m_varnames character vector. valid names hydrography90m files available download, (inherited 'download_tiles()'). h90m_tile_id character vector. valid IDs hydrography90m. regular tiles available download (inherited 'download_tiles()'). h90m_file_formats character vector. valid file types files available download (inherited 'download_tiles()'). file_size_table_sep data.frame. Lookup table including file names sizes (inherited 'download_tiles()').","code":""},{"path":"/reference/check_wsl.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if WSL and Ubuntu is installed on Windows — check_wsl","title":"Check if WSL and Ubuntu is installed on Windows — check_wsl","text":"Check WSL Ubuntu installed Windows","code":""},{"path":"/reference/check_wsl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if WSL and Ubuntu is installed on Windows — check_wsl","text":"","code":"check_wsl()"},{"path":"/reference/crop_to_extent.html","id":null,"dir":"Reference","previous_headings":"","what":"Crop raster to extent — crop_to_extent","title":"Crop raster to extent — crop_to_extent","text":"function crops input raster layer directly disk, .e. input layer need loaded R. raster .tif cropped polygon border line vector layer (cutline source) provided, otherwise bounding box provided (xmin, ymin, xmax, ymax coordinates spatial object extract bounding box), raster cropped extent bounding box. least cutline source (vector_layer) bounding box (bounding_box) provided. output always written disk, can optionally loaded R SpatRaster (terra package) object (using read = TRUE).","code":""},{"path":"/reference/crop_to_extent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Crop raster to extent — crop_to_extent","text":"","code":"crop_to_extent(   raster_layer,   vector_layer = NULL,   bounding_box = NULL,   out_dir,   file_name,   compression = \"low\",   bigtiff = TRUE,   read = TRUE,   quiet = TRUE )"},{"path":"/reference/crop_to_extent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Crop raster to extent — crop_to_extent","text":"raster_layer character. Full path input raster .tif layer. vector_layer character. Full path vector layer used cutline data source (similar mask operation). bounding_box numeric vector coordinates corners bounding box (xmin, ymin, xmax, ymax), SpatRaster, SpatVector, spatial object. out_dir character. directory output stored. file_name character. Name cropped output raster .tif file. compression character. Compression written output file. Compression levels can defined \"none\", \"low\", \"high\". Default \"low\". bigtiff logical. Define whether output file expected BIGTIFF (file size larger 4 GB). FALSE size > 4GB file written. Default TRUE. read logical. TRUE, cropped raster .tif layer gets read R. FALSE, layer stored disk. Default TRUE. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/crop_to_extent.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Crop raster to extent — crop_to_extent","text":"function returns always .tif raster file written disk. Optionally, SpatRaster (terra object) can loaded R read = TRUE.","code":""},{"path":"/reference/crop_to_extent.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Crop raster to extent — crop_to_extent","text":"Yusdiel Torres-Cambas","code":""},{"path":"/reference/crop_to_extent.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Crop raster to extent — crop_to_extent","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Define full path to the input raster .tif layer ans vector layer spi_raster <- paste0(my_directory, \"/hydrography90m_test_data\",                      \"/spi_1264942.tif\") basin_vector <- paste0(my_directory, \"/hydrography90m_test_data\",                       \"/basin_59.gpkg\")  # Crop the Stream Power Index to the basin spi_basin <- crop_to_extent(raster_layer = spi_raster,                             vector_layer = basin_vector,                             out_dir = my_directory,                             file_name = \"spi_basin_cropped.tif\",                             read = TRUE)"},{"path":"/reference/download_test_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Download test data — download_test_data","title":"Download test data — download_test_data","text":"Download test data package, includes Hydrography90m species point observation data small geographic extent, test functions. test data available https://drive.google.com/file/d/1kYNWXmtVm6X7MZLISOePGpvxB1pk1scD/view?usp=share_link can automatically downloaded unzipped function desired path.","code":""},{"path":"/reference/download_test_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download test data — download_test_data","text":"","code":"download_test_data(download_dir = \".\")"},{"path":"/reference/download_test_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download test data — download_test_data","text":"download_dir character. directory files downloaded. Default location working directory.","code":""},{"path":"/reference/download_test_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download test data — download_test_data","text":"Downloads test data Hydrography90m dataset","code":""},{"path":"/reference/download_test_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Download test data — download_test_data","text":"Amatulli, G., Garcia Marquez, J., Sethi, T., Kiesel, J., Grigoropoulou, ., Üblacker, M. M., Shen, L. Q., Domisch, S.: Hydrography90m: new high-resolution global hydrographic dataset, Earth Syst. Sci. Data, 14, 4525–4550, https://doi.org/10.5194/essd-14-4525-2022, 2022.\") Amatulli G., Garcia Marquez J., Sethi T., Kiesel J., Grigoropoulou ., Üblacker M., Shen L. & Domisch S. (2022-08-09 ). Hydrography90m: new high-resolution global hydrographic dataset. IGB Leibniz-Institute Freshwater Ecology Inland Fisheries. dataset. https://doi.org/10.18728/igb-fred-762.1","code":""},{"path":"/reference/download_test_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Download test data — download_test_data","text":"Afroditi Grigoropoulou","code":""},{"path":"/reference/download_test_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download test data — download_test_data","text":"","code":"# Download the test data to the current working directory download_test_data()  # Download the data to a specific (existing) directory download_test_data(\"path/to/your/directory\")"},{"path":"/reference/download_tiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Download files of the Hydrography90m dataset — download_tiles","title":"Download files of the Hydrography90m dataset — download_tiles","text":"function downloads files Hydrography90m dataset, available https://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path=%2F . files stored folder architecture domain. Multiple regular tile regional unit files can requested single call function. tile regional unit IDs can obtained using functions \"get_tile_id\" \"get_regional_unit_id\" respectively.","code":""},{"path":"/reference/download_tiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download files of the Hydrography90m dataset — download_tiles","text":"","code":"download_tiles(   variable,   file_format = \"tif\",   tile_id = NULL,   reg_unit_id = NULL,   global = FALSE,   download_dir = \".\" )"},{"path":"/reference/download_tiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download files of the Hydrography90m dataset — download_tiles","text":"variable character vector variable names. See Details variable names. file_format character. Format requested file (\"tif\" \"gpkg\"). See Details. tile_id character vector. IDs requested tiles. reg_unit_id character vector. IDs requested regional units. global logical. TRUE, global extent file downloaded. Default FALSE. download_dir character. directory files downloaded. Default working directory.","code":""},{"path":"/reference/download_tiles.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Download files of the Hydrography90m dataset — download_tiles","text":"following table can find variables included Hydrography90m dataset. column \"Variable\" includes variable names used input parameter \"variable\" function. Likewise, column \"File format\" contains input given \"file_format\" parameter. details visualisations spatial layers, please refer https://hydrography.org/hydrography90m/hydrography90m_layers/.","code":""},{"path":"/reference/download_tiles.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Download files of the Hydrography90m dataset — download_tiles","text":"error download file (likely case files bigger 3-4GB), can try manually download file pasting link returned error message browser.","code":""},{"path":"/reference/download_tiles.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Download files of the Hydrography90m dataset — download_tiles","text":"Amatulli G., Garcia Marquez J., Sethi T., Kiesel J., Grigoropoulou ., Üblacker M., Shen L. & Domisch S. (2022-08-09 ) Hydrography90m: new high-resolution global hydrographic dataset. IGB Leibniz-Institute Freshwater Ecology Inland Fisheries. dataset. https://doi.org/10.18728/igb-fred-762.1","code":""},{"path":"/reference/download_tiles.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Download files of the Hydrography90m dataset — download_tiles","text":"Afroditi Grigoropoulou","code":""},{"path":"/reference/download_tiles.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download files of the Hydrography90m dataset — download_tiles","text":"","code":"# Download data for two variables in three regular tiles # to the current working directory download_tiles(variable = c(\"sti\", \"stream_dist_up_farth\"),                file_format = \"tif\",                tile_id = c(\"h00v02\",\"h16v02\", \"h16v04\"))  # Download the global .tif layer for the variable \"direction\" # into the temporary R folder or define a different directory # Define directory my_directory <- tempdir() # Download layer download_tiles(variable = \"direction\",                file_format = \"tif\",                global = TRUE,                download_dir = my_directory)"},{"path":"/reference/download_tiles_base.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal function that downloads a single file from\nhttps://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path=\nGDrive folder.\nIt is called and inherits arguments by the function 'download_tiles()'. — download_tiles_base","title":"Internal function that downloads a single file from\nhttps://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path=\nGDrive folder.\nIt is called and inherits arguments by the function 'download_tiles()'. — download_tiles_base","text":"Internal function downloads single file https://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path= GDrive folder. called inherits arguments function 'download_tiles()'.","code":""},{"path":"/reference/download_tiles_base.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal function that downloads a single file from\nhttps://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path=\nGDrive folder.\nIt is called and inherits arguments by the function 'download_tiles()'. — download_tiles_base","text":"","code":"download_tiles_base(   variable,   file_format = \"tif\",   tile_id = NULL,   global = FALSE,   download_dir = \".\",   file_size_table_sep = NULL,   server_url = NULL )"},{"path":"/reference/download_tiles_base.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Internal function that downloads a single file from\nhttps://public.igb-berlin.de/index.php/s/agciopgzXjWswF4?path=\nGDrive folder.\nIt is called and inherits arguments by the function 'download_tiles()'. — download_tiles_base","text":"variable character vector variable names. file_format character. Format requested file (\"tif\" \"gpkg\"). tile_id character. ID requested tile regional unit. global logical. TRUE, global extent file downloaded. Default FALSE. download_dir character. directory files downloaded. Default working directory. file_size_table_sep data.frame. Lookup table including file names sizes (inherited 'download_tiles()'). server_url character. url home download folder either Nimbus GDrive (inherited 'download_tiles()').","code":""},{"path":"/reference/extract_from_gpkg.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract values from the stream order .gpkg files. — extract_from_gpkg","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"function reads attribute table stream network GeoPackage file (.gpkg) stored disk extracts data one () input sub-catchment (.e. stream segment) IDs. output data.table, output loaded R.","code":""},{"path":"/reference/extract_from_gpkg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"","code":"extract_from_gpkg(   data_dir,   subc_id,   subc_layer,   var_layer,   out_dir = NULL,   file_name = NULL,   n_cores = NULL,   quiet = TRUE )"},{"path":"/reference/extract_from_gpkg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"data_dir character. Path directory containing input data. subc_id numeric vector sub-catchment IDs \"\". \"\", attribute table extracted stream segments input .gpkg layer. stream segment IDs sub-catchment IDs. vector sub-catchment IDs can acquired extract_ids() function, sub-setting resulting data.frame. subc_layer character. Full path sub-catchment ID .tif layer var_layer character vector .gpkg files disk, e.g. \"order_vect_point_h18v04.gpkg\". out_dir character. directory output stored. out_dir specified, attribute tables stored .csv files location, named input variable vector files (e.g. \"/path//stats_order_vect_point_h18v04.csv\"). NULL, output loaded R stored disk. file_name character. Name .csv file output table stored. out_dir also specified purpose. n_cores numeric. Number cores used parallelization, case multiple .gpkg files provided var_layer. NULL, available cores - 1 used. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/extract_from_gpkg.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"https://grass.osgeo.org/grass82/manuals/v..ogr.html","code":""},{"path":"/reference/extract_from_gpkg.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"Afroditi Grigoropoulou, Jaime Garcia Marquez, Maria M. Üblacker","code":""},{"path":"/reference/extract_from_gpkg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract values from the stream order .gpkg files. — extract_from_gpkg","text":"","code":"# Download test data into temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Define path to the directory containing all input data test_data <- paste0(my_directory, \"/hydrography90m_test_data\")  # Define sub-catchment ID layer subc_raster <- paste0(my_directory, \"/hydrography90m_test_data\",                   \"/subcatchment_1264942.tif\")  # Extract the attribute table of the file order_vect_59.gpkg for all the # sub-catchment IDs of the subcatchment_1264942.tif raster layer attribute_table <- extract_from_gpkg(data_dir = test_data,                                      subc_id = \"all\",                                      subc_layer = subc_raster,                                      var_layer = \"order_vect_59.gpkg\",                                      n_cores = 1)  # Show the output table attribute_table"},{"path":"/reference/extract_ids.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract sub-catchment and/or basin IDs — extract_ids","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"Extracts ID value basin /sub-catchment raster layer given point location.","code":""},{"path":"/reference/extract_ids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"","code":"extract_ids(   data,   lon,   lat,   id = NULL,   basin_layer = NULL,   subc_layer = NULL,   quiet = TRUE )"},{"path":"/reference/extract_ids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"data data.frame data.table lat/lon coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates. id character. name column containing unique IDs row \"data\" (e.g., occurrence site IDs). basin_layer character. Full path basin ID .tif layer. subc_layer character. Full path sub-catchment ID .tif layer. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/extract_ids.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"extraction value given point location basin /sub-catchment raster layer Hydrography90m dataset, GDAL function 'gdallocationinfo' used. point locations defined coordinates WGS84 reference system. function can also used extract value given raster layer WGS84 projection, e.g. environmental information stored input raster file.","code":""},{"path":"/reference/extract_ids.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"Duplicated rows removed.","code":""},{"path":"/reference/extract_ids.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"https://gdal.org/programs/gdallocationinfo.html","code":""},{"path":"/reference/extract_ids.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"Afroditi Grigoropoulou, Maria Üblacker","code":""},{"path":"/reference/extract_ids.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract sub-catchment and/or basin IDs — extract_ids","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load occurrence data species_occurrence <- read.table(paste0(my_directory,                                         \"/hydrography90m_test_data\",                                         \"/spdata_1264942.txt\"),                                  header = TRUE)  # Define full path to the basin and sub-catchments raster layer basin_raster <- paste0(my_directory,                      \"/hydrography90m_test_data/basin_1264942.tif\") subc_raster <- paste0(my_directory,                     \"/hydrography90m_test_data/basin_1264942.tif\")  # Extract basin and sub-catchment IDs from the Hydrography90m layers hydrography90m_ids <- extract_ids(data = species_occurrence,                                   lon = \"longitude\",                                   lat = \"latitude\",                                   id = \"occurrence_id\",                                   subc_layer = subc_raster,                                   basin_layer = basin_raster)  # Show the output table hydrography90m_ids"},{"path":"/reference/extract_zonal_stat.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate zonal statistics — extract_zonal_stat","title":"Calculate zonal statistics — extract_zonal_stat","text":"Calculate zonal statistics based one environmental variable raster .tif layers. function can used aggregate data across set () sub-catchments. sub-catchment raster (.tif) input file stored disk. output data.table loaded R.","code":""},{"path":"/reference/extract_zonal_stat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate zonal statistics — extract_zonal_stat","text":"","code":"extract_zonal_stat(   data_dir,   subc_id,   subc_layer,   var_layer,   out_dir = NULL,   file_name = NULL,   n_cores = NULL,   quiet = TRUE )"},{"path":"/reference/extract_zonal_stat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate zonal statistics — extract_zonal_stat","text":"data_dir character. Path directory containing input data. subc_id Vector sub-catchment IDs \"\". \"\", zonal statistics calculated sub-catchments given sub-catchment raster layer. vector sub-catchment IDs can acquired extract_ids() function, sub-setting resulting data.frame. subc_layer character. Full path sub-catchment ID .tif layer. var_layer character vector variable raster layers disk, e.g. \"slope_grad_dw_cel_h00v00.tif\". Variable names remain intact file names, even file processing, .e., slope_grad_dw_cel appear file name. files cropped extent sub-catchment layer speed computation. out_dir character. directory output stored. out_dir file_name specified, output table stored .csv file location. NULL, output loaded R stored disk. file_name character. Name .csv file output table stored. out_dir also specified purpose. n_cores numeric. Number cores used parallelization, case multiple .tif files provided var_layer. Default NULL (= detectCores(logical=FALSE)-1). quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/extract_zonal_stat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate zonal statistics — extract_zonal_stat","text":"Returns table sub-catchment ID (subc_id), number cells value (data_cells), number cells NoData value (nodata_cells), minimum value (min), maximum value (max), value range (range), arithmetic mean (mean), arithmetic mean absolute values (mean_abs), standard deviation (sd), variance (var), coefficient variance (cv), sum (sum), sum absolute values (sum_abs).","code":""},{"path":"/reference/extract_zonal_stat.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate zonal statistics — extract_zonal_stat","text":"https://grass.osgeo.org/grass82/manuals/r.univar.html","code":""},{"path":[]},{"path":"/reference/extract_zonal_stat.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate zonal statistics — extract_zonal_stat","text":"Afroditi Grigoropoulou, Jaime Garcia Marquez, Maria M. Üblacker","code":""},{"path":"/reference/extract_zonal_stat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate zonal statistics — extract_zonal_stat","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Define full path to the sub-catchment ID .tif layer subc_raster <-  paste0(my_directory, \"/hydrography90m_test_data\",                        \"/subcatchment_1264942.tif\")  # Define the directory where the output will be stored output_folder <- paste0(my_directory, \"/hydrography90m_test_data/output\") # Create output folder if it doesn't exist if(!dir.exists(output_folder)) dir.create(output_folder)  # Calculate the zonal statistics for all sub-catchments for two variables stat <- extract_zonal_stat(data_dir = paste0(my_directory,                                              \"/hydrography90m_test_data\"),                            subc_id = c(513837216, 513841103,                                        513850467, 513868394,                                        513870312),                            subc_layer = subc_raster,                            var_layer = c(\"spi_1264942.tif\",                                          \"sti_1264942.tif\"),                            out_dir = output_folder,                            file_name = \"zonal_statistics.csv\",                            n_cores = 2) # Show output table stat"},{"path":"/reference/fix_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Fix path for WSL on Windows — fix_path","title":"Fix path for WSL on Windows — fix_path","text":"Fix path WSL Windows","code":""},{"path":"/reference/fix_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fix path for WSL on Windows — fix_path","text":"","code":"fix_path(path)"},{"path":"/reference/fix_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fix path for WSL on Windows — fix_path","text":"path Full Windows path.","code":""},{"path":"/reference/get_catchment_graph.html","id":null,"dir":"Reference","previous_headings":"","what":"Get catchment from graph — get_catchment_graph","title":"Get catchment from graph — get_catchment_graph","text":"Subset network graph extracting upstream, downstream entire catchment, one multiple stream segments. function return either one data.tables graph objects input stream segment. Note stream segment sub-catchment IDs identical, consistency, use term \"subc_id\". switching mode either \"\", \"\" \"\", upstream, downstream connected segments returned.","code":""},{"path":"/reference/get_catchment_graph.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get catchment from graph — get_catchment_graph","text":"","code":"get_catchment_graph(   g,   subc_id = NULL,   outlet = FALSE,   mode = NULL,   as_graph = FALSE,   n_cores = 1,   max_size = 1500 )"},{"path":"/reference/get_catchment_graph.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get catchment from graph — get_catchment_graph","text":"g igraph object. directed graph. subc_id numeric vector single multiple IDs, e.g (c(ID1, ID2, ID3, ...). sub-catchment (equivalent stream segment) IDs delineate upstream drainage area. empty, outlets used sub-catchment IDs (outlet = TRUE). Note can browse entire network online https://geo.igb-berlin.de/maps/351/view left hand side, select \"Stream segment ID\"  layer click map get ID. Optional. outlet logical. TRUE, outlets given network graph used additional input subc_ids. Outlets identified internally stream segments downstream connected segment. Default FALSE. mode character. One \"\", \"\" \"\". \"\" returns upstream catchment, \"\" returns downstream catchment (catchments reachable given input segment), \"\" returns . as_graph logical. TRUE, output new graph list new graphs original attributes. FALSE, output  new data.table list data.tables. List objects named subc_ids. Default FALSE. n_cores numeric. Number cores used parallelization case multiple stream segments / outlets. Default 1. Currently, parallelization process requires copying data core. case graph large, many segments used input, setting n_cores higher value can speed computation. comes however cost possible RAM limitations even slower processing since large data copied core. Hence consider testing n_cores = 1 first. Optional. max_size numeric. Specifies maximum size data passed parallel back-end MB. Default 1500 (1.5 GB). Consider higher value large study areas (one 20°x20° tile). Optional.","code":""},{"path":"/reference/get_catchment_graph.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get catchment from graph — get_catchment_graph","text":"graph data.table reports subc_ids. case multiple input segments, results stored list.","code":""},{"path":"/reference/get_catchment_graph.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get catchment from graph — get_catchment_graph","text":"Csardi G, Nepusz T: igraph software package complex network research, InterJournal, Complex Systems 1695. 2006. https://igraph.org","code":""},{"path":"/reference/get_catchment_graph.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get catchment from graph — get_catchment_graph","text":"Sami Domisch","code":""},{"path":"/reference/get_catchment_graph.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get catchment from graph — get_catchment_graph","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load stream network as a graph my_graph <- read_geopackage(gpkg = paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                            import_as = \"graph\")  # Pick a random subc_id subc_id = \"513855877\" # Get the upstream catchment as a graph g_up <- get_catchment_graph(g = my_graph, subc_id = subc_id, mode = \"in\",                             outlet = FALSE, as_graph = TRUE, n_cores = 1)  # Get the downstream segments as a data.table, g_down <- get_catchment_graph(g = my_graph, subc_id = subc_id, mode = \"out\",                               outlet = FALSE, as_graph = FALSE, n_cores = 1)  # Get the catchments of all outlets in the study area as a graph g_all <- get_catchment_graph(g = my_graph, mode = \"in\", outlet = TRUE,                              as_graph = TRUE, n_cores = 1)"},{"path":"/reference/get_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate euclidean or along the network distance between points — get_distance","title":"Calculate euclidean or along the network distance between points — get_distance","text":"Calculate euclidean along network distance points. calculate distance along network, point coordinates need snapped stream network using function snap_to_network snap_to_subc_segment.","code":""},{"path":"/reference/get_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate euclidean or along the network distance between points — get_distance","text":"","code":"get_distance(   data,   lon,   lat,   id,   basin_id = NULL,   basin_layer = NULL,   stream_layer = NULL,   distance = \"both\",   n_cores = 1,   quiet = TRUE )"},{"path":"/reference/get_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate euclidean or along the network distance between points — get_distance","text":"data data.frame data.table lat/lon coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates. id character. name column containing unique IDs row \"data\" (e.g., occurrence site IDs). basin_id character. name column basin IDs. NULL distance set 'network' '', basin IDs extracted automatically. Default NULL. basin_layer character. Full path basin ID .tif layer. Needs defined calculate distance along network. stream_layer character. Full path stream network .gpkg file. Needs defined calculate distance along network. distance character. One \"euclidean\", \"network\", \"\". \"euclidean\", euclidean distances pairs points calculated. \"network\", shortest path along network pairs points calculated. (see \"Details\" information). method set \"\", distance measures calculated. Distances given meters. Default \"\". n_cores numeric. Number cores used parallelization. Default 1. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/get_distance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate euclidean or along the network distance between points — get_distance","text":"distance='euclidean', distance matrix, meters, euclidean distances pairs points (object class data.frame). distance='network', data.frame three columns: from_id, to_id, dist. 'dist' column includes distance, meters, shortest path along network node from_id node to_id. distance='', list containing objects returned.","code":""},{"path":"/reference/get_distance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate euclidean or along the network distance between points — get_distance","text":"calculate euclidian distance pair points function uses v.distance command GRASS GIS, set produce square matrix distances. calculation distances along stream network implemented command v.net.allpairs GRASS GIS. along network distance calculation done pair points located within basin. points located different basins function can run parallel (.e., core distance calculations points within one basin). distance points located different basins zero connected network.","code":""},{"path":"/reference/get_distance.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate euclidean or along the network distance between points — get_distance","text":"https://grass.osgeo.org/grass82/manuals/v.net.allpairs.html https://grass.osgeo.org/grass82/manuals/v.distance.html","code":""},{"path":[]},{"path":"/reference/get_distance.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate euclidean or along the network distance between points — get_distance","text":"Afroditi Grigoropoulou, Maria M. Üblacker, Jaime Garcia Marquez","code":""},{"path":"/reference/get_distance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate euclidean or along the network distance between points — get_distance","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load occurrence data species_occurrence <- read.table(paste0(my_directory,                             \"/hydrography90m_test_data/spdata_1264942.txt\"),                               header = TRUE)  basin_rast <- paste0(my_directory,                      \"/hydrography90m_test_data/basin_1264942.tif\")  # Define full path to the sub-catchment raster layer subc_rast <- paste0(my_directory,                     \"/hydrography90m_test_data/subcatchment_1264942.tif\")  # Define full path to the vector file of the stream network stream_vect <- paste0(my_directory,                       \"/hydrography90m_test_data/order_vect_59.gpkg\")  # Automatically extract the basin and sub-catchment IDs and # snap the data points to the stream segment snapped_coordinates <- snap_to_subc_segment(data = species_occurrence,                                             lon = \"longitude\",                                             lat = \"latitude\",                                             id = \"occurrence_id\",                                             basin_layer = basin_rast,                                             subc_layer = subc_rast,                                             stream_layer = stream_vect,                                             n_cores = 2) # Show head of output table head(snapped_coordinates)  # Get the euclidean distance and the distance along the network between all # pairs of points distance_table <- get_distance(data = snapped_coordinates,                                lon = \"lon_snap\",                                lat = \"lat_snap\",                                id = \"occurrence_id\",                                basin_id = \"basin_id\",                                basin_layer = basin_rast,                                stream_layer = stream_vect,                                distance = \"network\") # Show table distance_table"},{"path":"/reference/get_os.html","id":null,"dir":"Reference","previous_headings":"","what":"Identify the operating system.\nThe function was written by Will Lowe and was copied from here:\nhttps://conjugateprior.org/2015/06/identifying-the-os-from-r/ — get_os","title":"Identify the operating system.\nThe function was written by Will Lowe and was copied from here:\nhttps://conjugateprior.org/2015/06/identifying-the-os-from-r/ — get_os","text":"Identify operating system. function written Lowe copied : https://conjugateprior.org/2015/06/identifying--os--r/","code":""},{"path":"/reference/get_os.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identify the operating system.\nThe function was written by Will Lowe and was copied from here:\nhttps://conjugateprior.org/2015/06/identifying-the-os-from-r/ — get_os","text":"","code":"get_os()"},{"path":"/reference/get_pfafstetter_basins.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Pfafstetter basins — get_pfafstetter_basins","title":"Get Pfafstetter basins — get_pfafstetter_basins","text":"Subset basin catchment nine smaller units following Pfafstetter basin delineation scheme.","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Pfafstetter basins — get_pfafstetter_basins","text":"","code":"get_pfafstetter_basins(   g,   subc_raster,   out_dir,   file_name,   data_table = FALSE,   n_cores = NULL )"},{"path":"/reference/get_pfafstetter_basins.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Pfafstetter basins — get_pfafstetter_basins","text":"g igraph object. directed graph basin one outlet. outlet can stream / sub-catchment upstream basin split smaller ones. subc_raster character. Full path sub-catchment raster file basin. need cropped / masked basin, IDs sub-catchments need match input graph. out_dir character. path output directory Pfafstetter raster layer written. needed data.table=FALSE. file_name character. filename extension Pfafstetter raster layer (e.g. 'pfafstetter_raster.tif\"). needed data.table=FALSE. data_table Logical. TRUE, result loaded R 2-column data.table (sub-catchment ID Pfafstetter code). FALSE, result loaded raster (terra object) R written disk. Default FALSE. n_cores numeric. Number cores used parallelization. Default NULL (= detectCores(logical=FALSE)-1). Optional.","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Pfafstetter basins — get_pfafstetter_basins","text":"Either data.table, raster (terra object) loaded R. case result raster, .tif file written disk.","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Pfafstetter basins — get_pfafstetter_basins","text":"drainage basin can split smaller sub-basins following hierarchical topological coding scheme (see Verdin & Verdin (1999) details). function splits given basin nine sub-basins using flow accumulation basis. user define sub-catchment (stream segment) ID serves outlet basin. Note can stream segment sufficient number upstream segments (sub-catchments).","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Get Pfafstetter basins — get_pfafstetter_basins","text":"can use online map https://geo.igb-berlin.de/maps/351/view identify ID stream segment (use \"Stream segment ID\" layer left)","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get Pfafstetter basins — get_pfafstetter_basins","text":"Verdin, K.L. & Verdin, J.P. (1999). topological system delineation codification Earth’s river basins. Journal Hydrology, 218(1-2), 1-12. doi:10.1016/s0022-1694(99)00011-6","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get Pfafstetter basins — get_pfafstetter_basins","text":"Sami Domisch","code":""},{"path":"/reference/get_pfafstetter_basins.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Pfafstetter basins — get_pfafstetter_basins","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Import the stream network as a graph # Load stream network as a graph my_graph <- read_geopackage(gpkg = paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                            import_as = \"graph\")  # Subset the graph such that it contains only one basin. You can use # a random ID, i.e. it does not need to be the real outlet of the basin. g_subset <- get_catchment_graph(g = my_graph,                          subc_id = \"513867227\",                          outlet = FALSE,                          mode = \"in\",                          as_graph = TRUE)  # Specify the sub-catchment raster file subc_raster <- paste0(my_directory,\"/hydrography90m_test_data\",                      \"/subcatchment_1264942.tif\")  # Specify the output directory out_dir <- my_directory  # Calculate the Pfafstetter sub-basins and write the raster layer to disk ( # and import into R) pfafstetter <- get_pfafstetter_basins(g = g_subset ,                                       subc_raster = subc_raster,                                       out_dir = out_dir,                                       file_name = \"pfafstetter_raster.tif\",                                       data_table = FALSE,                                       n_cores = 4)"},{"path":"/reference/get_regional_unit_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Hydrography90m regional unit IDs — get_regional_unit_id","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"Identifies IDs regional units within Hydrography90m data input points located. IDs required download data using download_tiles(). Input point data frame.","code":""},{"path":"/reference/get_regional_unit_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"","code":"get_regional_unit_id(data, lon, lat, quiet = TRUE)"},{"path":"/reference/get_regional_unit_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"data data.frame data.table lat/lon coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/get_regional_unit_id.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"https://gdal.org/programs/gdallocationinfo.html","code":""},{"path":"/reference/get_regional_unit_id.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"Afroditi Grigoropoulou","code":""},{"path":"/reference/get_regional_unit_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Hydrography90m regional unit IDs — get_regional_unit_id","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Read the species data species_occurence <- read.table(paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/spdata_1264942.txt\"),                               header = TRUE)  # Get the regional unit ID get_regional_unit_id(species_occurence, lon = \"longitude\",                     lat = \"latitude\")"},{"path":"/reference/get_segment_neighbours.html","id":null,"dir":"Reference","previous_headings":"","what":"Get stream segment neighbours — get_segment_neighbours","title":"Get stream segment neighbours — get_segment_neighbours","text":"segment, reports upstream, downstream, -downstream segments connected one multiple input segments within specified neighbour order, option summarize attributes across segments. Note stream segment sub-catchment IDs identical, consistency, use term \"subc_id\". function can also used create connectivity table Marxan using var_layer=\"length\" attach_only=TRUE. resulting table reports connectivity segment, along stream length connected segments.","code":""},{"path":"/reference/get_segment_neighbours.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get stream segment neighbours — get_segment_neighbours","text":"","code":"get_segment_neighbours(   g,   subc_id = NULL,   var_layer = NULL,   stat = NULL,   attach_only = FALSE,   order = 5,   mode = \"in\",   n_cores = 1,   max_size = 1500 )"},{"path":"/reference/get_segment_neighbours.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get stream segment neighbours — get_segment_neighbours","text":"g igraph object. directed graph. subc_id numeric vector input sub-catchment IDs (=stream segment IDs) search connected segments. var_layer character vector. One attributes (variable layers) input graph reported output segment_id (\"to_stream\"). Optional. Default NULL. stat one functions mean, median, min, max, sd (without quotes). Aggregates (summarizes) variables neighbourhood input segment (\"stream\", e.g., average land cover next five upstream segments sub-catchments). Default NULL. attach_only logical. TRUE, selected variables attached segment without aggregation. Default FALSE. order numeric. neighbouring order igraph::ego. Order = 1 immediate neighbours input sub-catchment IDs, order = 2 order 1 plus immediate neighbours sub-catchment IDs order 1, . mode character. One \"\", \"\", \"\". \"\" returns upstream neighbouring segments, \"\" returns downstream segments, \"\" returns . n_cores numeric. Number cores used parallelization case multiple stream segments / outlets. Default 1. Currently, parallelization process requires copying data core. case graph large, many segments used input, setting n_cores higher value can speed computation. comes however cost possible RAM limitations even slower processing since large data copied core. Hence consider testing n_cores = 1 first. Optional. max_size numeric. Specifies maximum size data passed parallel back-end MB. Default 1500 (1.5 GB). Consider higher value large study areas (one 20°x20° tile). Optional.","code":""},{"path":"/reference/get_segment_neighbours.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Get stream segment neighbours — get_segment_neighbours","text":"Csardi G, Nepusz T: igraph software package complex network research, InterJournal, Complex Systems 1695. 2006. https://igraph.org","code":""},{"path":"/reference/get_segment_neighbours.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get stream segment neighbours — get_segment_neighbours","text":"Sami Domisch","code":""},{"path":"/reference/get_segment_neighbours.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get stream segment neighbours — get_segment_neighbours","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load the stream network as graph my_graph <- read_geopackage(gpkg= paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                             import_as = \"graph\")  # Get the upstream segment neighbours in the 5th order # and report the length and source elevation # for the neighbours of each input segment get_segment_neighbours(g = my_graph, subc_id = subc_id,                        order = 5, mode = \"in\", n_cores = 1,                        var_layer = c(\"length\", \"source_elev\"),                        attach_only = TRUE)  # Get the downstream segment neighbours in the 5th order # and calculate the median length and source elevation # across the neighbours of each input segment get_segment_neighbours(g = my_graph, subc_id = subc_id,                        order = 2, mode =\"out\", n_cores = 1,                        var_layer = c(\"length\", \"source_elev\"),                        stat = median)  # Get the up-and downstream segment neighbours in the 5th order # and report the median length and source elevation # for the neighbours of each input segment get_segment_neighbours(g = my_graph, subc_id = subc_id, order = 2,                        mode = \"all\", n_cores = 1,                        var_layer = c(\"length\", \"source_elev\"),                        stat = mean, attach_only = TRUE)"},{"path":"/reference/get_tile_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Hydrography90m regular tile ID — get_tile_id","title":"Get the Hydrography90m regular tile ID — get_tile_id","text":"Identifies ids tiles within Hydrography90m data given points located. IDs required download data using download_tiles(). Input point data frame.","code":""},{"path":"/reference/get_tile_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Hydrography90m regular tile ID — get_tile_id","text":"","code":"get_tile_id(data, lon, lat)"},{"path":"/reference/get_tile_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Hydrography90m regular tile ID — get_tile_id","text":"data data.frame data.table lat/lon coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates.","code":""},{"path":"/reference/get_tile_id.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Get the Hydrography90m regular tile ID — get_tile_id","text":"Afroditi Grigoropoulou","code":""},{"path":"/reference/get_tile_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the Hydrography90m regular tile ID — get_tile_id","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load species occurrence data species_occurrence <- read.table(paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/spdata_1264942.txt\"),                                  header = TRUE)  # Get the tile ID get_tile_id(data = species_occurrence,             lon = \"longitude\", lat = \"latitude\")"},{"path":"/reference/get_upstream_catchment.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate upstream basin — get_upstream_catchment","title":"Calculate upstream basin — get_upstream_catchment","text":"Calculates upstream basin given point, considering point outlet.","code":""},{"path":"/reference/get_upstream_catchment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate upstream basin — get_upstream_catchment","text":"","code":"get_upstream_catchment(   data,   id,   lon,   lat,   direction_layer = NULL,   out_dir = NULL,   n_cores = NULL,   compression = \"low\",   bigtiff = TRUE,   quiet = TRUE )"},{"path":"/reference/get_upstream_catchment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate upstream basin — get_upstream_catchment","text":"data data.frame data.table lat/lon coordinates WGS84, snapped stream network. snapping can done using function 'snap_to_network'. id character. name column containing unique IDs row \"data\" (e.g., occurrence site IDs). lon character. name column longitude coordinates. lat character. name column latitude coordinates. direction_layer character. Full path raster file direction variable. out_dir Full path directory output(s) stored. identify upstream catchment output file name includes site id. n_cores numeric. Number cores used parallelization. NULL, available cores - 1 used. Default NULL. compression character. Compression written output file. Compression levels can defined \"none\", \"low\", \"high\". Default \"low\". bigtiff logical. Define whether output file expected BIGTIFF (file size larger 4 GB). FALSE size > 4GB file written. Default TRUE. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/get_upstream_catchment.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate upstream basin — get_upstream_catchment","text":"https://grass.osgeo.org/grass82/manuals/r.water.outlet.html https://grass.osgeo.org/grass82/manuals/r.region.html","code":""},{"path":[]},{"path":"/reference/get_upstream_catchment.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate upstream basin — get_upstream_catchment","text":"Jaime Garcia Marquez, Afroditi Grigoropoulou, Maria M. Üblacker","code":""},{"path":"/reference/get_upstream_catchment.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate upstream basin — get_upstream_catchment","text":"","code":"# Download test data into temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Before running the function get_upstream_catchment(), snap the points to # to the stream segment. There are multiple ways to snap the points. Here is # one example:  # Load occurrence data species_occurence <- read.table(paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/spdata_1264942.txt\"),                               header = TRUE)  # Define full path to the basin and sub-catchments raster layer basin_raster <- paste0(my_directory,                        \"/hydrography90m_test_data/basin_1264942.tif\") subc_raster <- paste0(my_directory,                       \"/hydrography90m_test_data/subcatchment_1264942.tif\")  # Define full path to the vector file of the stream network stream_vector <- paste0(my_directory,                         \"/hydrography90m_test_data/order_vect_59.gpkg\")  # Automatically extract the basin and sub-catchment IDs and # snap the data points to the stream segment snapped_coordinates <- snap_to_subc_segment(data = species_occurence,                                             lon = \"longitude\",                                             lat = \"latitude\",                                             id = \"occurrence_id\",                                             basin_layer = basin_raster,                                             subc_layer = subc_raster,                                             stream_layer = stream_vector,                                             n_cores = 2)  # Define full path to the direction .tif direction_raster <- paste0(my_directory,                            \"/hydrography90m_test_data/direction_1264942.tif\") # Define the path for the output file(s) output_folder <-  paste0(my_directory, \"/upstream_catchments\") if(!dir.exists(output_folder)) dir.create(output_folder) # Get the upstream catchment for each point location get_upstream_catchment(snapped_coordinates,                        lon = \"lon_snap\",                        lat = \"lat_snap\",                        id = \"occurrence_id\",                        direction_layer = direction_raster,                        out_dir = output_folder,                        n_cores = 2)"},{"path":"/reference/make_sh_exec.html","id":null,"dir":"Reference","previous_headings":"","what":"Make bash scripts executable — make_sh_exec","title":"Make bash scripts executable — make_sh_exec","text":"Make bash scripts executable","code":""},{"path":"/reference/make_sh_exec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make bash scripts executable — make_sh_exec","text":"","code":"make_sh_exec()"},{"path":"/reference/merge_tiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge raster or vector objects — merge_tiles","title":"Merge raster or vector objects — merge_tiles","text":"Merge multiple raster spatial vector objects disk form new raster spatial vector object larger spatial extent. directory least two raster .tif spatial vector geopackage files provided. Depending input, output .tif .gpkg file (saved out_dir). read = TRUE, output read R SpatRaster (terra package) object case .tif files, SpatVector (terra package) object case .gpkg files.","code":""},{"path":"/reference/merge_tiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge raster or vector objects — merge_tiles","text":"","code":"merge_tiles(   tile_dir,   tile_names,   out_dir,   file_name,   name = \"stream\",   compression = \"low\",   bigtiff = TRUE,   read = FALSE,   quiet = TRUE )"},{"path":"/reference/merge_tiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge raster or vector objects — merge_tiles","text":"tile_dir character. directory containing raster spatial vectors tiles, merged. tile_names character. names files merged, including file extension (.tif .gpkg). out_dir character. directory output stored. file_name character. Name merged output file, including file extension (.tif .gpkg). name character. attribute table column name stream segment (\"stream\"), sub-catchment (\"ID\"), basin (\"ID\") outlet (\"ID\") column used merging GeoPackages. Default \"stream\". compression character. Compression written output file. Compression levels can defined \"none\", \"low\", \"high\". Default \"low\". bigtiff logical. Define whether output file expected BIGTIFF (file size larger 4 GB). FALSE size > 4GB file written. Default TRUE. read logical. TRUE, merged layer gets read R. FALSE, layer stored disk. Default FALSE. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/merge_tiles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge raster or vector objects — merge_tiles","text":".tif raster file .gpkg spatial vector object always written disk, optionally loaded R.","code":""},{"path":"/reference/merge_tiles.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Merge raster or vector objects — merge_tiles","text":"https://gdal.org/programs/gdalbuildvrt.html https://gdal.org/programs/gdal_translate.html https://gdal.org/programs/ogrmerge.html#ogrmerge https://gdal.org/programs/ogr2ogr.html","code":""},{"path":"/reference/merge_tiles.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Merge raster or vector objects — merge_tiles","text":"Thomas Tomiczek, Jaime Garcia Marquez, Afroditi Grigoropoulou","code":""},{"path":"/reference/merge_tiles.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge raster or vector objects — merge_tiles","text":"","code":"# Download tiles into the temporary R folder # or define a different directory my_directory <- tempdir() download_tiles(variable = \"basin\",                file_format = \"tif\",                tile_id = c(\"h22v08\",\"h22v10\"),                download_dir = my_directory)  # Define folder containing only the tiles, which should me merged tiles_folder <- paste0(my_directory, \"/r.watershed/basin_tiles20d\") # Define output folder output_folder <- paste0(my_directory, \"/merged_tiles\") # Create output folder if it doesn't exist if(!dir.exists(output_folder)) dir.create(output_folder)   # Merge tiles merged_tiles <- merge_tiles(tile_dir = tiles_folder,                             tile_names = c(\"basin_h22v08.tif\", \"basin_h22v10.tif\"),                             out_dir = output_folder,                             file_name = \"basin_merged.tif\",                             read = TRUE)"},{"path":"/reference/read_geopackage.html","id":null,"dir":"Reference","previous_headings":"","what":"Read a GeoPackage file — read_geopackage","title":"Read a GeoPackage file — read_geopackage","text":"Reads entire, subset GeoPackage vector file disk either table (data.table), directed graph object (igraph), spatial dataframe (sf) SpatVect object (terra).","code":""},{"path":"/reference/read_geopackage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read a GeoPackage file — read_geopackage","text":"","code":"read_geopackage(   gpkg,   import_as = \"data.table\",   layer_name = NULL,   subc_id = NULL,   name = \"stream\" )"},{"path":"/reference/read_geopackage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read a GeoPackage file — read_geopackage","text":"gpkg character. Full path GeoPackage file. import_as character. \"data.table\", \"graph\", \"sf\", \"SpatVect\". \"data.table\" imports data data.table. \"graph\" imports layer directed graph (igraph object). option possible network layer (e.g. stream network) needs contain attributes stream next stream. \"sf\" imports layer  spatial data frame (sf object). \"SpatVect\" imports layer SpatVector (terra object). Default \"data.table\". layer_name character. Name specific data layer import GeoPackage. specific data layer needs defined GeoPackage contains multiple layers. see available layers function st_layers() R package 'sf' can used. Optional. Default NULL. subc_id numeric. Vector sub-catchment (stream segment) IDs form (c(ID1, ID2, ...) spatial objects attributes GeoPackage imported. Optional. Default NULL. name character. attribute table column name stream segment (\"stream\"), sub-catchment (\"ID\"), basin (\"ID\") outlet (\"ID\") column used subsetting GeoPackage prior importing. Optional. Default \"stream\".","code":""},{"path":"/reference/read_geopackage.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Read a GeoPackage file — read_geopackage","text":"Sami Domisch, Maria M.Üblacker","code":""},{"path":"/reference/read_geopackage.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read a GeoPackage file — read_geopackage","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)   # Read the stream network as a graph my_graph <- read_geopackage(gpkg = paste0(my_directory,                                           \"/hydrography90m_test_data\",                                           \"/order_vect_59.gpkg\"),                                           import_as = \"graph\")  # Read the stream network as a data.table my_dt <- read_geopackage(gpkg = paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/order_vect_59.gpkg\"))  # Read the stream network as a data.table for specific IDs my_dt <- read_geopackage(gpkg = paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/order_vect_59.gpkg\"),                                        subc_id = c(513833203, 513833594))  # Read the sub-catchments as a SF-object my_sf <- read_geopackage(gpkg = paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/sub_catchment_59.gpkg\"),                                        import_as = \"sf\",                                        layer_name = \"sub_catchment\")  # Read a subset of sub-catchments as a SF-object my_sf <- read_geopackage(gpkg = paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/sub_catchment_59.gpkg\"),                                        import_as = \"sf\",                                        subc_id = c(513833203, 513833594),                                        name = \"ID\")  # Read the basin as SpatVect object my_sv <- read_geopackage(gpkg = paste0(my_directory,                                        \"/hydrography90m_test_data\",                                        \"/basin_59.gpkg\"),                                        import_as = \"SpatVect\")"},{"path":"/reference/reclass_raster.html","id":null,"dir":"Reference","previous_headings":"","what":"Reclassify an integer raster layer — reclass_raster","title":"Reclassify an integer raster layer — reclass_raster","text":"Reclassifies integer raster .tif layer using r.reclass function GRASS GIS. reclassify raster layer present raster values new raster values defined. input raster layer floating point values, multiply input data factor (e.g. 1000) achieve integer values, otherwise GRASS GIS r.reclass round raster values next integer always desired.","code":""},{"path":"/reference/reclass_raster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reclassify an integer raster layer — reclass_raster","text":"","code":"reclass_raster(   data,   rast_val,   new_val,   raster_layer,   recl_layer,   no_data = -9999,   type = \"Int32\",   compression = \"low\",   bigtiff = TRUE,   read = FALSE,   quiet = TRUE )"},{"path":"/reference/reclass_raster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reclassify an integer raster layer — reclass_raster","text":"data data.frame data.table present new raster values. rast_val character. name column present raster values. new_val character. name column new raster values. raster_layer Full path input raster .tif layer. recl_layer character. Full path output .tif layer reclassified raster file. no_data numeric. no_data value new .tif layer. Default -9999. type character. Data type; Options Byte, Int16, UInt16, Int32, UInt32,CInt16, CInt32. Default Int32. compression character. Compression written output file. Compression levels can defined \"none\", \"low\", \"high\". Default \"low\". bigtiff logical. Define whether output file expected BIGTIFF (file size larger 4 GB). FALSE size > 4GB file written. Default TRUE. read logical. TRUE, reclassified raster .tif layer gets read R SpatRaster (terra object). FALSE, layer stored disk. Default FALSE. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/reclass_raster.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Reclassify an integer raster layer — reclass_raster","text":"https://grass.osgeo.org/grass82/manuals/r.reclass.html","code":""},{"path":"/reference/reclass_raster.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Reclassify an integer raster layer — reclass_raster","text":"Maria M. Üblacker","code":""},{"path":"/reference/reclass_raster.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reclassify an integer raster layer — reclass_raster","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Read the stream order for each sub-catchment as a data.table my_dt <- read_geopackage(gpkg= paste0(my_directory,                                          \"/hydrography90m_test_data\",                                          \"/order_vect_59.gpkg\"),                          import_as = \"data.table\")   # Select the stream segment ID and and the Strahler stream order str_ord <- my_dt[,c(\"stream\", \"strahler\")]  # Define input and output raster layer stream_raster <- paste0(my_directory,                         \"/hydrography90m_test_data/stream_1264942.tif\")  recl_raster <- paste0(my_directory,                       \"/hydrography90m_test_data/reclassified_raster.tif\")  # Reclassify the stream network to obtain the Strahler stream order raster str_ord_rast <- reclass_raster(data = str_ord,                                rast_val = \"stream\",                                new_val = \"strahler\",                                raster_layer = stream_raster,                                recl_layer = recl_raster)"},{"path":"/reference/report_no_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Report NoData value — report_no_data","title":"Report NoData value — report_no_data","text":"function reports defined NoData value raster layer. NoData value raster layer represents absence data. computations NoData value can treated different ways. Either NoData value reported Nodata value ignored value computed available values specified location.","code":""},{"path":"/reference/report_no_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Report NoData value — report_no_data","text":"","code":"report_no_data(data_dir, var_layer, n_cores = NULL)"},{"path":"/reference/report_no_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Report NoData value — report_no_data","text":"data_dir character. Path directory containing input data. var_layer character vector variable raster layers disk, e.g. \"slope_grad_dw_cel_h00v00.tif\". n_cores numeric. Number cores used parallelization, case multiple .tif files provided var_layer.","code":""},{"path":"/reference/report_no_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Report NoData value — report_no_data","text":"https://gdal.org/programs/gdalinfo.html","code":""},{"path":"/reference/report_no_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Report NoData value — report_no_data","text":"Afroditi Grigoropoulou, Maria M. Üblacker","code":""},{"path":"/reference/report_no_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Report NoData value — report_no_data","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Report the NoData value report_no_data(data_dir = paste0(my_directory, \"/hydrography90m_test_data\"),                var_layer = c(\"subcatchment_1264942.tif\", \"flow_1264942.tif\",                              \"spi_1264942.tif\"),                n_core = 2)"},{"path":"/reference/set_no_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Set no data value — set_no_data","title":"Set no data value — set_no_data","text":"Change set NoData value raster layer. change happens -place, meaning original file overwritten disk.","code":""},{"path":"/reference/set_no_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set no data value — set_no_data","text":"","code":"set_no_data(data_dir, var_layer, no_data, quiet = TRUE)"},{"path":"/reference/set_no_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set no data value — set_no_data","text":"data_dir character. Path directory containing input data. var_layer character vector variable layers disk, e.g. c(\"sti_h16v02.tif\", \"slope_grad_dw_cel_h00v00.tif\"). original files overwritten. no_data numeric. desired NoData value. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/set_no_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Set no data value — set_no_data","text":"https://gdal.org/programs/gdal_edit.html","code":""},{"path":"/reference/set_no_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Set no data value — set_no_data","text":"Afroditi Grigoropoulou, Maria M. Üblacker","code":""},{"path":"/reference/set_no_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set no data value — set_no_data","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Define no data value set_no_data(data_dir = paste0(my_directory, \"/hydrography90m_test_data\"),             var_layer = \"cti_1264942.tif\",             no_data = -9999)"},{"path":"/reference/snap_to_network.html","id":null,"dir":"Reference","previous_headings":"","what":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"Snap points next stream segment within defined radius minimum flow accumulation.","code":""},{"path":"/reference/snap_to_network.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"","code":"snap_to_network(   data,   lon,   lat,   id,   stream_layer,   accu_layer = NULL,   method = \"distance\",   distance = 500,   accumulation = 0.5,   quiet = TRUE )"},{"path":"/reference/snap_to_network.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"data data.frame data.table lat/lon coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates. id character. name column containing unique IDs row \"data\" (e.g., occurrence site IDs). unique IDs need numeric. stream_layer character. Full path stream network .tif file accu_layer character. Full path flow accumulation .tif file. Needed point snapped next stream segment accumulation value higher flow accumulation threshold (set 'accumulation'). prevents points snapped small stream tributaries. Optional. Default NULL. method character. One \"distance\", \"accumulation\", \"\". Defines points snapped using distance flow accumulation (see \"Details\" information). method set \"\" output contain new coordinates calculations. Default \"distance\". distance numeric. Maximum radius pixels. points snapped next stream within radius. Default 500. accumulation numeric. Minimum flow accumulation. Points snapped next stream flow accumulation equal higher given value. Default 0.5. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/snap_to_network.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"Returns data.frame snapped coordinates sub-catchment ID snapped stream segment. sub-catchment ID NA, stream segment found within given distance (method = \"distance\") stream segment wad found within given distance flow accumulation equal higher given threshold (method = \"accumulation\"). \"-bbox\" means provided coordinates within extend (bounding box) provided stream network layer.","code":""},{"path":"/reference/snap_to_network.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"function makes use r.stream.snap command available GRASS GIS simultaneously number points stream network. distance threshold can specified points snapped stream segment within distance radius. However, avoid snapping small tributaries, accumulation threshold can used snapping occurs stream segment equal higher accumulation threshold within given distance radius.","code":""},{"path":"/reference/snap_to_network.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"Duplicated rows removed input data.","code":""},{"path":"/reference/snap_to_network.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"https://grass.osgeo.org/grass78/manuals/addons/r.stream.snap.html","code":""},{"path":"/reference/snap_to_network.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"Maria M. Üblacker, Jaime Garcia Marquez","code":""},{"path":"/reference/snap_to_network.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Snap points to stream segment based on distance or flow accumulation — snap_to_network","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load occurrence data species_occurrence <- read.table(paste0(my_directory,                             \"/hydrography90m_test_data/spdata_1264942.txt\"),                               header = TRUE)  # Define full path to stream network and flow accumulation stream_raster <- paste0(my_directory,                      \"/hydrography90m_test_data/stream_1264942.tif\") flow_raster <- paste0(my_directory,                      \"/hydrography90m_test_data/flow_1264942.tif\")  # To calculate the new (snapped) coordinates for a radius and a flow snapped_coordinates <- snap_to_network(data = species_occurrence,                                        lon = \"longitude\",                                        lat = \"latitude\",                                        id = \"occurrence_id\",                                        stream_layer = stream_raster,                                        accu_layer = flow_raster,                                        method = \"both\",                                        distance = 300,                                        accumulation = 0.8)  # Show head of output table head(snapped_coordinates)"},{"path":"/reference/snap_to_subc_segment.html","id":null,"dir":"Reference","previous_headings":"","what":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"Snaps data points stream segment sub-catchment data point located.","code":""},{"path":"/reference/snap_to_subc_segment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"","code":"snap_to_subc_segment(   data,   lon,   lat,   id,   basin_id = NULL,   subc_id = NULL,   basin_layer,   subc_layer,   stream_layer,   n_cores = 1,   quiet = TRUE )"},{"path":"/reference/snap_to_subc_segment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"data data.frame data.table lat/lon coordinates WGS84. lon character. name column longitude coordinates. lat character. name column latitude coordinates. id character. name column containing unique IDs row \"data\" (e.g., occurrence site IDs). unique IDs need numeric. basin_id character. name column basin IDs. NULL, basin IDs extracted automatically. Optional. Default NULL subc_id character. name column sub-catchment IDs. NULL, sub-catchment IDs extracted automatically. Optional. Default NULL. basin_layer character. Full path basin ID .tif layer. subc_layer character. Full path sub-catchment ID .tif layer. stream_layer character. Full path stream network .gpkg file. n_cores numeric. Number cores used parallelization. Default 1. quiet logical. FALSE, standard output printed. Default TRUE.","code":""},{"path":"/reference/snap_to_subc_segment.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"function uses network preparation maintenance module GRASS GIS (v.net), connect vector lines map (stream network) points map (occurrence/sampling points). masking stream segment sub-catchment target point located, connect operation snaps point stream segment using distance threshold. threshold automatically calculated longest distance two points within sub-catchment. way snapping always take place.operation creates new node vector line (.e. stream segment) new snapped coordinates can extracted.","code":""},{"path":"/reference/snap_to_subc_segment.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"https://grass.osgeo.org/grass82/manuals/v.net.html","code":""},{"path":[]},{"path":"/reference/snap_to_subc_segment.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"Jaime Garcia Marquez, Maria M. Üblacker","code":""},{"path":"/reference/snap_to_subc_segment.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Snap points to stream segment within the sub-catchment — snap_to_subc_segment","text":"","code":"# Download test data into the temporary R folder # or define a different directory my_directory <- tempdir() download_test_data(my_directory)  # Load occurrence data species_occurence <- read.table(paste0(my_directory,                             \"/hydrography90m_test_data/spdata_1264942.txt\"),                               header = TRUE) basin_rast <- paste0(my_directory,                      \"/hydrography90m_test_data/basin_1264942.tif\") subc_rast <- paste0(my_directory,                     \"/hydrography90m_test_data/subcatchment_1264942.tif\")  # Define full path to the vector file of the stream network stream_vect <- paste0(my_directory,                       \"/hydrography90m_test_data/order_vect_59.gpkg\")  hydrography90m_ids <- extract_ids(data = species_occurence,                                   lon = \"longitude\",                                   lat = \"latitude\",                                   id = \"occurrence_id\",                                   subc_layer = subc_rast,                                   basin_layer = basin_rast)  # Snap data points to the stream segment of the provided sub-catchment ID snapped_coordinates <- snap_to_subc_segment(data = hydrography90m_ids,                                             lon = \"longitude\",                                             lat = \"latitude\",                                             id = \"occurrence_id\",                                             basin_id = \"basin_id\",                                             subc_id = \"subcatchment_id\",                                             basin_layer = basin_rast,                                             subc_layer = subc_rast,                                             stream_layer = stream_vect,                                             n_cores = 2) # Show head of output table head(snapped_coordinates)  # OR # Automatically extract the basin and sub-catchment IDs and # snap the data points to the stream segment snapped_coordinates <- snap_to_subc_segment(data = species_occurence,                                             lon = \"longitude\",                                             lat = \"latitude\",                                             id = \"occurrence_id\",                                             basin_layer = basin_rast,                                             subc_layer = subc_rast,                                             stream_layer = stream_vect,                                             n_cores = 2) # Show head of output table head(snapped_coordinates)"}]
